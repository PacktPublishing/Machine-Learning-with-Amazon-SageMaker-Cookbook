{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f888bc",
   "metadata": {},
   "source": [
    "# Inspecting SageMaker Debugger Logs and Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36213a",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"130\" src=\"https://raw.githubusercontent.com/PacktPublishing/Amazon-SageMaker-Cookbook/master/Extra/cover-small-padded.png\"/>\n",
    "\n",
    "This notebook contains the code to help readers work through one of the recipes of the book [Machine Learning with Amazon SageMaker Cookbook: 80 proven recipes for data scientists and developers to perform ML experiments and deployments](https://www.amazon.com/Machine-Learning-Amazon-SageMaker-Cookbook/dp/1800567030)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de103bc6",
   "metadata": {},
   "source": [
    "### How to do it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb69272c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-cookbook-bucket/debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%store -r artifacts_path\n",
    "artifacts_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e5fdecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smdebug in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.0.8)\n",
      "Requirement already satisfied: pyinstrument>=3.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smdebug) (3.4.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smdebug) (3.15.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smdebug) (1.19.5)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smdebug) (20.9)\n",
      "Requirement already satisfied: boto3>=1.10.32 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smdebug) (1.17.35)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.10.32->smdebug) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.10.32->smdebug) (1.20.35)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.10.32->smdebug) (0.3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.35->boto3>=1.10.32->smdebug) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.35->boto3>=1.10.32->smdebug) (2.8.1)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.6.0->smdebug) (1.15.0)\n",
      "Requirement already satisfied: pyinstrument-cext>=0.2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyinstrument>=3.1.3->smdebug) (0.2.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->smdebug) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install smdebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4138bbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021-04-23 18:09:26.590 ip-172-16-33-253:9310 INFO s3_trial.py:42] Loading trial debug-output at path s3://sagemaker-cookbook-bucket/debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output\n",
      "[2021-04-23 18:09:27.926 ip-172-16-33-253:9310 WARNING s3handler.py:183] Encountered the exception ('Connection broken: IncompleteRead(0 bytes read, 268 more expected)', IncompleteRead(0 bytes read, 268 more expected)) while reading s3://sagemaker-cookbook-bucket/debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/index/000000001/000000001314_worker_0.json . Will retry now\n"
     ]
    }
   ],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "trial = create_trial(artifacts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0d9f81a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train-error', 'validation-error']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.tensor_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db331c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'debug-output',\n",
       " '_tensors': {'train-error': <smdebug.core.tensor.Tensor at 0x7f463b108dd8>,\n",
       "  'validation-error': <smdebug.core.tensor.Tensor at 0x7f4638da5b00>},\n",
       " '_mode_to_global': {},\n",
       " '_global_to_mode': {0: (<ModeKeys.GLOBAL: 4>, 0),\n",
       "  2: (<ModeKeys.GLOBAL: 4>, 2),\n",
       "  4: (<ModeKeys.GLOBAL: 4>, 4),\n",
       "  6: (<ModeKeys.GLOBAL: 4>, 6),\n",
       "  8: (<ModeKeys.GLOBAL: 4>, 8),\n",
       "  10: (<ModeKeys.GLOBAL: 4>, 10),\n",
       "  12: (<ModeKeys.GLOBAL: 4>, 12),\n",
       "  14: (<ModeKeys.GLOBAL: 4>, 14),\n",
       "  16: (<ModeKeys.GLOBAL: 4>, 16),\n",
       "  18: (<ModeKeys.GLOBAL: 4>, 18),\n",
       "  20: (<ModeKeys.GLOBAL: 4>, 20),\n",
       "  22: (<ModeKeys.GLOBAL: 4>, 22),\n",
       "  24: (<ModeKeys.GLOBAL: 4>, 24),\n",
       "  26: (<ModeKeys.GLOBAL: 4>, 26),\n",
       "  28: (<ModeKeys.GLOBAL: 4>, 28),\n",
       "  30: (<ModeKeys.GLOBAL: 4>, 30),\n",
       "  32: (<ModeKeys.GLOBAL: 4>, 32),\n",
       "  34: (<ModeKeys.GLOBAL: 4>, 34),\n",
       "  36: (<ModeKeys.GLOBAL: 4>, 36),\n",
       "  38: (<ModeKeys.GLOBAL: 4>, 38),\n",
       "  40: (<ModeKeys.GLOBAL: 4>, 40),\n",
       "  42: (<ModeKeys.GLOBAL: 4>, 42),\n",
       "  44: (<ModeKeys.GLOBAL: 4>, 44),\n",
       "  46: (<ModeKeys.GLOBAL: 4>, 46),\n",
       "  48: (<ModeKeys.GLOBAL: 4>, 48),\n",
       "  50: (<ModeKeys.GLOBAL: 4>, 50),\n",
       "  52: (<ModeKeys.GLOBAL: 4>, 52),\n",
       "  54: (<ModeKeys.GLOBAL: 4>, 54),\n",
       "  56: (<ModeKeys.GLOBAL: 4>, 56),\n",
       "  58: (<ModeKeys.GLOBAL: 4>, 58),\n",
       "  60: (<ModeKeys.GLOBAL: 4>, 60),\n",
       "  62: (<ModeKeys.GLOBAL: 4>, 62),\n",
       "  64: (<ModeKeys.GLOBAL: 4>, 64),\n",
       "  66: (<ModeKeys.GLOBAL: 4>, 66),\n",
       "  68: (<ModeKeys.GLOBAL: 4>, 68),\n",
       "  70: (<ModeKeys.GLOBAL: 4>, 70),\n",
       "  72: (<ModeKeys.GLOBAL: 4>, 72),\n",
       "  74: (<ModeKeys.GLOBAL: 4>, 74),\n",
       "  76: (<ModeKeys.GLOBAL: 4>, 76),\n",
       "  78: (<ModeKeys.GLOBAL: 4>, 78),\n",
       "  80: (<ModeKeys.GLOBAL: 4>, 80),\n",
       "  82: (<ModeKeys.GLOBAL: 4>, 82),\n",
       "  84: (<ModeKeys.GLOBAL: 4>, 84),\n",
       "  86: (<ModeKeys.GLOBAL: 4>, 86),\n",
       "  88: (<ModeKeys.GLOBAL: 4>, 88),\n",
       "  90: (<ModeKeys.GLOBAL: 4>, 90),\n",
       "  92: (<ModeKeys.GLOBAL: 4>, 92),\n",
       "  94: (<ModeKeys.GLOBAL: 4>, 94),\n",
       "  96: (<ModeKeys.GLOBAL: 4>, 96),\n",
       "  98: (<ModeKeys.GLOBAL: 4>, 98),\n",
       "  100: (<ModeKeys.GLOBAL: 4>, 100),\n",
       "  102: (<ModeKeys.GLOBAL: 4>, 102),\n",
       "  104: (<ModeKeys.GLOBAL: 4>, 104),\n",
       "  106: (<ModeKeys.GLOBAL: 4>, 106),\n",
       "  108: (<ModeKeys.GLOBAL: 4>, 108),\n",
       "  110: (<ModeKeys.GLOBAL: 4>, 110),\n",
       "  112: (<ModeKeys.GLOBAL: 4>, 112),\n",
       "  114: (<ModeKeys.GLOBAL: 4>, 114),\n",
       "  116: (<ModeKeys.GLOBAL: 4>, 116),\n",
       "  118: (<ModeKeys.GLOBAL: 4>, 118),\n",
       "  120: (<ModeKeys.GLOBAL: 4>, 120),\n",
       "  122: (<ModeKeys.GLOBAL: 4>, 122),\n",
       "  124: (<ModeKeys.GLOBAL: 4>, 124),\n",
       "  126: (<ModeKeys.GLOBAL: 4>, 126),\n",
       "  128: (<ModeKeys.GLOBAL: 4>, 128),\n",
       "  130: (<ModeKeys.GLOBAL: 4>, 130),\n",
       "  132: (<ModeKeys.GLOBAL: 4>, 132),\n",
       "  134: (<ModeKeys.GLOBAL: 4>, 134),\n",
       "  136: (<ModeKeys.GLOBAL: 4>, 136),\n",
       "  138: (<ModeKeys.GLOBAL: 4>, 138),\n",
       "  140: (<ModeKeys.GLOBAL: 4>, 140),\n",
       "  142: (<ModeKeys.GLOBAL: 4>, 142),\n",
       "  144: (<ModeKeys.GLOBAL: 4>, 144),\n",
       "  146: (<ModeKeys.GLOBAL: 4>, 146),\n",
       "  148: (<ModeKeys.GLOBAL: 4>, 148),\n",
       "  150: (<ModeKeys.GLOBAL: 4>, 150),\n",
       "  152: (<ModeKeys.GLOBAL: 4>, 152),\n",
       "  154: (<ModeKeys.GLOBAL: 4>, 154),\n",
       "  156: (<ModeKeys.GLOBAL: 4>, 156),\n",
       "  158: (<ModeKeys.GLOBAL: 4>, 158),\n",
       "  160: (<ModeKeys.GLOBAL: 4>, 160),\n",
       "  162: (<ModeKeys.GLOBAL: 4>, 162),\n",
       "  164: (<ModeKeys.GLOBAL: 4>, 164),\n",
       "  166: (<ModeKeys.GLOBAL: 4>, 166),\n",
       "  168: (<ModeKeys.GLOBAL: 4>, 168),\n",
       "  170: (<ModeKeys.GLOBAL: 4>, 170),\n",
       "  172: (<ModeKeys.GLOBAL: 4>, 172),\n",
       "  174: (<ModeKeys.GLOBAL: 4>, 174),\n",
       "  176: (<ModeKeys.GLOBAL: 4>, 176),\n",
       "  178: (<ModeKeys.GLOBAL: 4>, 178),\n",
       "  180: (<ModeKeys.GLOBAL: 4>, 180),\n",
       "  182: (<ModeKeys.GLOBAL: 4>, 182),\n",
       "  184: (<ModeKeys.GLOBAL: 4>, 184),\n",
       "  186: (<ModeKeys.GLOBAL: 4>, 186),\n",
       "  188: (<ModeKeys.GLOBAL: 4>, 188),\n",
       "  190: (<ModeKeys.GLOBAL: 4>, 190),\n",
       "  192: (<ModeKeys.GLOBAL: 4>, 192),\n",
       "  194: (<ModeKeys.GLOBAL: 4>, 194),\n",
       "  196: (<ModeKeys.GLOBAL: 4>, 196),\n",
       "  198: (<ModeKeys.GLOBAL: 4>, 198),\n",
       "  200: (<ModeKeys.GLOBAL: 4>, 200),\n",
       "  202: (<ModeKeys.GLOBAL: 4>, 202),\n",
       "  204: (<ModeKeys.GLOBAL: 4>, 204),\n",
       "  206: (<ModeKeys.GLOBAL: 4>, 206),\n",
       "  208: (<ModeKeys.GLOBAL: 4>, 208),\n",
       "  210: (<ModeKeys.GLOBAL: 4>, 210),\n",
       "  212: (<ModeKeys.GLOBAL: 4>, 212),\n",
       "  214: (<ModeKeys.GLOBAL: 4>, 214),\n",
       "  216: (<ModeKeys.GLOBAL: 4>, 216),\n",
       "  218: (<ModeKeys.GLOBAL: 4>, 218),\n",
       "  220: (<ModeKeys.GLOBAL: 4>, 220),\n",
       "  222: (<ModeKeys.GLOBAL: 4>, 222),\n",
       "  224: (<ModeKeys.GLOBAL: 4>, 224),\n",
       "  226: (<ModeKeys.GLOBAL: 4>, 226),\n",
       "  228: (<ModeKeys.GLOBAL: 4>, 228),\n",
       "  230: (<ModeKeys.GLOBAL: 4>, 230),\n",
       "  232: (<ModeKeys.GLOBAL: 4>, 232),\n",
       "  234: (<ModeKeys.GLOBAL: 4>, 234),\n",
       "  236: (<ModeKeys.GLOBAL: 4>, 236),\n",
       "  238: (<ModeKeys.GLOBAL: 4>, 238),\n",
       "  240: (<ModeKeys.GLOBAL: 4>, 240),\n",
       "  242: (<ModeKeys.GLOBAL: 4>, 242),\n",
       "  244: (<ModeKeys.GLOBAL: 4>, 244),\n",
       "  246: (<ModeKeys.GLOBAL: 4>, 246),\n",
       "  248: (<ModeKeys.GLOBAL: 4>, 248),\n",
       "  250: (<ModeKeys.GLOBAL: 4>, 250),\n",
       "  252: (<ModeKeys.GLOBAL: 4>, 252),\n",
       "  254: (<ModeKeys.GLOBAL: 4>, 254),\n",
       "  256: (<ModeKeys.GLOBAL: 4>, 256),\n",
       "  258: (<ModeKeys.GLOBAL: 4>, 258),\n",
       "  260: (<ModeKeys.GLOBAL: 4>, 260),\n",
       "  262: (<ModeKeys.GLOBAL: 4>, 262),\n",
       "  264: (<ModeKeys.GLOBAL: 4>, 264),\n",
       "  266: (<ModeKeys.GLOBAL: 4>, 266),\n",
       "  268: (<ModeKeys.GLOBAL: 4>, 268),\n",
       "  270: (<ModeKeys.GLOBAL: 4>, 270),\n",
       "  272: (<ModeKeys.GLOBAL: 4>, 272),\n",
       "  274: (<ModeKeys.GLOBAL: 4>, 274),\n",
       "  276: (<ModeKeys.GLOBAL: 4>, 276),\n",
       "  278: (<ModeKeys.GLOBAL: 4>, 278),\n",
       "  280: (<ModeKeys.GLOBAL: 4>, 280),\n",
       "  282: (<ModeKeys.GLOBAL: 4>, 282),\n",
       "  284: (<ModeKeys.GLOBAL: 4>, 284),\n",
       "  286: (<ModeKeys.GLOBAL: 4>, 286),\n",
       "  288: (<ModeKeys.GLOBAL: 4>, 288),\n",
       "  290: (<ModeKeys.GLOBAL: 4>, 290),\n",
       "  292: (<ModeKeys.GLOBAL: 4>, 292),\n",
       "  294: (<ModeKeys.GLOBAL: 4>, 294),\n",
       "  296: (<ModeKeys.GLOBAL: 4>, 296),\n",
       "  298: (<ModeKeys.GLOBAL: 4>, 298),\n",
       "  300: (<ModeKeys.GLOBAL: 4>, 300),\n",
       "  302: (<ModeKeys.GLOBAL: 4>, 302),\n",
       "  304: (<ModeKeys.GLOBAL: 4>, 304),\n",
       "  306: (<ModeKeys.GLOBAL: 4>, 306),\n",
       "  308: (<ModeKeys.GLOBAL: 4>, 308),\n",
       "  310: (<ModeKeys.GLOBAL: 4>, 310),\n",
       "  312: (<ModeKeys.GLOBAL: 4>, 312),\n",
       "  314: (<ModeKeys.GLOBAL: 4>, 314),\n",
       "  316: (<ModeKeys.GLOBAL: 4>, 316),\n",
       "  318: (<ModeKeys.GLOBAL: 4>, 318),\n",
       "  320: (<ModeKeys.GLOBAL: 4>, 320),\n",
       "  322: (<ModeKeys.GLOBAL: 4>, 322),\n",
       "  324: (<ModeKeys.GLOBAL: 4>, 324),\n",
       "  326: (<ModeKeys.GLOBAL: 4>, 326),\n",
       "  328: (<ModeKeys.GLOBAL: 4>, 328),\n",
       "  330: (<ModeKeys.GLOBAL: 4>, 330),\n",
       "  332: (<ModeKeys.GLOBAL: 4>, 332),\n",
       "  334: (<ModeKeys.GLOBAL: 4>, 334),\n",
       "  336: (<ModeKeys.GLOBAL: 4>, 336),\n",
       "  338: (<ModeKeys.GLOBAL: 4>, 338),\n",
       "  340: (<ModeKeys.GLOBAL: 4>, 340),\n",
       "  342: (<ModeKeys.GLOBAL: 4>, 342),\n",
       "  344: (<ModeKeys.GLOBAL: 4>, 344),\n",
       "  346: (<ModeKeys.GLOBAL: 4>, 346),\n",
       "  348: (<ModeKeys.GLOBAL: 4>, 348),\n",
       "  350: (<ModeKeys.GLOBAL: 4>, 350),\n",
       "  352: (<ModeKeys.GLOBAL: 4>, 352),\n",
       "  354: (<ModeKeys.GLOBAL: 4>, 354),\n",
       "  356: (<ModeKeys.GLOBAL: 4>, 356),\n",
       "  358: (<ModeKeys.GLOBAL: 4>, 358),\n",
       "  360: (<ModeKeys.GLOBAL: 4>, 360),\n",
       "  362: (<ModeKeys.GLOBAL: 4>, 362),\n",
       "  364: (<ModeKeys.GLOBAL: 4>, 364),\n",
       "  366: (<ModeKeys.GLOBAL: 4>, 366),\n",
       "  368: (<ModeKeys.GLOBAL: 4>, 368),\n",
       "  370: (<ModeKeys.GLOBAL: 4>, 370),\n",
       "  372: (<ModeKeys.GLOBAL: 4>, 372),\n",
       "  374: (<ModeKeys.GLOBAL: 4>, 374),\n",
       "  376: (<ModeKeys.GLOBAL: 4>, 376),\n",
       "  378: (<ModeKeys.GLOBAL: 4>, 378),\n",
       "  380: (<ModeKeys.GLOBAL: 4>, 380),\n",
       "  382: (<ModeKeys.GLOBAL: 4>, 382),\n",
       "  384: (<ModeKeys.GLOBAL: 4>, 384),\n",
       "  386: (<ModeKeys.GLOBAL: 4>, 386),\n",
       "  388: (<ModeKeys.GLOBAL: 4>, 388),\n",
       "  390: (<ModeKeys.GLOBAL: 4>, 390),\n",
       "  392: (<ModeKeys.GLOBAL: 4>, 392),\n",
       "  394: (<ModeKeys.GLOBAL: 4>, 394),\n",
       "  396: (<ModeKeys.GLOBAL: 4>, 396),\n",
       "  398: (<ModeKeys.GLOBAL: 4>, 398),\n",
       "  400: (<ModeKeys.GLOBAL: 4>, 400),\n",
       "  402: (<ModeKeys.GLOBAL: 4>, 402),\n",
       "  404: (<ModeKeys.GLOBAL: 4>, 404),\n",
       "  406: (<ModeKeys.GLOBAL: 4>, 406),\n",
       "  408: (<ModeKeys.GLOBAL: 4>, 408),\n",
       "  410: (<ModeKeys.GLOBAL: 4>, 410),\n",
       "  412: (<ModeKeys.GLOBAL: 4>, 412),\n",
       "  414: (<ModeKeys.GLOBAL: 4>, 414),\n",
       "  416: (<ModeKeys.GLOBAL: 4>, 416),\n",
       "  418: (<ModeKeys.GLOBAL: 4>, 418),\n",
       "  420: (<ModeKeys.GLOBAL: 4>, 420),\n",
       "  422: (<ModeKeys.GLOBAL: 4>, 422),\n",
       "  424: (<ModeKeys.GLOBAL: 4>, 424),\n",
       "  426: (<ModeKeys.GLOBAL: 4>, 426),\n",
       "  428: (<ModeKeys.GLOBAL: 4>, 428),\n",
       "  430: (<ModeKeys.GLOBAL: 4>, 430),\n",
       "  432: (<ModeKeys.GLOBAL: 4>, 432),\n",
       "  434: (<ModeKeys.GLOBAL: 4>, 434),\n",
       "  436: (<ModeKeys.GLOBAL: 4>, 436),\n",
       "  438: (<ModeKeys.GLOBAL: 4>, 438),\n",
       "  440: (<ModeKeys.GLOBAL: 4>, 440),\n",
       "  442: (<ModeKeys.GLOBAL: 4>, 442),\n",
       "  444: (<ModeKeys.GLOBAL: 4>, 444),\n",
       "  446: (<ModeKeys.GLOBAL: 4>, 446),\n",
       "  448: (<ModeKeys.GLOBAL: 4>, 448),\n",
       "  450: (<ModeKeys.GLOBAL: 4>, 450),\n",
       "  452: (<ModeKeys.GLOBAL: 4>, 452),\n",
       "  454: (<ModeKeys.GLOBAL: 4>, 454),\n",
       "  456: (<ModeKeys.GLOBAL: 4>, 456),\n",
       "  458: (<ModeKeys.GLOBAL: 4>, 458),\n",
       "  460: (<ModeKeys.GLOBAL: 4>, 460),\n",
       "  462: (<ModeKeys.GLOBAL: 4>, 462),\n",
       "  464: (<ModeKeys.GLOBAL: 4>, 464),\n",
       "  466: (<ModeKeys.GLOBAL: 4>, 466),\n",
       "  468: (<ModeKeys.GLOBAL: 4>, 468),\n",
       "  470: (<ModeKeys.GLOBAL: 4>, 470),\n",
       "  472: (<ModeKeys.GLOBAL: 4>, 472),\n",
       "  474: (<ModeKeys.GLOBAL: 4>, 474),\n",
       "  476: (<ModeKeys.GLOBAL: 4>, 476),\n",
       "  478: (<ModeKeys.GLOBAL: 4>, 478),\n",
       "  480: (<ModeKeys.GLOBAL: 4>, 480),\n",
       "  482: (<ModeKeys.GLOBAL: 4>, 482),\n",
       "  484: (<ModeKeys.GLOBAL: 4>, 484),\n",
       "  486: (<ModeKeys.GLOBAL: 4>, 486),\n",
       "  488: (<ModeKeys.GLOBAL: 4>, 488),\n",
       "  490: (<ModeKeys.GLOBAL: 4>, 490),\n",
       "  492: (<ModeKeys.GLOBAL: 4>, 492),\n",
       "  494: (<ModeKeys.GLOBAL: 4>, 494),\n",
       "  496: (<ModeKeys.GLOBAL: 4>, 496),\n",
       "  498: (<ModeKeys.GLOBAL: 4>, 498),\n",
       "  500: (<ModeKeys.GLOBAL: 4>, 500),\n",
       "  502: (<ModeKeys.GLOBAL: 4>, 502),\n",
       "  504: (<ModeKeys.GLOBAL: 4>, 504),\n",
       "  506: (<ModeKeys.GLOBAL: 4>, 506),\n",
       "  508: (<ModeKeys.GLOBAL: 4>, 508),\n",
       "  510: (<ModeKeys.GLOBAL: 4>, 510),\n",
       "  512: (<ModeKeys.GLOBAL: 4>, 512),\n",
       "  514: (<ModeKeys.GLOBAL: 4>, 514),\n",
       "  516: (<ModeKeys.GLOBAL: 4>, 516),\n",
       "  518: (<ModeKeys.GLOBAL: 4>, 518),\n",
       "  520: (<ModeKeys.GLOBAL: 4>, 520),\n",
       "  522: (<ModeKeys.GLOBAL: 4>, 522),\n",
       "  524: (<ModeKeys.GLOBAL: 4>, 524),\n",
       "  526: (<ModeKeys.GLOBAL: 4>, 526),\n",
       "  528: (<ModeKeys.GLOBAL: 4>, 528),\n",
       "  530: (<ModeKeys.GLOBAL: 4>, 530),\n",
       "  532: (<ModeKeys.GLOBAL: 4>, 532),\n",
       "  534: (<ModeKeys.GLOBAL: 4>, 534),\n",
       "  536: (<ModeKeys.GLOBAL: 4>, 536),\n",
       "  538: (<ModeKeys.GLOBAL: 4>, 538),\n",
       "  540: (<ModeKeys.GLOBAL: 4>, 540),\n",
       "  542: (<ModeKeys.GLOBAL: 4>, 542),\n",
       "  544: (<ModeKeys.GLOBAL: 4>, 544),\n",
       "  546: (<ModeKeys.GLOBAL: 4>, 546),\n",
       "  548: (<ModeKeys.GLOBAL: 4>, 548),\n",
       "  550: (<ModeKeys.GLOBAL: 4>, 550),\n",
       "  552: (<ModeKeys.GLOBAL: 4>, 552),\n",
       "  554: (<ModeKeys.GLOBAL: 4>, 554),\n",
       "  556: (<ModeKeys.GLOBAL: 4>, 556),\n",
       "  558: (<ModeKeys.GLOBAL: 4>, 558),\n",
       "  560: (<ModeKeys.GLOBAL: 4>, 560),\n",
       "  562: (<ModeKeys.GLOBAL: 4>, 562),\n",
       "  564: (<ModeKeys.GLOBAL: 4>, 564),\n",
       "  566: (<ModeKeys.GLOBAL: 4>, 566),\n",
       "  568: (<ModeKeys.GLOBAL: 4>, 568),\n",
       "  570: (<ModeKeys.GLOBAL: 4>, 570),\n",
       "  572: (<ModeKeys.GLOBAL: 4>, 572),\n",
       "  574: (<ModeKeys.GLOBAL: 4>, 574),\n",
       "  576: (<ModeKeys.GLOBAL: 4>, 576),\n",
       "  578: (<ModeKeys.GLOBAL: 4>, 578),\n",
       "  580: (<ModeKeys.GLOBAL: 4>, 580),\n",
       "  582: (<ModeKeys.GLOBAL: 4>, 582),\n",
       "  584: (<ModeKeys.GLOBAL: 4>, 584),\n",
       "  586: (<ModeKeys.GLOBAL: 4>, 586),\n",
       "  588: (<ModeKeys.GLOBAL: 4>, 588),\n",
       "  590: (<ModeKeys.GLOBAL: 4>, 590),\n",
       "  592: (<ModeKeys.GLOBAL: 4>, 592),\n",
       "  594: (<ModeKeys.GLOBAL: 4>, 594),\n",
       "  596: (<ModeKeys.GLOBAL: 4>, 596),\n",
       "  598: (<ModeKeys.GLOBAL: 4>, 598),\n",
       "  600: (<ModeKeys.GLOBAL: 4>, 600),\n",
       "  602: (<ModeKeys.GLOBAL: 4>, 602),\n",
       "  604: (<ModeKeys.GLOBAL: 4>, 604),\n",
       "  606: (<ModeKeys.GLOBAL: 4>, 606),\n",
       "  608: (<ModeKeys.GLOBAL: 4>, 608),\n",
       "  610: (<ModeKeys.GLOBAL: 4>, 610),\n",
       "  612: (<ModeKeys.GLOBAL: 4>, 612),\n",
       "  614: (<ModeKeys.GLOBAL: 4>, 614),\n",
       "  616: (<ModeKeys.GLOBAL: 4>, 616),\n",
       "  618: (<ModeKeys.GLOBAL: 4>, 618),\n",
       "  620: (<ModeKeys.GLOBAL: 4>, 620),\n",
       "  622: (<ModeKeys.GLOBAL: 4>, 622),\n",
       "  624: (<ModeKeys.GLOBAL: 4>, 624),\n",
       "  626: (<ModeKeys.GLOBAL: 4>, 626),\n",
       "  628: (<ModeKeys.GLOBAL: 4>, 628),\n",
       "  630: (<ModeKeys.GLOBAL: 4>, 630),\n",
       "  632: (<ModeKeys.GLOBAL: 4>, 632),\n",
       "  634: (<ModeKeys.GLOBAL: 4>, 634),\n",
       "  636: (<ModeKeys.GLOBAL: 4>, 636),\n",
       "  638: (<ModeKeys.GLOBAL: 4>, 638),\n",
       "  640: (<ModeKeys.GLOBAL: 4>, 640),\n",
       "  642: (<ModeKeys.GLOBAL: 4>, 642),\n",
       "  644: (<ModeKeys.GLOBAL: 4>, 644),\n",
       "  646: (<ModeKeys.GLOBAL: 4>, 646),\n",
       "  648: (<ModeKeys.GLOBAL: 4>, 648),\n",
       "  650: (<ModeKeys.GLOBAL: 4>, 650),\n",
       "  652: (<ModeKeys.GLOBAL: 4>, 652),\n",
       "  654: (<ModeKeys.GLOBAL: 4>, 654),\n",
       "  656: (<ModeKeys.GLOBAL: 4>, 656),\n",
       "  658: (<ModeKeys.GLOBAL: 4>, 658),\n",
       "  660: (<ModeKeys.GLOBAL: 4>, 660),\n",
       "  662: (<ModeKeys.GLOBAL: 4>, 662),\n",
       "  664: (<ModeKeys.GLOBAL: 4>, 664),\n",
       "  666: (<ModeKeys.GLOBAL: 4>, 666),\n",
       "  668: (<ModeKeys.GLOBAL: 4>, 668),\n",
       "  670: (<ModeKeys.GLOBAL: 4>, 670),\n",
       "  672: (<ModeKeys.GLOBAL: 4>, 672),\n",
       "  674: (<ModeKeys.GLOBAL: 4>, 674),\n",
       "  676: (<ModeKeys.GLOBAL: 4>, 676),\n",
       "  678: (<ModeKeys.GLOBAL: 4>, 678),\n",
       "  680: (<ModeKeys.GLOBAL: 4>, 680),\n",
       "  682: (<ModeKeys.GLOBAL: 4>, 682),\n",
       "  684: (<ModeKeys.GLOBAL: 4>, 684),\n",
       "  686: (<ModeKeys.GLOBAL: 4>, 686),\n",
       "  688: (<ModeKeys.GLOBAL: 4>, 688),\n",
       "  690: (<ModeKeys.GLOBAL: 4>, 690),\n",
       "  692: (<ModeKeys.GLOBAL: 4>, 692),\n",
       "  694: (<ModeKeys.GLOBAL: 4>, 694),\n",
       "  696: (<ModeKeys.GLOBAL: 4>, 696),\n",
       "  698: (<ModeKeys.GLOBAL: 4>, 698),\n",
       "  700: (<ModeKeys.GLOBAL: 4>, 700),\n",
       "  702: (<ModeKeys.GLOBAL: 4>, 702),\n",
       "  704: (<ModeKeys.GLOBAL: 4>, 704),\n",
       "  706: (<ModeKeys.GLOBAL: 4>, 706),\n",
       "  708: (<ModeKeys.GLOBAL: 4>, 708),\n",
       "  710: (<ModeKeys.GLOBAL: 4>, 710),\n",
       "  712: (<ModeKeys.GLOBAL: 4>, 712),\n",
       "  714: (<ModeKeys.GLOBAL: 4>, 714),\n",
       "  716: (<ModeKeys.GLOBAL: 4>, 716),\n",
       "  718: (<ModeKeys.GLOBAL: 4>, 718),\n",
       "  720: (<ModeKeys.GLOBAL: 4>, 720),\n",
       "  722: (<ModeKeys.GLOBAL: 4>, 722),\n",
       "  724: (<ModeKeys.GLOBAL: 4>, 724),\n",
       "  726: (<ModeKeys.GLOBAL: 4>, 726),\n",
       "  728: (<ModeKeys.GLOBAL: 4>, 728),\n",
       "  730: (<ModeKeys.GLOBAL: 4>, 730),\n",
       "  732: (<ModeKeys.GLOBAL: 4>, 732),\n",
       "  734: (<ModeKeys.GLOBAL: 4>, 734),\n",
       "  736: (<ModeKeys.GLOBAL: 4>, 736),\n",
       "  738: (<ModeKeys.GLOBAL: 4>, 738),\n",
       "  740: (<ModeKeys.GLOBAL: 4>, 740),\n",
       "  742: (<ModeKeys.GLOBAL: 4>, 742),\n",
       "  744: (<ModeKeys.GLOBAL: 4>, 744),\n",
       "  746: (<ModeKeys.GLOBAL: 4>, 746),\n",
       "  748: (<ModeKeys.GLOBAL: 4>, 748),\n",
       "  750: (<ModeKeys.GLOBAL: 4>, 750),\n",
       "  752: (<ModeKeys.GLOBAL: 4>, 752),\n",
       "  754: (<ModeKeys.GLOBAL: 4>, 754),\n",
       "  756: (<ModeKeys.GLOBAL: 4>, 756),\n",
       "  758: (<ModeKeys.GLOBAL: 4>, 758),\n",
       "  760: (<ModeKeys.GLOBAL: 4>, 760),\n",
       "  762: (<ModeKeys.GLOBAL: 4>, 762),\n",
       "  764: (<ModeKeys.GLOBAL: 4>, 764),\n",
       "  766: (<ModeKeys.GLOBAL: 4>, 766),\n",
       "  768: (<ModeKeys.GLOBAL: 4>, 768),\n",
       "  770: (<ModeKeys.GLOBAL: 4>, 770),\n",
       "  772: (<ModeKeys.GLOBAL: 4>, 772),\n",
       "  774: (<ModeKeys.GLOBAL: 4>, 774),\n",
       "  776: (<ModeKeys.GLOBAL: 4>, 776),\n",
       "  778: (<ModeKeys.GLOBAL: 4>, 778),\n",
       "  780: (<ModeKeys.GLOBAL: 4>, 780),\n",
       "  782: (<ModeKeys.GLOBAL: 4>, 782),\n",
       "  784: (<ModeKeys.GLOBAL: 4>, 784),\n",
       "  786: (<ModeKeys.GLOBAL: 4>, 786),\n",
       "  788: (<ModeKeys.GLOBAL: 4>, 788),\n",
       "  790: (<ModeKeys.GLOBAL: 4>, 790),\n",
       "  792: (<ModeKeys.GLOBAL: 4>, 792),\n",
       "  794: (<ModeKeys.GLOBAL: 4>, 794),\n",
       "  796: (<ModeKeys.GLOBAL: 4>, 796),\n",
       "  798: (<ModeKeys.GLOBAL: 4>, 798),\n",
       "  800: (<ModeKeys.GLOBAL: 4>, 800),\n",
       "  802: (<ModeKeys.GLOBAL: 4>, 802),\n",
       "  804: (<ModeKeys.GLOBAL: 4>, 804),\n",
       "  806: (<ModeKeys.GLOBAL: 4>, 806),\n",
       "  808: (<ModeKeys.GLOBAL: 4>, 808),\n",
       "  810: (<ModeKeys.GLOBAL: 4>, 810),\n",
       "  812: (<ModeKeys.GLOBAL: 4>, 812),\n",
       "  814: (<ModeKeys.GLOBAL: 4>, 814),\n",
       "  816: (<ModeKeys.GLOBAL: 4>, 816),\n",
       "  818: (<ModeKeys.GLOBAL: 4>, 818),\n",
       "  820: (<ModeKeys.GLOBAL: 4>, 820),\n",
       "  822: (<ModeKeys.GLOBAL: 4>, 822),\n",
       "  824: (<ModeKeys.GLOBAL: 4>, 824),\n",
       "  826: (<ModeKeys.GLOBAL: 4>, 826),\n",
       "  828: (<ModeKeys.GLOBAL: 4>, 828),\n",
       "  830: (<ModeKeys.GLOBAL: 4>, 830),\n",
       "  832: (<ModeKeys.GLOBAL: 4>, 832),\n",
       "  834: (<ModeKeys.GLOBAL: 4>, 834),\n",
       "  836: (<ModeKeys.GLOBAL: 4>, 836),\n",
       "  838: (<ModeKeys.GLOBAL: 4>, 838),\n",
       "  840: (<ModeKeys.GLOBAL: 4>, 840),\n",
       "  842: (<ModeKeys.GLOBAL: 4>, 842),\n",
       "  844: (<ModeKeys.GLOBAL: 4>, 844),\n",
       "  846: (<ModeKeys.GLOBAL: 4>, 846),\n",
       "  848: (<ModeKeys.GLOBAL: 4>, 848),\n",
       "  850: (<ModeKeys.GLOBAL: 4>, 850),\n",
       "  852: (<ModeKeys.GLOBAL: 4>, 852),\n",
       "  854: (<ModeKeys.GLOBAL: 4>, 854),\n",
       "  856: (<ModeKeys.GLOBAL: 4>, 856),\n",
       "  858: (<ModeKeys.GLOBAL: 4>, 858),\n",
       "  860: (<ModeKeys.GLOBAL: 4>, 860),\n",
       "  862: (<ModeKeys.GLOBAL: 4>, 862),\n",
       "  864: (<ModeKeys.GLOBAL: 4>, 864),\n",
       "  866: (<ModeKeys.GLOBAL: 4>, 866),\n",
       "  868: (<ModeKeys.GLOBAL: 4>, 868),\n",
       "  870: (<ModeKeys.GLOBAL: 4>, 870),\n",
       "  872: (<ModeKeys.GLOBAL: 4>, 872),\n",
       "  874: (<ModeKeys.GLOBAL: 4>, 874),\n",
       "  876: (<ModeKeys.GLOBAL: 4>, 876),\n",
       "  878: (<ModeKeys.GLOBAL: 4>, 878),\n",
       "  880: (<ModeKeys.GLOBAL: 4>, 880),\n",
       "  882: (<ModeKeys.GLOBAL: 4>, 882),\n",
       "  884: (<ModeKeys.GLOBAL: 4>, 884),\n",
       "  886: (<ModeKeys.GLOBAL: 4>, 886),\n",
       "  888: (<ModeKeys.GLOBAL: 4>, 888),\n",
       "  890: (<ModeKeys.GLOBAL: 4>, 890),\n",
       "  892: (<ModeKeys.GLOBAL: 4>, 892),\n",
       "  894: (<ModeKeys.GLOBAL: 4>, 894),\n",
       "  896: (<ModeKeys.GLOBAL: 4>, 896),\n",
       "  898: (<ModeKeys.GLOBAL: 4>, 898),\n",
       "  900: (<ModeKeys.GLOBAL: 4>, 900),\n",
       "  902: (<ModeKeys.GLOBAL: 4>, 902),\n",
       "  904: (<ModeKeys.GLOBAL: 4>, 904),\n",
       "  906: (<ModeKeys.GLOBAL: 4>, 906),\n",
       "  908: (<ModeKeys.GLOBAL: 4>, 908),\n",
       "  910: (<ModeKeys.GLOBAL: 4>, 910),\n",
       "  912: (<ModeKeys.GLOBAL: 4>, 912),\n",
       "  914: (<ModeKeys.GLOBAL: 4>, 914),\n",
       "  916: (<ModeKeys.GLOBAL: 4>, 916),\n",
       "  918: (<ModeKeys.GLOBAL: 4>, 918),\n",
       "  920: (<ModeKeys.GLOBAL: 4>, 920),\n",
       "  922: (<ModeKeys.GLOBAL: 4>, 922),\n",
       "  924: (<ModeKeys.GLOBAL: 4>, 924),\n",
       "  926: (<ModeKeys.GLOBAL: 4>, 926),\n",
       "  928: (<ModeKeys.GLOBAL: 4>, 928),\n",
       "  930: (<ModeKeys.GLOBAL: 4>, 930),\n",
       "  932: (<ModeKeys.GLOBAL: 4>, 932),\n",
       "  934: (<ModeKeys.GLOBAL: 4>, 934),\n",
       "  936: (<ModeKeys.GLOBAL: 4>, 936),\n",
       "  938: (<ModeKeys.GLOBAL: 4>, 938),\n",
       "  940: (<ModeKeys.GLOBAL: 4>, 940),\n",
       "  942: (<ModeKeys.GLOBAL: 4>, 942),\n",
       "  944: (<ModeKeys.GLOBAL: 4>, 944),\n",
       "  946: (<ModeKeys.GLOBAL: 4>, 946),\n",
       "  948: (<ModeKeys.GLOBAL: 4>, 948),\n",
       "  950: (<ModeKeys.GLOBAL: 4>, 950),\n",
       "  952: (<ModeKeys.GLOBAL: 4>, 952),\n",
       "  954: (<ModeKeys.GLOBAL: 4>, 954),\n",
       "  956: (<ModeKeys.GLOBAL: 4>, 956),\n",
       "  958: (<ModeKeys.GLOBAL: 4>, 958),\n",
       "  960: (<ModeKeys.GLOBAL: 4>, 960),\n",
       "  962: (<ModeKeys.GLOBAL: 4>, 962),\n",
       "  964: (<ModeKeys.GLOBAL: 4>, 964),\n",
       "  966: (<ModeKeys.GLOBAL: 4>, 966),\n",
       "  968: (<ModeKeys.GLOBAL: 4>, 968),\n",
       "  970: (<ModeKeys.GLOBAL: 4>, 970),\n",
       "  972: (<ModeKeys.GLOBAL: 4>, 972),\n",
       "  974: (<ModeKeys.GLOBAL: 4>, 974),\n",
       "  976: (<ModeKeys.GLOBAL: 4>, 976),\n",
       "  978: (<ModeKeys.GLOBAL: 4>, 978),\n",
       "  980: (<ModeKeys.GLOBAL: 4>, 980),\n",
       "  982: (<ModeKeys.GLOBAL: 4>, 982),\n",
       "  984: (<ModeKeys.GLOBAL: 4>, 984),\n",
       "  986: (<ModeKeys.GLOBAL: 4>, 986),\n",
       "  988: (<ModeKeys.GLOBAL: 4>, 988),\n",
       "  990: (<ModeKeys.GLOBAL: 4>, 990),\n",
       "  992: (<ModeKeys.GLOBAL: 4>, 992),\n",
       "  994: (<ModeKeys.GLOBAL: 4>, 994),\n",
       "  996: (<ModeKeys.GLOBAL: 4>, 996),\n",
       "  998: (<ModeKeys.GLOBAL: 4>, 998),\n",
       "  1000: (<ModeKeys.GLOBAL: 4>, 1000),\n",
       "  1002: (<ModeKeys.GLOBAL: 4>, 1002),\n",
       "  1004: (<ModeKeys.GLOBAL: 4>, 1004),\n",
       "  1006: (<ModeKeys.GLOBAL: 4>, 1006),\n",
       "  1008: (<ModeKeys.GLOBAL: 4>, 1008),\n",
       "  1010: (<ModeKeys.GLOBAL: 4>, 1010),\n",
       "  1012: (<ModeKeys.GLOBAL: 4>, 1012),\n",
       "  1014: (<ModeKeys.GLOBAL: 4>, 1014),\n",
       "  1016: (<ModeKeys.GLOBAL: 4>, 1016),\n",
       "  1018: (<ModeKeys.GLOBAL: 4>, 1018),\n",
       "  1020: (<ModeKeys.GLOBAL: 4>, 1020),\n",
       "  1022: (<ModeKeys.GLOBAL: 4>, 1022),\n",
       "  1024: (<ModeKeys.GLOBAL: 4>, 1024),\n",
       "  1026: (<ModeKeys.GLOBAL: 4>, 1026),\n",
       "  1028: (<ModeKeys.GLOBAL: 4>, 1028),\n",
       "  1030: (<ModeKeys.GLOBAL: 4>, 1030),\n",
       "  1032: (<ModeKeys.GLOBAL: 4>, 1032),\n",
       "  1034: (<ModeKeys.GLOBAL: 4>, 1034),\n",
       "  1036: (<ModeKeys.GLOBAL: 4>, 1036),\n",
       "  1038: (<ModeKeys.GLOBAL: 4>, 1038),\n",
       "  1040: (<ModeKeys.GLOBAL: 4>, 1040),\n",
       "  1042: (<ModeKeys.GLOBAL: 4>, 1042),\n",
       "  1044: (<ModeKeys.GLOBAL: 4>, 1044),\n",
       "  1046: (<ModeKeys.GLOBAL: 4>, 1046),\n",
       "  1048: (<ModeKeys.GLOBAL: 4>, 1048),\n",
       "  1050: (<ModeKeys.GLOBAL: 4>, 1050),\n",
       "  1052: (<ModeKeys.GLOBAL: 4>, 1052),\n",
       "  1054: (<ModeKeys.GLOBAL: 4>, 1054),\n",
       "  1056: (<ModeKeys.GLOBAL: 4>, 1056),\n",
       "  1058: (<ModeKeys.GLOBAL: 4>, 1058),\n",
       "  1060: (<ModeKeys.GLOBAL: 4>, 1060),\n",
       "  1062: (<ModeKeys.GLOBAL: 4>, 1062),\n",
       "  1064: (<ModeKeys.GLOBAL: 4>, 1064),\n",
       "  1066: (<ModeKeys.GLOBAL: 4>, 1066),\n",
       "  1068: (<ModeKeys.GLOBAL: 4>, 1068),\n",
       "  1070: (<ModeKeys.GLOBAL: 4>, 1070),\n",
       "  1072: (<ModeKeys.GLOBAL: 4>, 1072),\n",
       "  1074: (<ModeKeys.GLOBAL: 4>, 1074),\n",
       "  1076: (<ModeKeys.GLOBAL: 4>, 1076),\n",
       "  1078: (<ModeKeys.GLOBAL: 4>, 1078),\n",
       "  1080: (<ModeKeys.GLOBAL: 4>, 1080),\n",
       "  1082: (<ModeKeys.GLOBAL: 4>, 1082),\n",
       "  1084: (<ModeKeys.GLOBAL: 4>, 1084),\n",
       "  1086: (<ModeKeys.GLOBAL: 4>, 1086),\n",
       "  1088: (<ModeKeys.GLOBAL: 4>, 1088),\n",
       "  1090: (<ModeKeys.GLOBAL: 4>, 1090),\n",
       "  1092: (<ModeKeys.GLOBAL: 4>, 1092),\n",
       "  1094: (<ModeKeys.GLOBAL: 4>, 1094),\n",
       "  1096: (<ModeKeys.GLOBAL: 4>, 1096),\n",
       "  1098: (<ModeKeys.GLOBAL: 4>, 1098),\n",
       "  1100: (<ModeKeys.GLOBAL: 4>, 1100),\n",
       "  1102: (<ModeKeys.GLOBAL: 4>, 1102),\n",
       "  1104: (<ModeKeys.GLOBAL: 4>, 1104),\n",
       "  1106: (<ModeKeys.GLOBAL: 4>, 1106),\n",
       "  1108: (<ModeKeys.GLOBAL: 4>, 1108),\n",
       "  1110: (<ModeKeys.GLOBAL: 4>, 1110),\n",
       "  1112: (<ModeKeys.GLOBAL: 4>, 1112),\n",
       "  1114: (<ModeKeys.GLOBAL: 4>, 1114),\n",
       "  1116: (<ModeKeys.GLOBAL: 4>, 1116),\n",
       "  1118: (<ModeKeys.GLOBAL: 4>, 1118),\n",
       "  1120: (<ModeKeys.GLOBAL: 4>, 1120),\n",
       "  1122: (<ModeKeys.GLOBAL: 4>, 1122),\n",
       "  1124: (<ModeKeys.GLOBAL: 4>, 1124),\n",
       "  1126: (<ModeKeys.GLOBAL: 4>, 1126),\n",
       "  1128: (<ModeKeys.GLOBAL: 4>, 1128),\n",
       "  1130: (<ModeKeys.GLOBAL: 4>, 1130),\n",
       "  1132: (<ModeKeys.GLOBAL: 4>, 1132),\n",
       "  1134: (<ModeKeys.GLOBAL: 4>, 1134),\n",
       "  1136: (<ModeKeys.GLOBAL: 4>, 1136),\n",
       "  1138: (<ModeKeys.GLOBAL: 4>, 1138),\n",
       "  1140: (<ModeKeys.GLOBAL: 4>, 1140),\n",
       "  1142: (<ModeKeys.GLOBAL: 4>, 1142),\n",
       "  1144: (<ModeKeys.GLOBAL: 4>, 1144),\n",
       "  1146: (<ModeKeys.GLOBAL: 4>, 1146),\n",
       "  1148: (<ModeKeys.GLOBAL: 4>, 1148),\n",
       "  1150: (<ModeKeys.GLOBAL: 4>, 1150),\n",
       "  1152: (<ModeKeys.GLOBAL: 4>, 1152),\n",
       "  1154: (<ModeKeys.GLOBAL: 4>, 1154),\n",
       "  1156: (<ModeKeys.GLOBAL: 4>, 1156),\n",
       "  1158: (<ModeKeys.GLOBAL: 4>, 1158),\n",
       "  1160: (<ModeKeys.GLOBAL: 4>, 1160),\n",
       "  1162: (<ModeKeys.GLOBAL: 4>, 1162),\n",
       "  1164: (<ModeKeys.GLOBAL: 4>, 1164),\n",
       "  1166: (<ModeKeys.GLOBAL: 4>, 1166),\n",
       "  1168: (<ModeKeys.GLOBAL: 4>, 1168),\n",
       "  1170: (<ModeKeys.GLOBAL: 4>, 1170),\n",
       "  1172: (<ModeKeys.GLOBAL: 4>, 1172),\n",
       "  1174: (<ModeKeys.GLOBAL: 4>, 1174),\n",
       "  1176: (<ModeKeys.GLOBAL: 4>, 1176),\n",
       "  1178: (<ModeKeys.GLOBAL: 4>, 1178),\n",
       "  1180: (<ModeKeys.GLOBAL: 4>, 1180),\n",
       "  1182: (<ModeKeys.GLOBAL: 4>, 1182),\n",
       "  1184: (<ModeKeys.GLOBAL: 4>, 1184),\n",
       "  1186: (<ModeKeys.GLOBAL: 4>, 1186),\n",
       "  1188: (<ModeKeys.GLOBAL: 4>, 1188),\n",
       "  1190: (<ModeKeys.GLOBAL: 4>, 1190),\n",
       "  1192: (<ModeKeys.GLOBAL: 4>, 1192),\n",
       "  1194: (<ModeKeys.GLOBAL: 4>, 1194),\n",
       "  1196: (<ModeKeys.GLOBAL: 4>, 1196),\n",
       "  1198: (<ModeKeys.GLOBAL: 4>, 1198),\n",
       "  1200: (<ModeKeys.GLOBAL: 4>, 1200),\n",
       "  1202: (<ModeKeys.GLOBAL: 4>, 1202),\n",
       "  1204: (<ModeKeys.GLOBAL: 4>, 1204),\n",
       "  1206: (<ModeKeys.GLOBAL: 4>, 1206),\n",
       "  1208: (<ModeKeys.GLOBAL: 4>, 1208),\n",
       "  1210: (<ModeKeys.GLOBAL: 4>, 1210),\n",
       "  1212: (<ModeKeys.GLOBAL: 4>, 1212),\n",
       "  1214: (<ModeKeys.GLOBAL: 4>, 1214),\n",
       "  1216: (<ModeKeys.GLOBAL: 4>, 1216),\n",
       "  1218: (<ModeKeys.GLOBAL: 4>, 1218),\n",
       "  1220: (<ModeKeys.GLOBAL: 4>, 1220),\n",
       "  1222: (<ModeKeys.GLOBAL: 4>, 1222),\n",
       "  1224: (<ModeKeys.GLOBAL: 4>, 1224),\n",
       "  1226: (<ModeKeys.GLOBAL: 4>, 1226),\n",
       "  1228: (<ModeKeys.GLOBAL: 4>, 1228),\n",
       "  1230: (<ModeKeys.GLOBAL: 4>, 1230),\n",
       "  1232: (<ModeKeys.GLOBAL: 4>, 1232),\n",
       "  1234: (<ModeKeys.GLOBAL: 4>, 1234),\n",
       "  1236: (<ModeKeys.GLOBAL: 4>, 1236),\n",
       "  1238: (<ModeKeys.GLOBAL: 4>, 1238),\n",
       "  1240: (<ModeKeys.GLOBAL: 4>, 1240),\n",
       "  1242: (<ModeKeys.GLOBAL: 4>, 1242),\n",
       "  1244: (<ModeKeys.GLOBAL: 4>, 1244),\n",
       "  1246: (<ModeKeys.GLOBAL: 4>, 1246),\n",
       "  1248: (<ModeKeys.GLOBAL: 4>, 1248),\n",
       "  1250: (<ModeKeys.GLOBAL: 4>, 1250),\n",
       "  1252: (<ModeKeys.GLOBAL: 4>, 1252),\n",
       "  1254: (<ModeKeys.GLOBAL: 4>, 1254),\n",
       "  1256: (<ModeKeys.GLOBAL: 4>, 1256),\n",
       "  1258: (<ModeKeys.GLOBAL: 4>, 1258),\n",
       "  1260: (<ModeKeys.GLOBAL: 4>, 1260),\n",
       "  1262: (<ModeKeys.GLOBAL: 4>, 1262),\n",
       "  1264: (<ModeKeys.GLOBAL: 4>, 1264),\n",
       "  1266: (<ModeKeys.GLOBAL: 4>, 1266),\n",
       "  1268: (<ModeKeys.GLOBAL: 4>, 1268),\n",
       "  1270: (<ModeKeys.GLOBAL: 4>, 1270),\n",
       "  1272: (<ModeKeys.GLOBAL: 4>, 1272),\n",
       "  1274: (<ModeKeys.GLOBAL: 4>, 1274),\n",
       "  1276: (<ModeKeys.GLOBAL: 4>, 1276),\n",
       "  1278: (<ModeKeys.GLOBAL: 4>, 1278),\n",
       "  1280: (<ModeKeys.GLOBAL: 4>, 1280),\n",
       "  1282: (<ModeKeys.GLOBAL: 4>, 1282),\n",
       "  1284: (<ModeKeys.GLOBAL: 4>, 1284),\n",
       "  1286: (<ModeKeys.GLOBAL: 4>, 1286),\n",
       "  1288: (<ModeKeys.GLOBAL: 4>, 1288),\n",
       "  1290: (<ModeKeys.GLOBAL: 4>, 1290),\n",
       "  1292: (<ModeKeys.GLOBAL: 4>, 1292),\n",
       "  1294: (<ModeKeys.GLOBAL: 4>, 1294),\n",
       "  1296: (<ModeKeys.GLOBAL: 4>, 1296),\n",
       "  1298: (<ModeKeys.GLOBAL: 4>, 1298),\n",
       "  1300: (<ModeKeys.GLOBAL: 4>, 1300),\n",
       "  1302: (<ModeKeys.GLOBAL: 4>, 1302),\n",
       "  1304: (<ModeKeys.GLOBAL: 4>, 1304),\n",
       "  1306: (<ModeKeys.GLOBAL: 4>, 1306),\n",
       "  1308: (<ModeKeys.GLOBAL: 4>, 1308),\n",
       "  1310: (<ModeKeys.GLOBAL: 4>, 1310),\n",
       "  1312: (<ModeKeys.GLOBAL: 4>, 1312),\n",
       "  1314: (<ModeKeys.GLOBAL: 4>, 1314),\n",
       "  1316: (<ModeKeys.GLOBAL: 4>, 1316),\n",
       "  1318: (<ModeKeys.GLOBAL: 4>, 1318),\n",
       "  1320: (<ModeKeys.GLOBAL: 4>, 1320),\n",
       "  1322: (<ModeKeys.GLOBAL: 4>, 1322),\n",
       "  1324: (<ModeKeys.GLOBAL: 4>, 1324),\n",
       "  1326: (<ModeKeys.GLOBAL: 4>, 1326),\n",
       "  1328: (<ModeKeys.GLOBAL: 4>, 1328),\n",
       "  1330: (<ModeKeys.GLOBAL: 4>, 1330),\n",
       "  1332: (<ModeKeys.GLOBAL: 4>, 1332),\n",
       "  1334: (<ModeKeys.GLOBAL: 4>, 1334),\n",
       "  1336: (<ModeKeys.GLOBAL: 4>, 1336),\n",
       "  1338: (<ModeKeys.GLOBAL: 4>, 1338),\n",
       "  1340: (<ModeKeys.GLOBAL: 4>, 1340),\n",
       "  1342: (<ModeKeys.GLOBAL: 4>, 1342),\n",
       "  1344: (<ModeKeys.GLOBAL: 4>, 1344),\n",
       "  1346: (<ModeKeys.GLOBAL: 4>, 1346),\n",
       "  1348: (<ModeKeys.GLOBAL: 4>, 1348),\n",
       "  1350: (<ModeKeys.GLOBAL: 4>, 1350),\n",
       "  1352: (<ModeKeys.GLOBAL: 4>, 1352),\n",
       "  1354: (<ModeKeys.GLOBAL: 4>, 1354),\n",
       "  1356: (<ModeKeys.GLOBAL: 4>, 1356),\n",
       "  1358: (<ModeKeys.GLOBAL: 4>, 1358),\n",
       "  1360: (<ModeKeys.GLOBAL: 4>, 1360),\n",
       "  1362: (<ModeKeys.GLOBAL: 4>, 1362),\n",
       "  1364: (<ModeKeys.GLOBAL: 4>, 1364),\n",
       "  1366: (<ModeKeys.GLOBAL: 4>, 1366),\n",
       "  1368: (<ModeKeys.GLOBAL: 4>, 1368),\n",
       "  1370: (<ModeKeys.GLOBAL: 4>, 1370),\n",
       "  1372: (<ModeKeys.GLOBAL: 4>, 1372),\n",
       "  1374: (<ModeKeys.GLOBAL: 4>, 1374),\n",
       "  1376: (<ModeKeys.GLOBAL: 4>, 1376),\n",
       "  1378: (<ModeKeys.GLOBAL: 4>, 1378),\n",
       "  1380: (<ModeKeys.GLOBAL: 4>, 1380),\n",
       "  1382: (<ModeKeys.GLOBAL: 4>, 1382),\n",
       "  1384: (<ModeKeys.GLOBAL: 4>, 1384),\n",
       "  1386: (<ModeKeys.GLOBAL: 4>, 1386),\n",
       "  1388: (<ModeKeys.GLOBAL: 4>, 1388),\n",
       "  1390: (<ModeKeys.GLOBAL: 4>, 1390),\n",
       "  1392: (<ModeKeys.GLOBAL: 4>, 1392),\n",
       "  1394: (<ModeKeys.GLOBAL: 4>, 1394),\n",
       "  1396: (<ModeKeys.GLOBAL: 4>, 1396),\n",
       "  1398: (<ModeKeys.GLOBAL: 4>, 1398),\n",
       "  1400: (<ModeKeys.GLOBAL: 4>, 1400),\n",
       "  1402: (<ModeKeys.GLOBAL: 4>, 1402),\n",
       "  1404: (<ModeKeys.GLOBAL: 4>, 1404),\n",
       "  1406: (<ModeKeys.GLOBAL: 4>, 1406),\n",
       "  1408: (<ModeKeys.GLOBAL: 4>, 1408),\n",
       "  1410: (<ModeKeys.GLOBAL: 4>, 1410),\n",
       "  1412: (<ModeKeys.GLOBAL: 4>, 1412),\n",
       "  1414: (<ModeKeys.GLOBAL: 4>, 1414),\n",
       "  1416: (<ModeKeys.GLOBAL: 4>, 1416),\n",
       "  1418: (<ModeKeys.GLOBAL: 4>, 1418),\n",
       "  1420: (<ModeKeys.GLOBAL: 4>, 1420),\n",
       "  1422: (<ModeKeys.GLOBAL: 4>, 1422),\n",
       "  1424: (<ModeKeys.GLOBAL: 4>, 1424),\n",
       "  1426: (<ModeKeys.GLOBAL: 4>, 1426),\n",
       "  1428: (<ModeKeys.GLOBAL: 4>, 1428),\n",
       "  1430: (<ModeKeys.GLOBAL: 4>, 1430),\n",
       "  1432: (<ModeKeys.GLOBAL: 4>, 1432),\n",
       "  1434: (<ModeKeys.GLOBAL: 4>, 1434),\n",
       "  1436: (<ModeKeys.GLOBAL: 4>, 1436),\n",
       "  1438: (<ModeKeys.GLOBAL: 4>, 1438),\n",
       "  1440: (<ModeKeys.GLOBAL: 4>, 1440),\n",
       "  1442: (<ModeKeys.GLOBAL: 4>, 1442),\n",
       "  1444: (<ModeKeys.GLOBAL: 4>, 1444),\n",
       "  1446: (<ModeKeys.GLOBAL: 4>, 1446),\n",
       "  1448: (<ModeKeys.GLOBAL: 4>, 1448),\n",
       "  1450: (<ModeKeys.GLOBAL: 4>, 1450),\n",
       "  1452: (<ModeKeys.GLOBAL: 4>, 1452),\n",
       "  1454: (<ModeKeys.GLOBAL: 4>, 1454),\n",
       "  1456: (<ModeKeys.GLOBAL: 4>, 1456),\n",
       "  1458: (<ModeKeys.GLOBAL: 4>, 1458),\n",
       "  1460: (<ModeKeys.GLOBAL: 4>, 1460),\n",
       "  1462: (<ModeKeys.GLOBAL: 4>, 1462),\n",
       "  1464: (<ModeKeys.GLOBAL: 4>, 1464),\n",
       "  1466: (<ModeKeys.GLOBAL: 4>, 1466),\n",
       "  1468: (<ModeKeys.GLOBAL: 4>, 1468),\n",
       "  1470: (<ModeKeys.GLOBAL: 4>, 1470),\n",
       "  1472: (<ModeKeys.GLOBAL: 4>, 1472),\n",
       "  1474: (<ModeKeys.GLOBAL: 4>, 1474),\n",
       "  1476: (<ModeKeys.GLOBAL: 4>, 1476),\n",
       "  1478: (<ModeKeys.GLOBAL: 4>, 1478),\n",
       "  1480: (<ModeKeys.GLOBAL: 4>, 1480),\n",
       "  1482: (<ModeKeys.GLOBAL: 4>, 1482),\n",
       "  1484: (<ModeKeys.GLOBAL: 4>, 1484),\n",
       "  1486: (<ModeKeys.GLOBAL: 4>, 1486),\n",
       "  1488: (<ModeKeys.GLOBAL: 4>, 1488),\n",
       "  1490: (<ModeKeys.GLOBAL: 4>, 1490),\n",
       "  1492: (<ModeKeys.GLOBAL: 4>, 1492),\n",
       "  1494: (<ModeKeys.GLOBAL: 4>, 1494),\n",
       "  1496: (<ModeKeys.GLOBAL: 4>, 1496),\n",
       "  1498: (<ModeKeys.GLOBAL: 4>, 1498),\n",
       "  1500: (<ModeKeys.GLOBAL: 4>, 1500),\n",
       "  1502: (<ModeKeys.GLOBAL: 4>, 1502),\n",
       "  1504: (<ModeKeys.GLOBAL: 4>, 1504),\n",
       "  1506: (<ModeKeys.GLOBAL: 4>, 1506),\n",
       "  1508: (<ModeKeys.GLOBAL: 4>, 1508),\n",
       "  1510: (<ModeKeys.GLOBAL: 4>, 1510),\n",
       "  1512: (<ModeKeys.GLOBAL: 4>, 1512),\n",
       "  1514: (<ModeKeys.GLOBAL: 4>, 1514),\n",
       "  1516: (<ModeKeys.GLOBAL: 4>, 1516),\n",
       "  1518: (<ModeKeys.GLOBAL: 4>, 1518),\n",
       "  1520: (<ModeKeys.GLOBAL: 4>, 1520),\n",
       "  1522: (<ModeKeys.GLOBAL: 4>, 1522),\n",
       "  1524: (<ModeKeys.GLOBAL: 4>, 1524),\n",
       "  1526: (<ModeKeys.GLOBAL: 4>, 1526),\n",
       "  1528: (<ModeKeys.GLOBAL: 4>, 1528),\n",
       "  1530: (<ModeKeys.GLOBAL: 4>, 1530),\n",
       "  1532: (<ModeKeys.GLOBAL: 4>, 1532),\n",
       "  1534: (<ModeKeys.GLOBAL: 4>, 1534),\n",
       "  1536: (<ModeKeys.GLOBAL: 4>, 1536),\n",
       "  1538: (<ModeKeys.GLOBAL: 4>, 1538),\n",
       "  1540: (<ModeKeys.GLOBAL: 4>, 1540),\n",
       "  1542: (<ModeKeys.GLOBAL: 4>, 1542),\n",
       "  1544: (<ModeKeys.GLOBAL: 4>, 1544),\n",
       "  1546: (<ModeKeys.GLOBAL: 4>, 1546),\n",
       "  1548: (<ModeKeys.GLOBAL: 4>, 1548),\n",
       "  1550: (<ModeKeys.GLOBAL: 4>, 1550),\n",
       "  1552: (<ModeKeys.GLOBAL: 4>, 1552),\n",
       "  1554: (<ModeKeys.GLOBAL: 4>, 1554),\n",
       "  1556: (<ModeKeys.GLOBAL: 4>, 1556),\n",
       "  1558: (<ModeKeys.GLOBAL: 4>, 1558),\n",
       "  1560: (<ModeKeys.GLOBAL: 4>, 1560),\n",
       "  1562: (<ModeKeys.GLOBAL: 4>, 1562),\n",
       "  1564: (<ModeKeys.GLOBAL: 4>, 1564),\n",
       "  1566: (<ModeKeys.GLOBAL: 4>, 1566),\n",
       "  1568: (<ModeKeys.GLOBAL: 4>, 1568),\n",
       "  1570: (<ModeKeys.GLOBAL: 4>, 1570),\n",
       "  1572: (<ModeKeys.GLOBAL: 4>, 1572),\n",
       "  1574: (<ModeKeys.GLOBAL: 4>, 1574),\n",
       "  1576: (<ModeKeys.GLOBAL: 4>, 1576),\n",
       "  1578: (<ModeKeys.GLOBAL: 4>, 1578),\n",
       "  1580: (<ModeKeys.GLOBAL: 4>, 1580),\n",
       "  1582: (<ModeKeys.GLOBAL: 4>, 1582),\n",
       "  1584: (<ModeKeys.GLOBAL: 4>, 1584),\n",
       "  1586: (<ModeKeys.GLOBAL: 4>, 1586),\n",
       "  1588: (<ModeKeys.GLOBAL: 4>, 1588),\n",
       "  1590: (<ModeKeys.GLOBAL: 4>, 1590),\n",
       "  1592: (<ModeKeys.GLOBAL: 4>, 1592),\n",
       "  1594: (<ModeKeys.GLOBAL: 4>, 1594),\n",
       "  1596: (<ModeKeys.GLOBAL: 4>, 1596),\n",
       "  1598: (<ModeKeys.GLOBAL: 4>, 1598),\n",
       "  1600: (<ModeKeys.GLOBAL: 4>, 1600),\n",
       "  1602: (<ModeKeys.GLOBAL: 4>, 1602),\n",
       "  1604: (<ModeKeys.GLOBAL: 4>, 1604),\n",
       "  1606: (<ModeKeys.GLOBAL: 4>, 1606),\n",
       "  1608: (<ModeKeys.GLOBAL: 4>, 1608),\n",
       "  1610: (<ModeKeys.GLOBAL: 4>, 1610),\n",
       "  1612: (<ModeKeys.GLOBAL: 4>, 1612),\n",
       "  1614: (<ModeKeys.GLOBAL: 4>, 1614),\n",
       "  1616: (<ModeKeys.GLOBAL: 4>, 1616),\n",
       "  1618: (<ModeKeys.GLOBAL: 4>, 1618),\n",
       "  1620: (<ModeKeys.GLOBAL: 4>, 1620),\n",
       "  1622: (<ModeKeys.GLOBAL: 4>, 1622),\n",
       "  1624: (<ModeKeys.GLOBAL: 4>, 1624),\n",
       "  1626: (<ModeKeys.GLOBAL: 4>, 1626),\n",
       "  1628: (<ModeKeys.GLOBAL: 4>, 1628),\n",
       "  1630: (<ModeKeys.GLOBAL: 4>, 1630),\n",
       "  1632: (<ModeKeys.GLOBAL: 4>, 1632),\n",
       "  1634: (<ModeKeys.GLOBAL: 4>, 1634),\n",
       "  1636: (<ModeKeys.GLOBAL: 4>, 1636),\n",
       "  1638: (<ModeKeys.GLOBAL: 4>, 1638),\n",
       "  1640: (<ModeKeys.GLOBAL: 4>, 1640),\n",
       "  1642: (<ModeKeys.GLOBAL: 4>, 1642),\n",
       "  1644: (<ModeKeys.GLOBAL: 4>, 1644),\n",
       "  1646: (<ModeKeys.GLOBAL: 4>, 1646),\n",
       "  1648: (<ModeKeys.GLOBAL: 4>, 1648),\n",
       "  1650: (<ModeKeys.GLOBAL: 4>, 1650),\n",
       "  1652: (<ModeKeys.GLOBAL: 4>, 1652),\n",
       "  1654: (<ModeKeys.GLOBAL: 4>, 1654),\n",
       "  1656: (<ModeKeys.GLOBAL: 4>, 1656),\n",
       "  1658: (<ModeKeys.GLOBAL: 4>, 1658),\n",
       "  1660: (<ModeKeys.GLOBAL: 4>, 1660),\n",
       "  1662: (<ModeKeys.GLOBAL: 4>, 1662),\n",
       "  1664: (<ModeKeys.GLOBAL: 4>, 1664),\n",
       "  1666: (<ModeKeys.GLOBAL: 4>, 1666),\n",
       "  1668: (<ModeKeys.GLOBAL: 4>, 1668),\n",
       "  1670: (<ModeKeys.GLOBAL: 4>, 1670),\n",
       "  1672: (<ModeKeys.GLOBAL: 4>, 1672),\n",
       "  1674: (<ModeKeys.GLOBAL: 4>, 1674),\n",
       "  1676: (<ModeKeys.GLOBAL: 4>, 1676),\n",
       "  1678: (<ModeKeys.GLOBAL: 4>, 1678),\n",
       "  1680: (<ModeKeys.GLOBAL: 4>, 1680),\n",
       "  1682: (<ModeKeys.GLOBAL: 4>, 1682),\n",
       "  1684: (<ModeKeys.GLOBAL: 4>, 1684),\n",
       "  1686: (<ModeKeys.GLOBAL: 4>, 1686),\n",
       "  1688: (<ModeKeys.GLOBAL: 4>, 1688),\n",
       "  1690: (<ModeKeys.GLOBAL: 4>, 1690),\n",
       "  1692: (<ModeKeys.GLOBAL: 4>, 1692),\n",
       "  1694: (<ModeKeys.GLOBAL: 4>, 1694),\n",
       "  1696: (<ModeKeys.GLOBAL: 4>, 1696),\n",
       "  1698: (<ModeKeys.GLOBAL: 4>, 1698),\n",
       "  1700: (<ModeKeys.GLOBAL: 4>, 1700),\n",
       "  1702: (<ModeKeys.GLOBAL: 4>, 1702),\n",
       "  1704: (<ModeKeys.GLOBAL: 4>, 1704),\n",
       "  1706: (<ModeKeys.GLOBAL: 4>, 1706),\n",
       "  1708: (<ModeKeys.GLOBAL: 4>, 1708),\n",
       "  1710: (<ModeKeys.GLOBAL: 4>, 1710),\n",
       "  1712: (<ModeKeys.GLOBAL: 4>, 1712),\n",
       "  1714: (<ModeKeys.GLOBAL: 4>, 1714),\n",
       "  1716: (<ModeKeys.GLOBAL: 4>, 1716),\n",
       "  1718: (<ModeKeys.GLOBAL: 4>, 1718),\n",
       "  1720: (<ModeKeys.GLOBAL: 4>, 1720),\n",
       "  1722: (<ModeKeys.GLOBAL: 4>, 1722),\n",
       "  1724: (<ModeKeys.GLOBAL: 4>, 1724),\n",
       "  1726: (<ModeKeys.GLOBAL: 4>, 1726),\n",
       "  1728: (<ModeKeys.GLOBAL: 4>, 1728),\n",
       "  1730: (<ModeKeys.GLOBAL: 4>, 1730),\n",
       "  1732: (<ModeKeys.GLOBAL: 4>, 1732),\n",
       "  1734: (<ModeKeys.GLOBAL: 4>, 1734),\n",
       "  1736: (<ModeKeys.GLOBAL: 4>, 1736),\n",
       "  1738: (<ModeKeys.GLOBAL: 4>, 1738),\n",
       "  1740: (<ModeKeys.GLOBAL: 4>, 1740),\n",
       "  1742: (<ModeKeys.GLOBAL: 4>, 1742),\n",
       "  1744: (<ModeKeys.GLOBAL: 4>, 1744),\n",
       "  1746: (<ModeKeys.GLOBAL: 4>, 1746),\n",
       "  1748: (<ModeKeys.GLOBAL: 4>, 1748),\n",
       "  1750: (<ModeKeys.GLOBAL: 4>, 1750),\n",
       "  1752: (<ModeKeys.GLOBAL: 4>, 1752),\n",
       "  1754: (<ModeKeys.GLOBAL: 4>, 1754),\n",
       "  1756: (<ModeKeys.GLOBAL: 4>, 1756),\n",
       "  1758: (<ModeKeys.GLOBAL: 4>, 1758),\n",
       "  1760: (<ModeKeys.GLOBAL: 4>, 1760),\n",
       "  1762: (<ModeKeys.GLOBAL: 4>, 1762),\n",
       "  1764: (<ModeKeys.GLOBAL: 4>, 1764),\n",
       "  1766: (<ModeKeys.GLOBAL: 4>, 1766),\n",
       "  1768: (<ModeKeys.GLOBAL: 4>, 1768),\n",
       "  1770: (<ModeKeys.GLOBAL: 4>, 1770),\n",
       "  1772: (<ModeKeys.GLOBAL: 4>, 1772),\n",
       "  1774: (<ModeKeys.GLOBAL: 4>, 1774),\n",
       "  1776: (<ModeKeys.GLOBAL: 4>, 1776),\n",
       "  1778: (<ModeKeys.GLOBAL: 4>, 1778),\n",
       "  1780: (<ModeKeys.GLOBAL: 4>, 1780),\n",
       "  1782: (<ModeKeys.GLOBAL: 4>, 1782),\n",
       "  1784: (<ModeKeys.GLOBAL: 4>, 1784),\n",
       "  1786: (<ModeKeys.GLOBAL: 4>, 1786),\n",
       "  1788: (<ModeKeys.GLOBAL: 4>, 1788),\n",
       "  1790: (<ModeKeys.GLOBAL: 4>, 1790),\n",
       "  1792: (<ModeKeys.GLOBAL: 4>, 1792),\n",
       "  1794: (<ModeKeys.GLOBAL: 4>, 1794),\n",
       "  1796: (<ModeKeys.GLOBAL: 4>, 1796),\n",
       "  1798: (<ModeKeys.GLOBAL: 4>, 1798),\n",
       "  1800: (<ModeKeys.GLOBAL: 4>, 1800),\n",
       "  1802: (<ModeKeys.GLOBAL: 4>, 1802),\n",
       "  1804: (<ModeKeys.GLOBAL: 4>, 1804),\n",
       "  1806: (<ModeKeys.GLOBAL: 4>, 1806),\n",
       "  1808: (<ModeKeys.GLOBAL: 4>, 1808),\n",
       "  1810: (<ModeKeys.GLOBAL: 4>, 1810),\n",
       "  1812: (<ModeKeys.GLOBAL: 4>, 1812),\n",
       "  1814: (<ModeKeys.GLOBAL: 4>, 1814),\n",
       "  1816: (<ModeKeys.GLOBAL: 4>, 1816),\n",
       "  1818: (<ModeKeys.GLOBAL: 4>, 1818),\n",
       "  1820: (<ModeKeys.GLOBAL: 4>, 1820),\n",
       "  1822: (<ModeKeys.GLOBAL: 4>, 1822),\n",
       "  1824: (<ModeKeys.GLOBAL: 4>, 1824),\n",
       "  1826: (<ModeKeys.GLOBAL: 4>, 1826),\n",
       "  1828: (<ModeKeys.GLOBAL: 4>, 1828),\n",
       "  1830: (<ModeKeys.GLOBAL: 4>, 1830),\n",
       "  1832: (<ModeKeys.GLOBAL: 4>, 1832),\n",
       "  1834: (<ModeKeys.GLOBAL: 4>, 1834),\n",
       "  1836: (<ModeKeys.GLOBAL: 4>, 1836),\n",
       "  1838: (<ModeKeys.GLOBAL: 4>, 1838),\n",
       "  1840: (<ModeKeys.GLOBAL: 4>, 1840),\n",
       "  1842: (<ModeKeys.GLOBAL: 4>, 1842),\n",
       "  1844: (<ModeKeys.GLOBAL: 4>, 1844),\n",
       "  1846: (<ModeKeys.GLOBAL: 4>, 1846),\n",
       "  1848: (<ModeKeys.GLOBAL: 4>, 1848),\n",
       "  1850: (<ModeKeys.GLOBAL: 4>, 1850),\n",
       "  1852: (<ModeKeys.GLOBAL: 4>, 1852),\n",
       "  1854: (<ModeKeys.GLOBAL: 4>, 1854),\n",
       "  1856: (<ModeKeys.GLOBAL: 4>, 1856),\n",
       "  1858: (<ModeKeys.GLOBAL: 4>, 1858),\n",
       "  1860: (<ModeKeys.GLOBAL: 4>, 1860),\n",
       "  1862: (<ModeKeys.GLOBAL: 4>, 1862),\n",
       "  1864: (<ModeKeys.GLOBAL: 4>, 1864),\n",
       "  1866: (<ModeKeys.GLOBAL: 4>, 1866),\n",
       "  1868: (<ModeKeys.GLOBAL: 4>, 1868),\n",
       "  1870: (<ModeKeys.GLOBAL: 4>, 1870),\n",
       "  1872: (<ModeKeys.GLOBAL: 4>, 1872),\n",
       "  1874: (<ModeKeys.GLOBAL: 4>, 1874),\n",
       "  1876: (<ModeKeys.GLOBAL: 4>, 1876),\n",
       "  1878: (<ModeKeys.GLOBAL: 4>, 1878),\n",
       "  1880: (<ModeKeys.GLOBAL: 4>, 1880),\n",
       "  1882: (<ModeKeys.GLOBAL: 4>, 1882),\n",
       "  1884: (<ModeKeys.GLOBAL: 4>, 1884),\n",
       "  1886: (<ModeKeys.GLOBAL: 4>, 1886),\n",
       "  1888: (<ModeKeys.GLOBAL: 4>, 1888),\n",
       "  1890: (<ModeKeys.GLOBAL: 4>, 1890),\n",
       "  1892: (<ModeKeys.GLOBAL: 4>, 1892),\n",
       "  1894: (<ModeKeys.GLOBAL: 4>, 1894),\n",
       "  1896: (<ModeKeys.GLOBAL: 4>, 1896),\n",
       "  1898: (<ModeKeys.GLOBAL: 4>, 1898),\n",
       "  1900: (<ModeKeys.GLOBAL: 4>, 1900),\n",
       "  1902: (<ModeKeys.GLOBAL: 4>, 1902),\n",
       "  1904: (<ModeKeys.GLOBAL: 4>, 1904),\n",
       "  1906: (<ModeKeys.GLOBAL: 4>, 1906),\n",
       "  1908: (<ModeKeys.GLOBAL: 4>, 1908),\n",
       "  1910: (<ModeKeys.GLOBAL: 4>, 1910),\n",
       "  1912: (<ModeKeys.GLOBAL: 4>, 1912),\n",
       "  1914: (<ModeKeys.GLOBAL: 4>, 1914),\n",
       "  1916: (<ModeKeys.GLOBAL: 4>, 1916),\n",
       "  1918: (<ModeKeys.GLOBAL: 4>, 1918),\n",
       "  1920: (<ModeKeys.GLOBAL: 4>, 1920),\n",
       "  1922: (<ModeKeys.GLOBAL: 4>, 1922),\n",
       "  1924: (<ModeKeys.GLOBAL: 4>, 1924),\n",
       "  1926: (<ModeKeys.GLOBAL: 4>, 1926),\n",
       "  1928: (<ModeKeys.GLOBAL: 4>, 1928),\n",
       "  1930: (<ModeKeys.GLOBAL: 4>, 1930),\n",
       "  1932: (<ModeKeys.GLOBAL: 4>, 1932),\n",
       "  1934: (<ModeKeys.GLOBAL: 4>, 1934),\n",
       "  1936: (<ModeKeys.GLOBAL: 4>, 1936),\n",
       "  1938: (<ModeKeys.GLOBAL: 4>, 1938),\n",
       "  1940: (<ModeKeys.GLOBAL: 4>, 1940),\n",
       "  1942: (<ModeKeys.GLOBAL: 4>, 1942),\n",
       "  1944: (<ModeKeys.GLOBAL: 4>, 1944),\n",
       "  1946: (<ModeKeys.GLOBAL: 4>, 1946),\n",
       "  1948: (<ModeKeys.GLOBAL: 4>, 1948),\n",
       "  1950: (<ModeKeys.GLOBAL: 4>, 1950),\n",
       "  1952: (<ModeKeys.GLOBAL: 4>, 1952),\n",
       "  1954: (<ModeKeys.GLOBAL: 4>, 1954),\n",
       "  1956: (<ModeKeys.GLOBAL: 4>, 1956),\n",
       "  1958: (<ModeKeys.GLOBAL: 4>, 1958),\n",
       "  1960: (<ModeKeys.GLOBAL: 4>, 1960),\n",
       "  1962: (<ModeKeys.GLOBAL: 4>, 1962),\n",
       "  1964: (<ModeKeys.GLOBAL: 4>, 1964),\n",
       "  1966: (<ModeKeys.GLOBAL: 4>, 1966),\n",
       "  1968: (<ModeKeys.GLOBAL: 4>, 1968),\n",
       "  1970: (<ModeKeys.GLOBAL: 4>, 1970),\n",
       "  1972: (<ModeKeys.GLOBAL: 4>, 1972),\n",
       "  1974: (<ModeKeys.GLOBAL: 4>, 1974),\n",
       "  1976: (<ModeKeys.GLOBAL: 4>, 1976),\n",
       "  1978: (<ModeKeys.GLOBAL: 4>, 1978),\n",
       "  1980: (<ModeKeys.GLOBAL: 4>, 1980),\n",
       "  1982: (<ModeKeys.GLOBAL: 4>, 1982),\n",
       "  1984: (<ModeKeys.GLOBAL: 4>, 1984),\n",
       "  1986: (<ModeKeys.GLOBAL: 4>, 1986),\n",
       "  1988: (<ModeKeys.GLOBAL: 4>, 1988),\n",
       "  1990: (<ModeKeys.GLOBAL: 4>, 1990),\n",
       "  1992: (<ModeKeys.GLOBAL: 4>, 1992),\n",
       "  1994: (<ModeKeys.GLOBAL: 4>, 1994),\n",
       "  1996: (<ModeKeys.GLOBAL: 4>, 1996),\n",
       "  1998: (<ModeKeys.GLOBAL: 4>, 1998),\n",
       "  ...},\n",
       " 'logger': <Logger smdebug (INFO)>,\n",
       " 'parallel': False,\n",
       " 'check': False,\n",
       " 'range_steps': None,\n",
       " 'collection_manager': <class CollectionManager: collection_names=['hyperparameters', 'metrics', 'predictions', 'labels', 'feature_importance', 'average_shap', 'full_shap', 'trees', 'default', 'losses']>,\n",
       " 'loaded_all_steps': False,\n",
       " 'cache': False,\n",
       " 'path': 's3://sagemaker-cookbook-bucket/debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/',\n",
       " 'index_reader': <smdebug.core.index_reader.S3IndexReader at 0x7f463b580908>,\n",
       " 'index_tensors_dict': {'train-error': {0: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f464be06cf8>}},\n",
       "   2: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f464be066d8>}},\n",
       "   4: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088ac8>}},\n",
       "   6: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0885c0>}},\n",
       "   8: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0888d0>}},\n",
       "   10: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088908>}},\n",
       "   12: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0889b0>}},\n",
       "   14: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088fd0>}},\n",
       "   16: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088978>}},\n",
       "   18: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0886d8>}},\n",
       "   20: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088f28>}},\n",
       "   22: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088710>}},\n",
       "   24: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088780>}},\n",
       "   26: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088828>}},\n",
       "   28: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088b00>}},\n",
       "   30: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088b70>}},\n",
       "   32: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088be0>}},\n",
       "   34: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088c50>}},\n",
       "   36: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088cc0>}},\n",
       "   38: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088d30>}},\n",
       "   40: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f464be06710>}},\n",
       "   42: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0e5128>}},\n",
       "   44: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088588>}},\n",
       "   46: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f464919b438>}},\n",
       "   48: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f464c9898d0>}},\n",
       "   50: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f464919b5c0>}},\n",
       "   52: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f464c697b70>}},\n",
       "   54: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba6a0>}},\n",
       "   56: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba710>}},\n",
       "   58: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba5f8>}},\n",
       "   60: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba518>}},\n",
       "   62: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba588>}},\n",
       "   64: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba2b0>}},\n",
       "   66: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba2e8>}},\n",
       "   68: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba320>}},\n",
       "   70: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba438>}},\n",
       "   72: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba470>}},\n",
       "   74: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba4a8>}},\n",
       "   76: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba128>}},\n",
       "   78: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba780>}},\n",
       "   80: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba7f0>}},\n",
       "   82: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba860>}},\n",
       "   84: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba908>}},\n",
       "   86: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba978>}},\n",
       "   88: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba9e8>}},\n",
       "   90: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebaa58>}},\n",
       "   92: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebaac8>}},\n",
       "   94: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebab38>}},\n",
       "   96: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebaba8>}},\n",
       "   98: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebac18>}},\n",
       "   100: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebac88>}},\n",
       "   102: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebacf8>}},\n",
       "   104: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebad68>}},\n",
       "   106: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebadd8>}},\n",
       "   108: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebae48>}},\n",
       "   110: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebaeb8>}},\n",
       "   112: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebaf28>}},\n",
       "   114: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebaf98>}},\n",
       "   116: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba160>}},\n",
       "   118: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba0f0>}},\n",
       "   120: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba048>}},\n",
       "   122: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f464bf26f28>}},\n",
       "   124: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9080>}},\n",
       "   126: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a90f0>}},\n",
       "   128: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9160>}},\n",
       "   130: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a91d0>}},\n",
       "   132: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9240>}},\n",
       "   134: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a92b0>}},\n",
       "   136: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9320>}},\n",
       "   138: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9390>}},\n",
       "   140: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9400>}},\n",
       "   142: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9470>}},\n",
       "   144: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a94e0>}},\n",
       "   146: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9550>}},\n",
       "   148: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a95c0>}},\n",
       "   150: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9630>}},\n",
       "   152: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a96a0>}},\n",
       "   154: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9710>}},\n",
       "   156: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9780>}},\n",
       "   158: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a97f0>}},\n",
       "   160: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9860>}},\n",
       "   162: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a98d0>}},\n",
       "   164: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9940>}},\n",
       "   166: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a99b0>}},\n",
       "   168: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9a20>}},\n",
       "   170: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9a90>}},\n",
       "   172: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9b00>}},\n",
       "   174: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9b70>}},\n",
       "   176: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9be0>}},\n",
       "   178: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9c50>}},\n",
       "   180: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9cc0>}},\n",
       "   182: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9d30>}},\n",
       "   184: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9da0>}},\n",
       "   186: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9e10>}},\n",
       "   188: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9e80>}},\n",
       "   190: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9ef0>}},\n",
       "   192: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9f60>}},\n",
       "   194: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba1d0>}},\n",
       "   196: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096080>}},\n",
       "   198: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0960f0>}},\n",
       "   200: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096160>}},\n",
       "   202: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0961d0>}},\n",
       "   204: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096240>}},\n",
       "   206: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0962b0>}},\n",
       "   208: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096320>}},\n",
       "   210: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096390>}},\n",
       "   212: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096400>}},\n",
       "   214: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096470>}},\n",
       "   216: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0964e0>}},\n",
       "   218: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096550>}},\n",
       "   220: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0965c0>}},\n",
       "   222: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096630>}},\n",
       "   224: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0966a0>}},\n",
       "   226: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096710>}},\n",
       "   228: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096780>}},\n",
       "   230: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0967f0>}},\n",
       "   232: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096860>}},\n",
       "   234: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0968d0>}},\n",
       "   236: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096940>}},\n",
       "   238: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0969b0>}},\n",
       "   240: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096a20>}},\n",
       "   242: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096a90>}},\n",
       "   244: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096b00>}},\n",
       "   246: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096b70>}},\n",
       "   248: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096be0>}},\n",
       "   250: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096c50>}},\n",
       "   252: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096cc0>}},\n",
       "   254: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096d30>}},\n",
       "   256: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096da0>}},\n",
       "   258: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096e10>}},\n",
       "   260: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096e80>}},\n",
       "   262: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096ef0>}},\n",
       "   264: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096f60>}},\n",
       "   266: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9fd0>}},\n",
       "   268: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab080>}},\n",
       "   270: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab0f0>}},\n",
       "   272: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab160>}},\n",
       "   274: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab1d0>}},\n",
       "   276: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab240>}},\n",
       "   278: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab2b0>}},\n",
       "   280: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab320>}},\n",
       "   282: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab390>}},\n",
       "   284: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab400>}},\n",
       "   286: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab470>}},\n",
       "   288: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab4e0>}},\n",
       "   290: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab550>}},\n",
       "   292: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab5c0>}},\n",
       "   294: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab630>}},\n",
       "   296: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab6a0>}},\n",
       "   298: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab710>}},\n",
       "   300: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab780>}},\n",
       "   302: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab7f0>}},\n",
       "   304: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab860>}},\n",
       "   306: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab8d0>}},\n",
       "   308: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab940>}},\n",
       "   310: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab9b0>}},\n",
       "   312: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0aba20>}},\n",
       "   314: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0aba90>}},\n",
       "   316: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abb00>}},\n",
       "   318: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abb70>}},\n",
       "   320: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abbe0>}},\n",
       "   322: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abc50>}},\n",
       "   324: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abcc0>}},\n",
       "   326: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abd30>}},\n",
       "   328: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abda0>}},\n",
       "   330: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abe10>}},\n",
       "   332: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abe80>}},\n",
       "   334: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abef0>}},\n",
       "   336: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abf60>}},\n",
       "   338: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096fd0>}},\n",
       "   340: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19080>}},\n",
       "   342: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af190f0>}},\n",
       "   344: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19160>}},\n",
       "   346: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af191d0>}},\n",
       "   348: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19240>}},\n",
       "   350: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af192b0>}},\n",
       "   352: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19320>}},\n",
       "   354: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19390>}},\n",
       "   356: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19400>}},\n",
       "   358: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19470>}},\n",
       "   360: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af194e0>}},\n",
       "   362: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19550>}},\n",
       "   364: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af195c0>}},\n",
       "   366: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19630>}},\n",
       "   368: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af196a0>}},\n",
       "   370: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19710>}},\n",
       "   372: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19780>}},\n",
       "   374: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af197f0>}},\n",
       "   376: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19860>}},\n",
       "   378: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af198d0>}},\n",
       "   380: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19940>}},\n",
       "   382: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af199b0>}},\n",
       "   384: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19a20>}},\n",
       "   386: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19a90>}},\n",
       "   388: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19b00>}},\n",
       "   390: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19b70>}},\n",
       "   392: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19be0>}},\n",
       "   394: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19c50>}},\n",
       "   396: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19cc0>}},\n",
       "   398: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19d30>}},\n",
       "   400: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19da0>}},\n",
       "   402: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19e10>}},\n",
       "   404: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19e80>}},\n",
       "   406: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19ef0>}},\n",
       "   408: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19f60>}},\n",
       "   410: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abfd0>}},\n",
       "   412: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36080>}},\n",
       "   414: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af360f0>}},\n",
       "   416: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36160>}},\n",
       "   418: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af361d0>}},\n",
       "   420: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36240>}},\n",
       "   422: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af362b0>}},\n",
       "   424: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36320>}},\n",
       "   426: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36390>}},\n",
       "   428: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36400>}},\n",
       "   430: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36470>}},\n",
       "   432: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af364e0>}},\n",
       "   434: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36550>}},\n",
       "   436: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af365c0>}},\n",
       "   438: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36630>}},\n",
       "   440: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af366a0>}},\n",
       "   442: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36710>}},\n",
       "   444: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36780>}},\n",
       "   446: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af367f0>}},\n",
       "   448: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36860>}},\n",
       "   450: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af368d0>}},\n",
       "   452: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36940>}},\n",
       "   454: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af369b0>}},\n",
       "   456: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36a20>}},\n",
       "   458: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36a90>}},\n",
       "   460: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36b00>}},\n",
       "   462: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36b70>}},\n",
       "   464: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36be0>}},\n",
       "   466: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36c50>}},\n",
       "   468: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36cc0>}},\n",
       "   470: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36d30>}},\n",
       "   472: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36da0>}},\n",
       "   474: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36e10>}},\n",
       "   476: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36e80>}},\n",
       "   478: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36ef0>}},\n",
       "   480: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36f60>}},\n",
       "   482: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19fd0>}},\n",
       "   484: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06080>}},\n",
       "   486: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af060f0>}},\n",
       "   488: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06160>}},\n",
       "   490: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af061d0>}},\n",
       "   492: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06240>}},\n",
       "   494: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af062b0>}},\n",
       "   496: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06320>}},\n",
       "   498: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06390>}},\n",
       "   500: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06400>}},\n",
       "   502: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06470>}},\n",
       "   504: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af064e0>}},\n",
       "   506: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06550>}},\n",
       "   508: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af065c0>}},\n",
       "   510: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06630>}},\n",
       "   512: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af066a0>}},\n",
       "   514: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06710>}},\n",
       "   516: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06780>}},\n",
       "   518: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af067f0>}},\n",
       "   520: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06860>}},\n",
       "   522: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af068d0>}},\n",
       "   524: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06940>}},\n",
       "   526: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af069b0>}},\n",
       "   528: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06a20>}},\n",
       "   530: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06a90>}},\n",
       "   532: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06b00>}},\n",
       "   534: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06b70>}},\n",
       "   536: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06be0>}},\n",
       "   538: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06c50>}},\n",
       "   540: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06cc0>}},\n",
       "   542: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06d30>}},\n",
       "   544: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06da0>}},\n",
       "   546: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06e10>}},\n",
       "   548: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06e80>}},\n",
       "   550: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06ef0>}},\n",
       "   552: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06f60>}},\n",
       "   554: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36fd0>}},\n",
       "   556: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d080>}},\n",
       "   558: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d0f0>}},\n",
       "   560: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d160>}},\n",
       "   562: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d1d0>}},\n",
       "   564: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d240>}},\n",
       "   566: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d2b0>}},\n",
       "   568: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d320>}},\n",
       "   570: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d390>}},\n",
       "   572: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d400>}},\n",
       "   574: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d470>}},\n",
       "   576: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d4e0>}},\n",
       "   578: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d550>}},\n",
       "   580: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d5c0>}},\n",
       "   582: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d630>}},\n",
       "   584: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d6a0>}},\n",
       "   586: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d710>}},\n",
       "   588: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d780>}},\n",
       "   590: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d7f0>}},\n",
       "   592: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d860>}},\n",
       "   594: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d8d0>}},\n",
       "   596: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d940>}},\n",
       "   598: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d9b0>}},\n",
       "   600: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8da20>}},\n",
       "   602: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8da90>}},\n",
       "   604: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8db00>}},\n",
       "   606: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8db70>}},\n",
       "   608: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8dbe0>}},\n",
       "   610: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8dc50>}},\n",
       "   612: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8dcc0>}},\n",
       "   614: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8dd30>}},\n",
       "   616: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8dda0>}},\n",
       "   618: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8de10>}},\n",
       "   620: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8de80>}},\n",
       "   622: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8def0>}},\n",
       "   624: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8df60>}},\n",
       "   626: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06fd0>}},\n",
       "   628: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1080>}},\n",
       "   630: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea10f0>}},\n",
       "   632: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1160>}},\n",
       "   634: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea11d0>}},\n",
       "   636: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1240>}},\n",
       "   638: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea12b0>}},\n",
       "   640: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1320>}},\n",
       "   642: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1390>}},\n",
       "   644: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1400>}},\n",
       "   646: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1470>}},\n",
       "   648: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea14e0>}},\n",
       "   650: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1550>}},\n",
       "   652: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea15c0>}},\n",
       "   654: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1630>}},\n",
       "   656: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea16a0>}},\n",
       "   658: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1710>}},\n",
       "   660: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1780>}},\n",
       "   662: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea17f0>}},\n",
       "   664: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1860>}},\n",
       "   666: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea18d0>}},\n",
       "   668: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1940>}},\n",
       "   670: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea19b0>}},\n",
       "   672: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1a20>}},\n",
       "   674: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1a90>}},\n",
       "   676: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1b00>}},\n",
       "   678: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1b70>}},\n",
       "   680: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1be0>}},\n",
       "   682: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1c50>}},\n",
       "   684: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1cc0>}},\n",
       "   686: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1d30>}},\n",
       "   688: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1da0>}},\n",
       "   690: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1e10>}},\n",
       "   692: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1e80>}},\n",
       "   694: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1ef0>}},\n",
       "   696: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1f60>}},\n",
       "   698: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8dfd0>}},\n",
       "   700: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7080>}},\n",
       "   702: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb70f0>}},\n",
       "   704: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7160>}},\n",
       "   706: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb71d0>}},\n",
       "   708: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7240>}},\n",
       "   710: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb72b0>}},\n",
       "   712: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7320>}},\n",
       "   714: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7390>}},\n",
       "   716: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7400>}},\n",
       "   718: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7470>}},\n",
       "   720: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb74e0>}},\n",
       "   722: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7550>}},\n",
       "   724: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb75c0>}},\n",
       "   726: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7630>}},\n",
       "   728: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb76a0>}},\n",
       "   730: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7710>}},\n",
       "   732: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7780>}},\n",
       "   734: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb77f0>}},\n",
       "   736: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7860>}},\n",
       "   738: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb78d0>}},\n",
       "   740: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7940>}},\n",
       "   742: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb79b0>}},\n",
       "   744: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7a20>}},\n",
       "   746: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7a90>}},\n",
       "   748: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7b00>}},\n",
       "   750: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7b70>}},\n",
       "   752: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7be0>}},\n",
       "   754: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7c50>}},\n",
       "   756: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7cc0>}},\n",
       "   758: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7d30>}},\n",
       "   760: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7da0>}},\n",
       "   762: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7e10>}},\n",
       "   764: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7e80>}},\n",
       "   766: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7ef0>}},\n",
       "   768: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7f60>}},\n",
       "   770: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1fd0>}},\n",
       "   772: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46080>}},\n",
       "   774: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af460f0>}},\n",
       "   776: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46160>}},\n",
       "   778: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af461d0>}},\n",
       "   780: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46240>}},\n",
       "   782: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af462b0>}},\n",
       "   784: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46320>}},\n",
       "   786: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46390>}},\n",
       "   788: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46400>}},\n",
       "   790: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46470>}},\n",
       "   792: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af464e0>}},\n",
       "   794: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46550>}},\n",
       "   796: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af465c0>}},\n",
       "   798: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46630>}},\n",
       "   800: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af466a0>}},\n",
       "   802: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46710>}},\n",
       "   804: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46780>}},\n",
       "   806: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af467f0>}},\n",
       "   808: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46860>}},\n",
       "   810: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af468d0>}},\n",
       "   812: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46940>}},\n",
       "   814: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af469b0>}},\n",
       "   816: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46a20>}},\n",
       "   818: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46a90>}},\n",
       "   820: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46b00>}},\n",
       "   822: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46b70>}},\n",
       "   824: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46be0>}},\n",
       "   826: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46c50>}},\n",
       "   828: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46cc0>}},\n",
       "   830: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46d30>}},\n",
       "   832: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46da0>}},\n",
       "   834: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46e10>}},\n",
       "   836: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46e80>}},\n",
       "   838: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46ef0>}},\n",
       "   840: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46f60>}},\n",
       "   842: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7fd0>}},\n",
       "   844: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57080>}},\n",
       "   846: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af570f0>}},\n",
       "   848: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57160>}},\n",
       "   850: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af571d0>}},\n",
       "   852: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57240>}},\n",
       "   854: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af572b0>}},\n",
       "   856: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57320>}},\n",
       "   858: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57390>}},\n",
       "   860: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57400>}},\n",
       "   862: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57470>}},\n",
       "   864: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af574e0>}},\n",
       "   866: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57550>}},\n",
       "   868: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af575c0>}},\n",
       "   870: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57630>}},\n",
       "   872: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af576a0>}},\n",
       "   874: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57710>}},\n",
       "   876: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57780>}},\n",
       "   878: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af577f0>}},\n",
       "   880: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57860>}},\n",
       "   882: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af578d0>}},\n",
       "   884: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57940>}},\n",
       "   886: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af579b0>}},\n",
       "   888: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57a20>}},\n",
       "   890: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57a90>}},\n",
       "   892: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57b00>}},\n",
       "   894: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57b70>}},\n",
       "   896: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57be0>}},\n",
       "   898: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57c50>}},\n",
       "   900: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57cc0>}},\n",
       "   902: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57d30>}},\n",
       "   904: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57da0>}},\n",
       "   906: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57e10>}},\n",
       "   908: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57e80>}},\n",
       "   910: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57ef0>}},\n",
       "   912: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57f60>}},\n",
       "   914: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46fd0>}},\n",
       "   916: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78080>}},\n",
       "   918: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af780f0>}},\n",
       "   920: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78160>}},\n",
       "   922: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af781d0>}},\n",
       "   924: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78240>}},\n",
       "   926: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af782b0>}},\n",
       "   928: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78320>}},\n",
       "   930: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78390>}},\n",
       "   932: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78400>}},\n",
       "   934: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78470>}},\n",
       "   936: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af784e0>}},\n",
       "   938: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78550>}},\n",
       "   940: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af785c0>}},\n",
       "   942: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78630>}},\n",
       "   944: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af786a0>}},\n",
       "   946: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78710>}},\n",
       "   948: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78780>}},\n",
       "   950: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af787f0>}},\n",
       "   952: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78860>}},\n",
       "   954: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af788d0>}},\n",
       "   956: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78940>}},\n",
       "   958: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af789b0>}},\n",
       "   960: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78a20>}},\n",
       "   962: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78a90>}},\n",
       "   964: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78b00>}},\n",
       "   966: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78b70>}},\n",
       "   968: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78be0>}},\n",
       "   970: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78c50>}},\n",
       "   972: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78cc0>}},\n",
       "   974: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78d30>}},\n",
       "   976: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78da0>}},\n",
       "   978: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78e10>}},\n",
       "   980: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78e80>}},\n",
       "   982: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78ef0>}},\n",
       "   984: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78f60>}},\n",
       "   986: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57fd0>}},\n",
       "   988: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f080>}},\n",
       "   990: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f0f0>}},\n",
       "   992: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f160>}},\n",
       "   994: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f1d0>}},\n",
       "   996: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f240>}},\n",
       "   998: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f2b0>}},\n",
       "   1000: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f320>}},\n",
       "   1002: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f390>}},\n",
       "   1004: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f400>}},\n",
       "   1006: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f470>}},\n",
       "   1008: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f4e0>}},\n",
       "   1010: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f550>}},\n",
       "   1012: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f5c0>}},\n",
       "   1014: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f630>}},\n",
       "   1016: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f6a0>}},\n",
       "   1018: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f710>}},\n",
       "   1020: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f780>}},\n",
       "   1022: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f7f0>}},\n",
       "   1024: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f860>}},\n",
       "   1026: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f8d0>}},\n",
       "   1028: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f940>}},\n",
       "   1030: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f9b0>}},\n",
       "   1032: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fa20>}},\n",
       "   1034: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fa90>}},\n",
       "   1036: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fb00>}},\n",
       "   1038: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fb70>}},\n",
       "   1040: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fbe0>}},\n",
       "   1042: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fc50>}},\n",
       "   1044: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fcc0>}},\n",
       "   1046: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fd30>}},\n",
       "   1048: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fda0>}},\n",
       "   1050: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fe10>}},\n",
       "   1052: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fe80>}},\n",
       "   1054: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fef0>}},\n",
       "   1056: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0ff60>}},\n",
       "   1058: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78fd0>}},\n",
       "   1060: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22080>}},\n",
       "   1062: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae220f0>}},\n",
       "   1064: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22160>}},\n",
       "   1066: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae221d0>}},\n",
       "   1068: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22240>}},\n",
       "   1070: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae222b0>}},\n",
       "   1072: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22320>}},\n",
       "   1074: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22390>}},\n",
       "   1076: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22400>}},\n",
       "   1078: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22470>}},\n",
       "   1080: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae224e0>}},\n",
       "   1082: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22550>}},\n",
       "   1084: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae225c0>}},\n",
       "   1086: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22630>}},\n",
       "   1088: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae226a0>}},\n",
       "   1090: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22710>}},\n",
       "   1092: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22780>}},\n",
       "   1094: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae227f0>}},\n",
       "   1096: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22860>}},\n",
       "   1098: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae228d0>}},\n",
       "   1100: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22940>}},\n",
       "   1102: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae229b0>}},\n",
       "   1104: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22a20>}},\n",
       "   1106: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22a90>}},\n",
       "   1108: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22b00>}},\n",
       "   1110: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22b70>}},\n",
       "   1112: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22be0>}},\n",
       "   1114: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22c50>}},\n",
       "   1116: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22cc0>}},\n",
       "   1118: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22d30>}},\n",
       "   1120: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22da0>}},\n",
       "   1122: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22e10>}},\n",
       "   1124: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22e80>}},\n",
       "   1126: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22ef0>}},\n",
       "   1128: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22f60>}},\n",
       "   1130: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0ffd0>}},\n",
       "   1132: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34080>}},\n",
       "   1134: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae340f0>}},\n",
       "   1136: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34160>}},\n",
       "   1138: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae341d0>}},\n",
       "   1140: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34240>}},\n",
       "   1142: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae342b0>}},\n",
       "   1144: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34320>}},\n",
       "   1146: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34390>}},\n",
       "   1148: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34400>}},\n",
       "   1150: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34470>}},\n",
       "   1152: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae344e0>}},\n",
       "   1154: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34550>}},\n",
       "   1156: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae345c0>}},\n",
       "   1158: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34630>}},\n",
       "   1160: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae346a0>}},\n",
       "   1162: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34710>}},\n",
       "   1164: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34780>}},\n",
       "   1166: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae347f0>}},\n",
       "   1168: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34860>}},\n",
       "   1170: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae348d0>}},\n",
       "   1172: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34940>}},\n",
       "   1174: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae349b0>}},\n",
       "   1176: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34a20>}},\n",
       "   1178: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34a90>}},\n",
       "   1180: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34b00>}},\n",
       "   1182: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34b70>}},\n",
       "   1184: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34be0>}},\n",
       "   1186: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34c50>}},\n",
       "   1188: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34cc0>}},\n",
       "   1190: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34d30>}},\n",
       "   1192: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34da0>}},\n",
       "   1194: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34e10>}},\n",
       "   1196: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34e80>}},\n",
       "   1198: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34ef0>}},\n",
       "   1200: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34f60>}},\n",
       "   1202: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22fd0>}},\n",
       "   1204: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3080>}},\n",
       "   1206: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff30f0>}},\n",
       "   1208: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3160>}},\n",
       "   1210: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff31d0>}},\n",
       "   1212: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3240>}},\n",
       "   1214: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff32b0>}},\n",
       "   1216: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3320>}},\n",
       "   1218: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3390>}},\n",
       "   1220: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3400>}},\n",
       "   1222: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3470>}},\n",
       "   1224: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff34e0>}},\n",
       "   1226: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3550>}},\n",
       "   1228: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff35c0>}},\n",
       "   1230: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3630>}},\n",
       "   1232: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff36a0>}},\n",
       "   1234: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3710>}},\n",
       "   1236: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3780>}},\n",
       "   1238: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff37f0>}},\n",
       "   1240: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3860>}},\n",
       "   1242: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff38d0>}},\n",
       "   1244: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3940>}},\n",
       "   1246: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff39b0>}},\n",
       "   1248: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3a20>}},\n",
       "   1250: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3a90>}},\n",
       "   1252: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3b00>}},\n",
       "   1254: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3b70>}},\n",
       "   1256: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3be0>}},\n",
       "   1258: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3c50>}},\n",
       "   1260: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3cc0>}},\n",
       "   1262: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3d30>}},\n",
       "   1264: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3da0>}},\n",
       "   1266: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3e10>}},\n",
       "   1268: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3e80>}},\n",
       "   1270: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3ef0>}},\n",
       "   1272: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3f60>}},\n",
       "   1274: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34fd0>}},\n",
       "   1276: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7080>}},\n",
       "   1278: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff70f0>}},\n",
       "   1280: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7160>}},\n",
       "   1282: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff71d0>}},\n",
       "   1284: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7240>}},\n",
       "   1286: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff72b0>}},\n",
       "   1288: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7320>}},\n",
       "   1290: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7390>}},\n",
       "   1292: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7400>}},\n",
       "   1294: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7470>}},\n",
       "   1296: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff74e0>}},\n",
       "   1298: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7550>}},\n",
       "   1300: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff75c0>}},\n",
       "   1302: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7630>}},\n",
       "   1304: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff76a0>}},\n",
       "   1306: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7710>}},\n",
       "   1308: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7780>}},\n",
       "   1310: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff77f0>}},\n",
       "   1312: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7860>}},\n",
       "   1314: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff78d0>}},\n",
       "   1316: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7940>}},\n",
       "   1318: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff79b0>}},\n",
       "   1320: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7a20>}},\n",
       "   1322: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7a90>}},\n",
       "   1324: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7b00>}},\n",
       "   1326: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7b70>}},\n",
       "   1328: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7be0>}},\n",
       "   1330: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7c50>}},\n",
       "   1332: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7cc0>}},\n",
       "   1334: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7d30>}},\n",
       "   1336: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7da0>}},\n",
       "   1338: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7e10>}},\n",
       "   1340: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7e80>}},\n",
       "   1342: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7ef0>}},\n",
       "   1344: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7f60>}},\n",
       "   1346: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3fd0>}},\n",
       "   1348: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2080>}},\n",
       "   1350: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff20f0>}},\n",
       "   1352: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2160>}},\n",
       "   1354: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff21d0>}},\n",
       "   1356: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2240>}},\n",
       "   1358: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff22b0>}},\n",
       "   1360: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2320>}},\n",
       "   1362: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2390>}},\n",
       "   1364: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2400>}},\n",
       "   1366: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2470>}},\n",
       "   1368: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff24e0>}},\n",
       "   1370: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2550>}},\n",
       "   1372: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff25c0>}},\n",
       "   1374: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2630>}},\n",
       "   1376: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff26a0>}},\n",
       "   1378: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2710>}},\n",
       "   1380: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2780>}},\n",
       "   1382: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff27f0>}},\n",
       "   1384: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2860>}},\n",
       "   1386: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff28d0>}},\n",
       "   1388: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2940>}},\n",
       "   1390: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff29b0>}},\n",
       "   1392: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2a20>}},\n",
       "   1394: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2a90>}},\n",
       "   1396: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2b00>}},\n",
       "   1398: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2b70>}},\n",
       "   1400: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2be0>}},\n",
       "   1402: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2c50>}},\n",
       "   1404: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2cc0>}},\n",
       "   1406: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2d30>}},\n",
       "   1408: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2da0>}},\n",
       "   1410: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2e10>}},\n",
       "   1412: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2e80>}},\n",
       "   1414: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2ef0>}},\n",
       "   1416: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2f60>}},\n",
       "   1418: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7fd0>}},\n",
       "   1420: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98080>}},\n",
       "   1422: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af980f0>}},\n",
       "   1424: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98160>}},\n",
       "   1426: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af981d0>}},\n",
       "   1428: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98240>}},\n",
       "   1430: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af982b0>}},\n",
       "   1432: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98320>}},\n",
       "   1434: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98390>}},\n",
       "   1436: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98400>}},\n",
       "   1438: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98470>}},\n",
       "   1440: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af984e0>}},\n",
       "   1442: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98550>}},\n",
       "   1444: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af985c0>}},\n",
       "   1446: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98630>}},\n",
       "   1448: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af986a0>}},\n",
       "   1450: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98710>}},\n",
       "   1452: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98780>}},\n",
       "   1454: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af987f0>}},\n",
       "   1456: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98860>}},\n",
       "   1458: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af988d0>}},\n",
       "   1460: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98940>}},\n",
       "   1462: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af989b0>}},\n",
       "   1464: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98a20>}},\n",
       "   1466: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98a90>}},\n",
       "   1468: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98b00>}},\n",
       "   1470: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98b70>}},\n",
       "   1472: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98be0>}},\n",
       "   1474: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98c50>}},\n",
       "   1476: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98cc0>}},\n",
       "   1478: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98d30>}},\n",
       "   1480: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98da0>}},\n",
       "   1482: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98e10>}},\n",
       "   1484: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98e80>}},\n",
       "   1486: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98ef0>}},\n",
       "   1488: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98f60>}},\n",
       "   1490: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2fd0>}},\n",
       "   1492: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f080>}},\n",
       "   1494: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f0f0>}},\n",
       "   1496: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f160>}},\n",
       "   1498: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f1d0>}},\n",
       "   1500: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f240>}},\n",
       "   1502: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f2b0>}},\n",
       "   1504: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f320>}},\n",
       "   1506: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f390>}},\n",
       "   1508: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f400>}},\n",
       "   1510: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f470>}},\n",
       "   1512: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f4e0>}},\n",
       "   1514: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f550>}},\n",
       "   1516: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f5c0>}},\n",
       "   1518: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f630>}},\n",
       "   1520: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f6a0>}},\n",
       "   1522: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f710>}},\n",
       "   1524: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f780>}},\n",
       "   1526: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f7f0>}},\n",
       "   1528: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f860>}},\n",
       "   1530: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f8d0>}},\n",
       "   1532: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f940>}},\n",
       "   1534: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f9b0>}},\n",
       "   1536: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fa20>}},\n",
       "   1538: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fa90>}},\n",
       "   1540: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fb00>}},\n",
       "   1542: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fb70>}},\n",
       "   1544: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fbe0>}},\n",
       "   1546: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fc50>}},\n",
       "   1548: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fcc0>}},\n",
       "   1550: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fd30>}},\n",
       "   1552: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fda0>}},\n",
       "   1554: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fe10>}},\n",
       "   1556: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fe80>}},\n",
       "   1558: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fef0>}},\n",
       "   1560: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9ff60>}},\n",
       "   1562: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98fd0>}},\n",
       "   1564: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac080>}},\n",
       "   1566: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac0f0>}},\n",
       "   1568: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac160>}},\n",
       "   1570: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac1d0>}},\n",
       "   1572: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac240>}},\n",
       "   1574: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac2b0>}},\n",
       "   1576: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac320>}},\n",
       "   1578: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac390>}},\n",
       "   1580: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac400>}},\n",
       "   1582: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac470>}},\n",
       "   1584: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac4e0>}},\n",
       "   1586: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac550>}},\n",
       "   1588: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac5c0>}},\n",
       "   1590: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac630>}},\n",
       "   1592: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac6a0>}},\n",
       "   1594: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac710>}},\n",
       "   1596: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac780>}},\n",
       "   1598: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac7f0>}},\n",
       "   1600: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac860>}},\n",
       "   1602: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac8d0>}},\n",
       "   1604: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac940>}},\n",
       "   1606: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac9b0>}},\n",
       "   1608: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afaca20>}},\n",
       "   1610: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afaca90>}},\n",
       "   1612: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacb00>}},\n",
       "   1614: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacb70>}},\n",
       "   1616: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacbe0>}},\n",
       "   1618: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacc50>}},\n",
       "   1620: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afaccc0>}},\n",
       "   1622: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacd30>}},\n",
       "   1624: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacda0>}},\n",
       "   1626: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aface10>}},\n",
       "   1628: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aface80>}},\n",
       "   1630: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacef0>}},\n",
       "   1632: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacf60>}},\n",
       "   1634: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9ffd0>}},\n",
       "   1636: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c080>}},\n",
       "   1638: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c0f0>}},\n",
       "   1640: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c160>}},\n",
       "   1642: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c1d0>}},\n",
       "   1644: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c240>}},\n",
       "   1646: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c2b0>}},\n",
       "   1648: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c320>}},\n",
       "   1650: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c390>}},\n",
       "   1652: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c400>}},\n",
       "   1654: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c470>}},\n",
       "   1656: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c4e0>}},\n",
       "   1658: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c550>}},\n",
       "   1660: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c5c0>}},\n",
       "   1662: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c630>}},\n",
       "   1664: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c6a0>}},\n",
       "   1666: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c710>}},\n",
       "   1668: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c780>}},\n",
       "   1670: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c7f0>}},\n",
       "   1672: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c860>}},\n",
       "   1674: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c8d0>}},\n",
       "   1676: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c940>}},\n",
       "   1678: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c9b0>}},\n",
       "   1680: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9ca20>}},\n",
       "   1682: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9ca90>}},\n",
       "   1684: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cb00>}},\n",
       "   1686: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cb70>}},\n",
       "   1688: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cbe0>}},\n",
       "   1690: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cc50>}},\n",
       "   1692: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9ccc0>}},\n",
       "   1694: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cd30>}},\n",
       "   1696: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cda0>}},\n",
       "   1698: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9ce10>}},\n",
       "   1700: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9ce80>}},\n",
       "   1702: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cef0>}},\n",
       "   1704: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cf60>}},\n",
       "   1706: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacfd0>}},\n",
       "   1708: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026080>}},\n",
       "   1710: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0260f0>}},\n",
       "   1712: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026160>}},\n",
       "   1714: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0261d0>}},\n",
       "   1716: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026240>}},\n",
       "   1718: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0262b0>}},\n",
       "   1720: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026320>}},\n",
       "   1722: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026390>}},\n",
       "   1724: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026400>}},\n",
       "   1726: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026470>}},\n",
       "   1728: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0264e0>}},\n",
       "   1730: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026550>}},\n",
       "   1732: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0265c0>}},\n",
       "   1734: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026630>}},\n",
       "   1736: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0266a0>}},\n",
       "   1738: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026710>}},\n",
       "   1740: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026780>}},\n",
       "   1742: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0267f0>}},\n",
       "   1744: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026860>}},\n",
       "   1746: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0268d0>}},\n",
       "   1748: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026940>}},\n",
       "   1750: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0269b0>}},\n",
       "   1752: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026a20>}},\n",
       "   1754: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026a90>}},\n",
       "   1756: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026b00>}},\n",
       "   1758: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026b70>}},\n",
       "   1760: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026be0>}},\n",
       "   1762: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026c50>}},\n",
       "   1764: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026cc0>}},\n",
       "   1766: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026d30>}},\n",
       "   1768: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026da0>}},\n",
       "   1770: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026e10>}},\n",
       "   1772: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026e80>}},\n",
       "   1774: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026ef0>}},\n",
       "   1776: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026f60>}},\n",
       "   1778: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cfd0>}},\n",
       "   1780: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029080>}},\n",
       "   1782: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0290f0>}},\n",
       "   1784: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029160>}},\n",
       "   1786: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0291d0>}},\n",
       "   1788: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029240>}},\n",
       "   1790: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0292b0>}},\n",
       "   1792: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029320>}},\n",
       "   1794: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029390>}},\n",
       "   1796: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029400>}},\n",
       "   1798: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029470>}},\n",
       "   1800: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0294e0>}},\n",
       "   1802: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029550>}},\n",
       "   1804: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0295c0>}},\n",
       "   1806: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029630>}},\n",
       "   1808: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0296a0>}},\n",
       "   1810: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029710>}},\n",
       "   1812: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029780>}},\n",
       "   1814: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0297f0>}},\n",
       "   1816: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029860>}},\n",
       "   1818: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0298d0>}},\n",
       "   1820: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029940>}},\n",
       "   1822: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0299b0>}},\n",
       "   1824: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029a20>}},\n",
       "   1826: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029a90>}},\n",
       "   1828: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029b00>}},\n",
       "   1830: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029b70>}},\n",
       "   1832: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029be0>}},\n",
       "   1834: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029c50>}},\n",
       "   1836: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029cc0>}},\n",
       "   1838: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029d30>}},\n",
       "   1840: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029da0>}},\n",
       "   1842: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029e10>}},\n",
       "   1844: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029e80>}},\n",
       "   1846: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029ef0>}},\n",
       "   1848: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029f60>}},\n",
       "   1850: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026fd0>}},\n",
       "   1852: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d080>}},\n",
       "   1854: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d0f0>}},\n",
       "   1856: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d160>}},\n",
       "   1858: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d1d0>}},\n",
       "   1860: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d240>}},\n",
       "   1862: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d2b0>}},\n",
       "   1864: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d320>}},\n",
       "   1866: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d390>}},\n",
       "   1868: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d400>}},\n",
       "   1870: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d470>}},\n",
       "   1872: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d4e0>}},\n",
       "   1874: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d550>}},\n",
       "   1876: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d5c0>}},\n",
       "   1878: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d630>}},\n",
       "   1880: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d6a0>}},\n",
       "   1882: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d710>}},\n",
       "   1884: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d780>}},\n",
       "   1886: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d7f0>}},\n",
       "   1888: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d860>}},\n",
       "   1890: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d8d0>}},\n",
       "   1892: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d940>}},\n",
       "   1894: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d9b0>}},\n",
       "   1896: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00da20>}},\n",
       "   1898: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00da90>}},\n",
       "   1900: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00db00>}},\n",
       "   1902: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00db70>}},\n",
       "   1904: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00dbe0>}},\n",
       "   1906: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00dc50>}},\n",
       "   1908: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00dcc0>}},\n",
       "   1910: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00dd30>}},\n",
       "   1912: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00dda0>}},\n",
       "   1914: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00de10>}},\n",
       "   1916: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00de80>}},\n",
       "   1918: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00def0>}},\n",
       "   1920: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00df60>}},\n",
       "   1922: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029fd0>}},\n",
       "   1924: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068080>}},\n",
       "   1926: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0680f0>}},\n",
       "   1928: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068160>}},\n",
       "   1930: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0681d0>}},\n",
       "   1932: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068240>}},\n",
       "   1934: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0682b0>}},\n",
       "   1936: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068320>}},\n",
       "   1938: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068390>}},\n",
       "   1940: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068400>}},\n",
       "   1942: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068470>}},\n",
       "   1944: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0684e0>}},\n",
       "   1946: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068550>}},\n",
       "   1948: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0685c0>}},\n",
       "   1950: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068630>}},\n",
       "   1952: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0686a0>}},\n",
       "   1954: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068710>}},\n",
       "   1956: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068780>}},\n",
       "   1958: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0687f0>}},\n",
       "   1960: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068860>}},\n",
       "   1962: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0688d0>}},\n",
       "   1964: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068940>}},\n",
       "   1966: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0689b0>}},\n",
       "   1968: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068a20>}},\n",
       "   1970: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068a90>}},\n",
       "   1972: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068b00>}},\n",
       "   1974: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068b70>}},\n",
       "   1976: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068be0>}},\n",
       "   1978: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068c50>}},\n",
       "   1980: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068cc0>}},\n",
       "   1982: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068d30>}},\n",
       "   1984: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068da0>}},\n",
       "   1986: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068e10>}},\n",
       "   1988: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068e80>}},\n",
       "   1990: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068ef0>}},\n",
       "   1992: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068f60>}},\n",
       "   1994: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00dfd0>}},\n",
       "   1996: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b07a080>}},\n",
       "   1998: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b07a0f0>}},\n",
       "   ...},\n",
       "  'validation-error': {0: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f464be066a0>}},\n",
       "   2: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088d68>}},\n",
       "   4: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088a20>}},\n",
       "   6: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088748>}},\n",
       "   8: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0884e0>}},\n",
       "   10: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088a58>}},\n",
       "   12: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0887f0>}},\n",
       "   14: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0885f8>}},\n",
       "   16: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088898>}},\n",
       "   18: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088f60>}},\n",
       "   20: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088eb8>}},\n",
       "   22: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088630>}},\n",
       "   24: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0884a8>}},\n",
       "   26: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088a90>}},\n",
       "   28: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088b38>}},\n",
       "   30: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088ba8>}},\n",
       "   32: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088c18>}},\n",
       "   34: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088c88>}},\n",
       "   36: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088cf8>}},\n",
       "   38: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b088da0>}},\n",
       "   40: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0e5320>}},\n",
       "   42: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0e52e8>}},\n",
       "   44: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f464919b518>}},\n",
       "   46: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f464919b240>}},\n",
       "   48: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f464c697160>}},\n",
       "   50: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f46493f5ba8>}},\n",
       "   52: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba748>}},\n",
       "   54: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba6d8>}},\n",
       "   56: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba5c0>}},\n",
       "   58: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba630>}},\n",
       "   60: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba550>}},\n",
       "   62: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba208>}},\n",
       "   64: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba278>}},\n",
       "   66: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba390>}},\n",
       "   68: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba3c8>}},\n",
       "   70: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba400>}},\n",
       "   72: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba4e0>}},\n",
       "   74: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba198>}},\n",
       "   76: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba668>}},\n",
       "   78: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba7b8>}},\n",
       "   80: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba828>}},\n",
       "   82: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba8d0>}},\n",
       "   84: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba940>}},\n",
       "   86: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba9b0>}},\n",
       "   88: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebaa20>}},\n",
       "   90: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebaa90>}},\n",
       "   92: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebab00>}},\n",
       "   94: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebab70>}},\n",
       "   96: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebabe0>}},\n",
       "   98: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebac50>}},\n",
       "   100: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebacc0>}},\n",
       "   102: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebad30>}},\n",
       "   104: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebada0>}},\n",
       "   106: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebae10>}},\n",
       "   108: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebae80>}},\n",
       "   110: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebaef0>}},\n",
       "   112: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebaf60>}},\n",
       "   114: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aebafd0>}},\n",
       "   116: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba240>}},\n",
       "   118: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba0b8>}},\n",
       "   120: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeba080>}},\n",
       "   122: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9048>}},\n",
       "   124: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a90b8>}},\n",
       "   126: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9128>}},\n",
       "   128: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9198>}},\n",
       "   130: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9208>}},\n",
       "   132: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9278>}},\n",
       "   134: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a92e8>}},\n",
       "   136: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9358>}},\n",
       "   138: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a93c8>}},\n",
       "   140: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9438>}},\n",
       "   142: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a94a8>}},\n",
       "   144: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9518>}},\n",
       "   146: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9588>}},\n",
       "   148: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a95f8>}},\n",
       "   150: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9668>}},\n",
       "   152: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a96d8>}},\n",
       "   154: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9748>}},\n",
       "   156: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a97b8>}},\n",
       "   158: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9828>}},\n",
       "   160: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9898>}},\n",
       "   162: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9908>}},\n",
       "   164: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9978>}},\n",
       "   166: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a99e8>}},\n",
       "   168: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9a58>}},\n",
       "   170: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9ac8>}},\n",
       "   172: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9b38>}},\n",
       "   174: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9ba8>}},\n",
       "   176: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9c18>}},\n",
       "   178: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9c88>}},\n",
       "   180: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9cf8>}},\n",
       "   182: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9d68>}},\n",
       "   184: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9dd8>}},\n",
       "   186: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9e48>}},\n",
       "   188: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9eb8>}},\n",
       "   190: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9f28>}},\n",
       "   192: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0a9f98>}},\n",
       "   194: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096048>}},\n",
       "   196: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0960b8>}},\n",
       "   198: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096128>}},\n",
       "   200: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096198>}},\n",
       "   202: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096208>}},\n",
       "   204: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096278>}},\n",
       "   206: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0962e8>}},\n",
       "   208: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096358>}},\n",
       "   210: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0963c8>}},\n",
       "   212: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096438>}},\n",
       "   214: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0964a8>}},\n",
       "   216: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096518>}},\n",
       "   218: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096588>}},\n",
       "   220: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0965f8>}},\n",
       "   222: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096668>}},\n",
       "   224: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0966d8>}},\n",
       "   226: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096748>}},\n",
       "   228: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0967b8>}},\n",
       "   230: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096828>}},\n",
       "   232: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096898>}},\n",
       "   234: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096908>}},\n",
       "   236: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096978>}},\n",
       "   238: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0969e8>}},\n",
       "   240: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096a58>}},\n",
       "   242: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096ac8>}},\n",
       "   244: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096b38>}},\n",
       "   246: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096ba8>}},\n",
       "   248: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096c18>}},\n",
       "   250: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096c88>}},\n",
       "   252: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096cf8>}},\n",
       "   254: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096d68>}},\n",
       "   256: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096dd8>}},\n",
       "   258: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096e48>}},\n",
       "   260: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096eb8>}},\n",
       "   262: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096f28>}},\n",
       "   264: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b096f98>}},\n",
       "   266: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab048>}},\n",
       "   268: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab0b8>}},\n",
       "   270: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab128>}},\n",
       "   272: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab198>}},\n",
       "   274: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab208>}},\n",
       "   276: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab278>}},\n",
       "   278: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab2e8>}},\n",
       "   280: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab358>}},\n",
       "   282: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab3c8>}},\n",
       "   284: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab438>}},\n",
       "   286: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab4a8>}},\n",
       "   288: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab518>}},\n",
       "   290: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab588>}},\n",
       "   292: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab5f8>}},\n",
       "   294: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab668>}},\n",
       "   296: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab6d8>}},\n",
       "   298: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab748>}},\n",
       "   300: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab7b8>}},\n",
       "   302: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab828>}},\n",
       "   304: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab898>}},\n",
       "   306: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab908>}},\n",
       "   308: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab978>}},\n",
       "   310: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0ab9e8>}},\n",
       "   312: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0aba58>}},\n",
       "   314: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abac8>}},\n",
       "   316: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abb38>}},\n",
       "   318: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abba8>}},\n",
       "   320: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abc18>}},\n",
       "   322: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abc88>}},\n",
       "   324: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abcf8>}},\n",
       "   326: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abd68>}},\n",
       "   328: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abdd8>}},\n",
       "   330: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abe48>}},\n",
       "   332: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abeb8>}},\n",
       "   334: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abf28>}},\n",
       "   336: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0abf98>}},\n",
       "   338: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19048>}},\n",
       "   340: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af190b8>}},\n",
       "   342: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19128>}},\n",
       "   344: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19198>}},\n",
       "   346: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19208>}},\n",
       "   348: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19278>}},\n",
       "   350: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af192e8>}},\n",
       "   352: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19358>}},\n",
       "   354: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af193c8>}},\n",
       "   356: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19438>}},\n",
       "   358: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af194a8>}},\n",
       "   360: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19518>}},\n",
       "   362: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19588>}},\n",
       "   364: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af195f8>}},\n",
       "   366: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19668>}},\n",
       "   368: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af196d8>}},\n",
       "   370: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19748>}},\n",
       "   372: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af197b8>}},\n",
       "   374: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19828>}},\n",
       "   376: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19898>}},\n",
       "   378: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19908>}},\n",
       "   380: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19978>}},\n",
       "   382: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af199e8>}},\n",
       "   384: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19a58>}},\n",
       "   386: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19ac8>}},\n",
       "   388: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19b38>}},\n",
       "   390: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19ba8>}},\n",
       "   392: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19c18>}},\n",
       "   394: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19c88>}},\n",
       "   396: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19cf8>}},\n",
       "   398: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19d68>}},\n",
       "   400: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19dd8>}},\n",
       "   402: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19e48>}},\n",
       "   404: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19eb8>}},\n",
       "   406: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19f28>}},\n",
       "   408: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af19f98>}},\n",
       "   410: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36048>}},\n",
       "   412: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af360b8>}},\n",
       "   414: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36128>}},\n",
       "   416: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36198>}},\n",
       "   418: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36208>}},\n",
       "   420: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36278>}},\n",
       "   422: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af362e8>}},\n",
       "   424: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36358>}},\n",
       "   426: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af363c8>}},\n",
       "   428: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36438>}},\n",
       "   430: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af364a8>}},\n",
       "   432: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36518>}},\n",
       "   434: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36588>}},\n",
       "   436: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af365f8>}},\n",
       "   438: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36668>}},\n",
       "   440: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af366d8>}},\n",
       "   442: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36748>}},\n",
       "   444: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af367b8>}},\n",
       "   446: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36828>}},\n",
       "   448: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36898>}},\n",
       "   450: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36908>}},\n",
       "   452: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36978>}},\n",
       "   454: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af369e8>}},\n",
       "   456: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36a58>}},\n",
       "   458: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36ac8>}},\n",
       "   460: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36b38>}},\n",
       "   462: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36ba8>}},\n",
       "   464: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36c18>}},\n",
       "   466: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36c88>}},\n",
       "   468: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36cf8>}},\n",
       "   470: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36d68>}},\n",
       "   472: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36dd8>}},\n",
       "   474: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36e48>}},\n",
       "   476: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36eb8>}},\n",
       "   478: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36f28>}},\n",
       "   480: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af36f98>}},\n",
       "   482: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06048>}},\n",
       "   484: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af060b8>}},\n",
       "   486: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06128>}},\n",
       "   488: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06198>}},\n",
       "   490: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06208>}},\n",
       "   492: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06278>}},\n",
       "   494: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af062e8>}},\n",
       "   496: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06358>}},\n",
       "   498: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af063c8>}},\n",
       "   500: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06438>}},\n",
       "   502: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af064a8>}},\n",
       "   504: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06518>}},\n",
       "   506: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06588>}},\n",
       "   508: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af065f8>}},\n",
       "   510: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06668>}},\n",
       "   512: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af066d8>}},\n",
       "   514: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06748>}},\n",
       "   516: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af067b8>}},\n",
       "   518: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06828>}},\n",
       "   520: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06898>}},\n",
       "   522: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06908>}},\n",
       "   524: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06978>}},\n",
       "   526: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af069e8>}},\n",
       "   528: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06a58>}},\n",
       "   530: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06ac8>}},\n",
       "   532: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06b38>}},\n",
       "   534: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06ba8>}},\n",
       "   536: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06c18>}},\n",
       "   538: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06c88>}},\n",
       "   540: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06cf8>}},\n",
       "   542: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06d68>}},\n",
       "   544: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06dd8>}},\n",
       "   546: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06e48>}},\n",
       "   548: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06eb8>}},\n",
       "   550: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06f28>}},\n",
       "   552: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af06f98>}},\n",
       "   554: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d048>}},\n",
       "   556: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d0b8>}},\n",
       "   558: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d128>}},\n",
       "   560: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d198>}},\n",
       "   562: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d208>}},\n",
       "   564: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d278>}},\n",
       "   566: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d2e8>}},\n",
       "   568: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d358>}},\n",
       "   570: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d3c8>}},\n",
       "   572: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d438>}},\n",
       "   574: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d4a8>}},\n",
       "   576: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d518>}},\n",
       "   578: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d588>}},\n",
       "   580: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d5f8>}},\n",
       "   582: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d668>}},\n",
       "   584: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d6d8>}},\n",
       "   586: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d748>}},\n",
       "   588: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d7b8>}},\n",
       "   590: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d828>}},\n",
       "   592: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d898>}},\n",
       "   594: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d908>}},\n",
       "   596: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d978>}},\n",
       "   598: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8d9e8>}},\n",
       "   600: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8da58>}},\n",
       "   602: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8dac8>}},\n",
       "   604: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8db38>}},\n",
       "   606: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8dba8>}},\n",
       "   608: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8dc18>}},\n",
       "   610: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8dc88>}},\n",
       "   612: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8dcf8>}},\n",
       "   614: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8dd68>}},\n",
       "   616: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8ddd8>}},\n",
       "   618: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8de48>}},\n",
       "   620: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8deb8>}},\n",
       "   622: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8df28>}},\n",
       "   624: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae8df98>}},\n",
       "   626: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1048>}},\n",
       "   628: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea10b8>}},\n",
       "   630: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1128>}},\n",
       "   632: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1198>}},\n",
       "   634: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1208>}},\n",
       "   636: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1278>}},\n",
       "   638: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea12e8>}},\n",
       "   640: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1358>}},\n",
       "   642: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea13c8>}},\n",
       "   644: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1438>}},\n",
       "   646: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea14a8>}},\n",
       "   648: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1518>}},\n",
       "   650: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1588>}},\n",
       "   652: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea15f8>}},\n",
       "   654: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1668>}},\n",
       "   656: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea16d8>}},\n",
       "   658: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1748>}},\n",
       "   660: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea17b8>}},\n",
       "   662: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1828>}},\n",
       "   664: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1898>}},\n",
       "   666: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1908>}},\n",
       "   668: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1978>}},\n",
       "   670: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea19e8>}},\n",
       "   672: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1a58>}},\n",
       "   674: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1ac8>}},\n",
       "   676: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1b38>}},\n",
       "   678: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1ba8>}},\n",
       "   680: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1c18>}},\n",
       "   682: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1c88>}},\n",
       "   684: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1cf8>}},\n",
       "   686: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1d68>}},\n",
       "   688: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1dd8>}},\n",
       "   690: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1e48>}},\n",
       "   692: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1eb8>}},\n",
       "   694: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1f28>}},\n",
       "   696: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aea1f98>}},\n",
       "   698: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7048>}},\n",
       "   700: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb70b8>}},\n",
       "   702: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7128>}},\n",
       "   704: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7198>}},\n",
       "   706: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7208>}},\n",
       "   708: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7278>}},\n",
       "   710: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb72e8>}},\n",
       "   712: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7358>}},\n",
       "   714: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb73c8>}},\n",
       "   716: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7438>}},\n",
       "   718: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb74a8>}},\n",
       "   720: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7518>}},\n",
       "   722: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7588>}},\n",
       "   724: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb75f8>}},\n",
       "   726: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7668>}},\n",
       "   728: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb76d8>}},\n",
       "   730: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7748>}},\n",
       "   732: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb77b8>}},\n",
       "   734: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7828>}},\n",
       "   736: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7898>}},\n",
       "   738: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7908>}},\n",
       "   740: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7978>}},\n",
       "   742: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb79e8>}},\n",
       "   744: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7a58>}},\n",
       "   746: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7ac8>}},\n",
       "   748: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7b38>}},\n",
       "   750: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7ba8>}},\n",
       "   752: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7c18>}},\n",
       "   754: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7c88>}},\n",
       "   756: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7cf8>}},\n",
       "   758: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7d68>}},\n",
       "   760: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7dd8>}},\n",
       "   762: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7e48>}},\n",
       "   764: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7eb8>}},\n",
       "   766: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7f28>}},\n",
       "   768: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aeb7f98>}},\n",
       "   770: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46048>}},\n",
       "   772: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af460b8>}},\n",
       "   774: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46128>}},\n",
       "   776: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46198>}},\n",
       "   778: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46208>}},\n",
       "   780: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46278>}},\n",
       "   782: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af462e8>}},\n",
       "   784: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46358>}},\n",
       "   786: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af463c8>}},\n",
       "   788: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46438>}},\n",
       "   790: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af464a8>}},\n",
       "   792: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46518>}},\n",
       "   794: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46588>}},\n",
       "   796: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af465f8>}},\n",
       "   798: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46668>}},\n",
       "   800: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af466d8>}},\n",
       "   802: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46748>}},\n",
       "   804: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af467b8>}},\n",
       "   806: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46828>}},\n",
       "   808: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46898>}},\n",
       "   810: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46908>}},\n",
       "   812: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46978>}},\n",
       "   814: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af469e8>}},\n",
       "   816: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46a58>}},\n",
       "   818: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46ac8>}},\n",
       "   820: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46b38>}},\n",
       "   822: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46ba8>}},\n",
       "   824: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46c18>}},\n",
       "   826: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46c88>}},\n",
       "   828: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46cf8>}},\n",
       "   830: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46d68>}},\n",
       "   832: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46dd8>}},\n",
       "   834: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46e48>}},\n",
       "   836: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46eb8>}},\n",
       "   838: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46f28>}},\n",
       "   840: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af46f98>}},\n",
       "   842: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57048>}},\n",
       "   844: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af570b8>}},\n",
       "   846: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57128>}},\n",
       "   848: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57198>}},\n",
       "   850: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57208>}},\n",
       "   852: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57278>}},\n",
       "   854: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af572e8>}},\n",
       "   856: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57358>}},\n",
       "   858: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af573c8>}},\n",
       "   860: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57438>}},\n",
       "   862: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af574a8>}},\n",
       "   864: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57518>}},\n",
       "   866: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57588>}},\n",
       "   868: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af575f8>}},\n",
       "   870: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57668>}},\n",
       "   872: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af576d8>}},\n",
       "   874: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57748>}},\n",
       "   876: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af577b8>}},\n",
       "   878: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57828>}},\n",
       "   880: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57898>}},\n",
       "   882: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57908>}},\n",
       "   884: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57978>}},\n",
       "   886: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af579e8>}},\n",
       "   888: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57a58>}},\n",
       "   890: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57ac8>}},\n",
       "   892: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57b38>}},\n",
       "   894: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57ba8>}},\n",
       "   896: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57c18>}},\n",
       "   898: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57c88>}},\n",
       "   900: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57cf8>}},\n",
       "   902: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57d68>}},\n",
       "   904: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57dd8>}},\n",
       "   906: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57e48>}},\n",
       "   908: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57eb8>}},\n",
       "   910: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57f28>}},\n",
       "   912: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af57f98>}},\n",
       "   914: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78048>}},\n",
       "   916: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af780b8>}},\n",
       "   918: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78128>}},\n",
       "   920: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78198>}},\n",
       "   922: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78208>}},\n",
       "   924: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78278>}},\n",
       "   926: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af782e8>}},\n",
       "   928: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78358>}},\n",
       "   930: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af783c8>}},\n",
       "   932: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78438>}},\n",
       "   934: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af784a8>}},\n",
       "   936: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78518>}},\n",
       "   938: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78588>}},\n",
       "   940: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af785f8>}},\n",
       "   942: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78668>}},\n",
       "   944: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af786d8>}},\n",
       "   946: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78748>}},\n",
       "   948: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af787b8>}},\n",
       "   950: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78828>}},\n",
       "   952: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78898>}},\n",
       "   954: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78908>}},\n",
       "   956: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78978>}},\n",
       "   958: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af789e8>}},\n",
       "   960: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78a58>}},\n",
       "   962: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78ac8>}},\n",
       "   964: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78b38>}},\n",
       "   966: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78ba8>}},\n",
       "   968: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78c18>}},\n",
       "   970: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78c88>}},\n",
       "   972: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78cf8>}},\n",
       "   974: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78d68>}},\n",
       "   976: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78dd8>}},\n",
       "   978: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78e48>}},\n",
       "   980: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78eb8>}},\n",
       "   982: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78f28>}},\n",
       "   984: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af78f98>}},\n",
       "   986: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f048>}},\n",
       "   988: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f0b8>}},\n",
       "   990: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f128>}},\n",
       "   992: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f198>}},\n",
       "   994: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f208>}},\n",
       "   996: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f278>}},\n",
       "   998: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f2e8>}},\n",
       "   1000: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f358>}},\n",
       "   1002: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f3c8>}},\n",
       "   1004: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f438>}},\n",
       "   1006: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f4a8>}},\n",
       "   1008: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f518>}},\n",
       "   1010: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f588>}},\n",
       "   1012: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f5f8>}},\n",
       "   1014: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f668>}},\n",
       "   1016: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f6d8>}},\n",
       "   1018: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f748>}},\n",
       "   1020: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f7b8>}},\n",
       "   1022: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f828>}},\n",
       "   1024: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f898>}},\n",
       "   1026: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f908>}},\n",
       "   1028: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f978>}},\n",
       "   1030: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0f9e8>}},\n",
       "   1032: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fa58>}},\n",
       "   1034: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fac8>}},\n",
       "   1036: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fb38>}},\n",
       "   1038: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fba8>}},\n",
       "   1040: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fc18>}},\n",
       "   1042: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fc88>}},\n",
       "   1044: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fcf8>}},\n",
       "   1046: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fd68>}},\n",
       "   1048: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fdd8>}},\n",
       "   1050: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0fe48>}},\n",
       "   1052: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0feb8>}},\n",
       "   1054: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0ff28>}},\n",
       "   1056: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae0ff98>}},\n",
       "   1058: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22048>}},\n",
       "   1060: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae220b8>}},\n",
       "   1062: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22128>}},\n",
       "   1064: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22198>}},\n",
       "   1066: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22208>}},\n",
       "   1068: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22278>}},\n",
       "   1070: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae222e8>}},\n",
       "   1072: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22358>}},\n",
       "   1074: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae223c8>}},\n",
       "   1076: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22438>}},\n",
       "   1078: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae224a8>}},\n",
       "   1080: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22518>}},\n",
       "   1082: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22588>}},\n",
       "   1084: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae225f8>}},\n",
       "   1086: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22668>}},\n",
       "   1088: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae226d8>}},\n",
       "   1090: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22748>}},\n",
       "   1092: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae227b8>}},\n",
       "   1094: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22828>}},\n",
       "   1096: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22898>}},\n",
       "   1098: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22908>}},\n",
       "   1100: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22978>}},\n",
       "   1102: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae229e8>}},\n",
       "   1104: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22a58>}},\n",
       "   1106: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22ac8>}},\n",
       "   1108: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22b38>}},\n",
       "   1110: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22ba8>}},\n",
       "   1112: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22c18>}},\n",
       "   1114: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22c88>}},\n",
       "   1116: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22cf8>}},\n",
       "   1118: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22d68>}},\n",
       "   1120: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22dd8>}},\n",
       "   1122: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22e48>}},\n",
       "   1124: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22eb8>}},\n",
       "   1126: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22f28>}},\n",
       "   1128: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae22f98>}},\n",
       "   1130: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34048>}},\n",
       "   1132: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae340b8>}},\n",
       "   1134: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34128>}},\n",
       "   1136: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34198>}},\n",
       "   1138: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34208>}},\n",
       "   1140: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34278>}},\n",
       "   1142: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae342e8>}},\n",
       "   1144: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34358>}},\n",
       "   1146: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae343c8>}},\n",
       "   1148: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34438>}},\n",
       "   1150: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae344a8>}},\n",
       "   1152: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34518>}},\n",
       "   1154: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34588>}},\n",
       "   1156: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae345f8>}},\n",
       "   1158: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34668>}},\n",
       "   1160: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae346d8>}},\n",
       "   1162: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34748>}},\n",
       "   1164: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae347b8>}},\n",
       "   1166: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34828>}},\n",
       "   1168: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34898>}},\n",
       "   1170: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34908>}},\n",
       "   1172: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34978>}},\n",
       "   1174: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae349e8>}},\n",
       "   1176: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34a58>}},\n",
       "   1178: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34ac8>}},\n",
       "   1180: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34b38>}},\n",
       "   1182: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34ba8>}},\n",
       "   1184: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34c18>}},\n",
       "   1186: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34c88>}},\n",
       "   1188: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34cf8>}},\n",
       "   1190: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34d68>}},\n",
       "   1192: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34dd8>}},\n",
       "   1194: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34e48>}},\n",
       "   1196: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34eb8>}},\n",
       "   1198: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34f28>}},\n",
       "   1200: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463ae34f98>}},\n",
       "   1202: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3048>}},\n",
       "   1204: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff30b8>}},\n",
       "   1206: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3128>}},\n",
       "   1208: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3198>}},\n",
       "   1210: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3208>}},\n",
       "   1212: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3278>}},\n",
       "   1214: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff32e8>}},\n",
       "   1216: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3358>}},\n",
       "   1218: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff33c8>}},\n",
       "   1220: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3438>}},\n",
       "   1222: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff34a8>}},\n",
       "   1224: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3518>}},\n",
       "   1226: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3588>}},\n",
       "   1228: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff35f8>}},\n",
       "   1230: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3668>}},\n",
       "   1232: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff36d8>}},\n",
       "   1234: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3748>}},\n",
       "   1236: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff37b8>}},\n",
       "   1238: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3828>}},\n",
       "   1240: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3898>}},\n",
       "   1242: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3908>}},\n",
       "   1244: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3978>}},\n",
       "   1246: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff39e8>}},\n",
       "   1248: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3a58>}},\n",
       "   1250: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3ac8>}},\n",
       "   1252: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3b38>}},\n",
       "   1254: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3ba8>}},\n",
       "   1256: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3c18>}},\n",
       "   1258: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3c88>}},\n",
       "   1260: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3cf8>}},\n",
       "   1262: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3d68>}},\n",
       "   1264: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3dd8>}},\n",
       "   1266: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3e48>}},\n",
       "   1268: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3eb8>}},\n",
       "   1270: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3f28>}},\n",
       "   1272: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff3f98>}},\n",
       "   1274: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7048>}},\n",
       "   1276: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff70b8>}},\n",
       "   1278: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7128>}},\n",
       "   1280: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7198>}},\n",
       "   1282: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7208>}},\n",
       "   1284: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7278>}},\n",
       "   1286: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff72e8>}},\n",
       "   1288: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7358>}},\n",
       "   1290: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff73c8>}},\n",
       "   1292: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7438>}},\n",
       "   1294: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff74a8>}},\n",
       "   1296: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7518>}},\n",
       "   1298: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7588>}},\n",
       "   1300: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff75f8>}},\n",
       "   1302: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7668>}},\n",
       "   1304: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff76d8>}},\n",
       "   1306: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7748>}},\n",
       "   1308: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff77b8>}},\n",
       "   1310: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7828>}},\n",
       "   1312: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7898>}},\n",
       "   1314: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7908>}},\n",
       "   1316: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7978>}},\n",
       "   1318: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff79e8>}},\n",
       "   1320: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7a58>}},\n",
       "   1322: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7ac8>}},\n",
       "   1324: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7b38>}},\n",
       "   1326: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7ba8>}},\n",
       "   1328: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7c18>}},\n",
       "   1330: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7c88>}},\n",
       "   1332: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7cf8>}},\n",
       "   1334: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7d68>}},\n",
       "   1336: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7dd8>}},\n",
       "   1338: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7e48>}},\n",
       "   1340: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7eb8>}},\n",
       "   1342: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7f28>}},\n",
       "   1344: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff7f98>}},\n",
       "   1346: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2048>}},\n",
       "   1348: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff20b8>}},\n",
       "   1350: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2128>}},\n",
       "   1352: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2198>}},\n",
       "   1354: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2208>}},\n",
       "   1356: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2278>}},\n",
       "   1358: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff22e8>}},\n",
       "   1360: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2358>}},\n",
       "   1362: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff23c8>}},\n",
       "   1364: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2438>}},\n",
       "   1366: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff24a8>}},\n",
       "   1368: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2518>}},\n",
       "   1370: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2588>}},\n",
       "   1372: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff25f8>}},\n",
       "   1374: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2668>}},\n",
       "   1376: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff26d8>}},\n",
       "   1378: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2748>}},\n",
       "   1380: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff27b8>}},\n",
       "   1382: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2828>}},\n",
       "   1384: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2898>}},\n",
       "   1386: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2908>}},\n",
       "   1388: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2978>}},\n",
       "   1390: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff29e8>}},\n",
       "   1392: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2a58>}},\n",
       "   1394: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2ac8>}},\n",
       "   1396: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2b38>}},\n",
       "   1398: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2ba8>}},\n",
       "   1400: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2c18>}},\n",
       "   1402: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2c88>}},\n",
       "   1404: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2cf8>}},\n",
       "   1406: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2d68>}},\n",
       "   1408: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2dd8>}},\n",
       "   1410: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2e48>}},\n",
       "   1412: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2eb8>}},\n",
       "   1414: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2f28>}},\n",
       "   1416: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aff2f98>}},\n",
       "   1418: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98048>}},\n",
       "   1420: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af980b8>}},\n",
       "   1422: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98128>}},\n",
       "   1424: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98198>}},\n",
       "   1426: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98208>}},\n",
       "   1428: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98278>}},\n",
       "   1430: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af982e8>}},\n",
       "   1432: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98358>}},\n",
       "   1434: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af983c8>}},\n",
       "   1436: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98438>}},\n",
       "   1438: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af984a8>}},\n",
       "   1440: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98518>}},\n",
       "   1442: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98588>}},\n",
       "   1444: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af985f8>}},\n",
       "   1446: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98668>}},\n",
       "   1448: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af986d8>}},\n",
       "   1450: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98748>}},\n",
       "   1452: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af987b8>}},\n",
       "   1454: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98828>}},\n",
       "   1456: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98898>}},\n",
       "   1458: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98908>}},\n",
       "   1460: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98978>}},\n",
       "   1462: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af989e8>}},\n",
       "   1464: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98a58>}},\n",
       "   1466: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98ac8>}},\n",
       "   1468: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98b38>}},\n",
       "   1470: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98ba8>}},\n",
       "   1472: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98c18>}},\n",
       "   1474: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98c88>}},\n",
       "   1476: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98cf8>}},\n",
       "   1478: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98d68>}},\n",
       "   1480: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98dd8>}},\n",
       "   1482: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98e48>}},\n",
       "   1484: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98eb8>}},\n",
       "   1486: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98f28>}},\n",
       "   1488: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af98f98>}},\n",
       "   1490: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f048>}},\n",
       "   1492: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f0b8>}},\n",
       "   1494: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f128>}},\n",
       "   1496: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f198>}},\n",
       "   1498: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f208>}},\n",
       "   1500: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f278>}},\n",
       "   1502: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f2e8>}},\n",
       "   1504: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f358>}},\n",
       "   1506: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f3c8>}},\n",
       "   1508: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f438>}},\n",
       "   1510: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f4a8>}},\n",
       "   1512: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f518>}},\n",
       "   1514: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f588>}},\n",
       "   1516: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f5f8>}},\n",
       "   1518: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f668>}},\n",
       "   1520: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f6d8>}},\n",
       "   1522: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f748>}},\n",
       "   1524: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f7b8>}},\n",
       "   1526: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f828>}},\n",
       "   1528: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f898>}},\n",
       "   1530: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f908>}},\n",
       "   1532: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f978>}},\n",
       "   1534: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9f9e8>}},\n",
       "   1536: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fa58>}},\n",
       "   1538: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fac8>}},\n",
       "   1540: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fb38>}},\n",
       "   1542: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fba8>}},\n",
       "   1544: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fc18>}},\n",
       "   1546: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fc88>}},\n",
       "   1548: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fcf8>}},\n",
       "   1550: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fd68>}},\n",
       "   1552: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fdd8>}},\n",
       "   1554: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9fe48>}},\n",
       "   1556: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9feb8>}},\n",
       "   1558: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9ff28>}},\n",
       "   1560: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9ff98>}},\n",
       "   1562: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac048>}},\n",
       "   1564: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac0b8>}},\n",
       "   1566: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac128>}},\n",
       "   1568: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac198>}},\n",
       "   1570: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac208>}},\n",
       "   1572: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac278>}},\n",
       "   1574: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac2e8>}},\n",
       "   1576: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac358>}},\n",
       "   1578: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac3c8>}},\n",
       "   1580: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac438>}},\n",
       "   1582: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac4a8>}},\n",
       "   1584: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac518>}},\n",
       "   1586: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac588>}},\n",
       "   1588: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac5f8>}},\n",
       "   1590: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac668>}},\n",
       "   1592: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac6d8>}},\n",
       "   1594: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac748>}},\n",
       "   1596: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac7b8>}},\n",
       "   1598: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac828>}},\n",
       "   1600: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac898>}},\n",
       "   1602: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac908>}},\n",
       "   1604: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac978>}},\n",
       "   1606: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afac9e8>}},\n",
       "   1608: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afaca58>}},\n",
       "   1610: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacac8>}},\n",
       "   1612: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacb38>}},\n",
       "   1614: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacba8>}},\n",
       "   1616: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacc18>}},\n",
       "   1618: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacc88>}},\n",
       "   1620: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afaccf8>}},\n",
       "   1622: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacd68>}},\n",
       "   1624: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacdd8>}},\n",
       "   1626: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463aface48>}},\n",
       "   1628: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afaceb8>}},\n",
       "   1630: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacf28>}},\n",
       "   1632: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463afacf98>}},\n",
       "   1634: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c048>}},\n",
       "   1636: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c0b8>}},\n",
       "   1638: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c128>}},\n",
       "   1640: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c198>}},\n",
       "   1642: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c208>}},\n",
       "   1644: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c278>}},\n",
       "   1646: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c2e8>}},\n",
       "   1648: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c358>}},\n",
       "   1650: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c3c8>}},\n",
       "   1652: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c438>}},\n",
       "   1654: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c4a8>}},\n",
       "   1656: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c518>}},\n",
       "   1658: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c588>}},\n",
       "   1660: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c5f8>}},\n",
       "   1662: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c668>}},\n",
       "   1664: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c6d8>}},\n",
       "   1666: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c748>}},\n",
       "   1668: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c7b8>}},\n",
       "   1670: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c828>}},\n",
       "   1672: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c898>}},\n",
       "   1674: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c908>}},\n",
       "   1676: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c978>}},\n",
       "   1678: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9c9e8>}},\n",
       "   1680: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9ca58>}},\n",
       "   1682: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cac8>}},\n",
       "   1684: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cb38>}},\n",
       "   1686: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cba8>}},\n",
       "   1688: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cc18>}},\n",
       "   1690: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cc88>}},\n",
       "   1692: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9ccf8>}},\n",
       "   1694: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cd68>}},\n",
       "   1696: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cdd8>}},\n",
       "   1698: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9ce48>}},\n",
       "   1700: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9ceb8>}},\n",
       "   1702: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cf28>}},\n",
       "   1704: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463af9cf98>}},\n",
       "   1706: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026048>}},\n",
       "   1708: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0260b8>}},\n",
       "   1710: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026128>}},\n",
       "   1712: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026198>}},\n",
       "   1714: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026208>}},\n",
       "   1716: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026278>}},\n",
       "   1718: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0262e8>}},\n",
       "   1720: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026358>}},\n",
       "   1722: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0263c8>}},\n",
       "   1724: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026438>}},\n",
       "   1726: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0264a8>}},\n",
       "   1728: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026518>}},\n",
       "   1730: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026588>}},\n",
       "   1732: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0265f8>}},\n",
       "   1734: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026668>}},\n",
       "   1736: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0266d8>}},\n",
       "   1738: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026748>}},\n",
       "   1740: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0267b8>}},\n",
       "   1742: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026828>}},\n",
       "   1744: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026898>}},\n",
       "   1746: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026908>}},\n",
       "   1748: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026978>}},\n",
       "   1750: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0269e8>}},\n",
       "   1752: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026a58>}},\n",
       "   1754: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026ac8>}},\n",
       "   1756: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026b38>}},\n",
       "   1758: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026ba8>}},\n",
       "   1760: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026c18>}},\n",
       "   1762: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026c88>}},\n",
       "   1764: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026cf8>}},\n",
       "   1766: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026d68>}},\n",
       "   1768: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026dd8>}},\n",
       "   1770: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026e48>}},\n",
       "   1772: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026eb8>}},\n",
       "   1774: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026f28>}},\n",
       "   1776: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b026f98>}},\n",
       "   1778: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029048>}},\n",
       "   1780: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0290b8>}},\n",
       "   1782: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029128>}},\n",
       "   1784: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029198>}},\n",
       "   1786: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029208>}},\n",
       "   1788: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029278>}},\n",
       "   1790: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0292e8>}},\n",
       "   1792: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029358>}},\n",
       "   1794: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0293c8>}},\n",
       "   1796: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029438>}},\n",
       "   1798: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0294a8>}},\n",
       "   1800: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029518>}},\n",
       "   1802: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029588>}},\n",
       "   1804: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0295f8>}},\n",
       "   1806: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029668>}},\n",
       "   1808: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0296d8>}},\n",
       "   1810: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029748>}},\n",
       "   1812: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0297b8>}},\n",
       "   1814: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029828>}},\n",
       "   1816: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029898>}},\n",
       "   1818: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029908>}},\n",
       "   1820: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029978>}},\n",
       "   1822: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0299e8>}},\n",
       "   1824: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029a58>}},\n",
       "   1826: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029ac8>}},\n",
       "   1828: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029b38>}},\n",
       "   1830: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029ba8>}},\n",
       "   1832: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029c18>}},\n",
       "   1834: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029c88>}},\n",
       "   1836: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029cf8>}},\n",
       "   1838: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029d68>}},\n",
       "   1840: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029dd8>}},\n",
       "   1842: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029e48>}},\n",
       "   1844: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029eb8>}},\n",
       "   1846: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029f28>}},\n",
       "   1848: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b029f98>}},\n",
       "   1850: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d048>}},\n",
       "   1852: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d0b8>}},\n",
       "   1854: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d128>}},\n",
       "   1856: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d198>}},\n",
       "   1858: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d208>}},\n",
       "   1860: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d278>}},\n",
       "   1862: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d2e8>}},\n",
       "   1864: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d358>}},\n",
       "   1866: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d3c8>}},\n",
       "   1868: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d438>}},\n",
       "   1870: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d4a8>}},\n",
       "   1872: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d518>}},\n",
       "   1874: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d588>}},\n",
       "   1876: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d5f8>}},\n",
       "   1878: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d668>}},\n",
       "   1880: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d6d8>}},\n",
       "   1882: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d748>}},\n",
       "   1884: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d7b8>}},\n",
       "   1886: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d828>}},\n",
       "   1888: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d898>}},\n",
       "   1890: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d908>}},\n",
       "   1892: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d978>}},\n",
       "   1894: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00d9e8>}},\n",
       "   1896: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00da58>}},\n",
       "   1898: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00dac8>}},\n",
       "   1900: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00db38>}},\n",
       "   1902: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00dba8>}},\n",
       "   1904: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00dc18>}},\n",
       "   1906: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00dc88>}},\n",
       "   1908: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00dcf8>}},\n",
       "   1910: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00dd68>}},\n",
       "   1912: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00ddd8>}},\n",
       "   1914: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00de48>}},\n",
       "   1916: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00deb8>}},\n",
       "   1918: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00df28>}},\n",
       "   1920: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b00df98>}},\n",
       "   1922: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068048>}},\n",
       "   1924: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0680b8>}},\n",
       "   1926: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068128>}},\n",
       "   1928: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068198>}},\n",
       "   1930: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068208>}},\n",
       "   1932: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068278>}},\n",
       "   1934: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0682e8>}},\n",
       "   1936: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068358>}},\n",
       "   1938: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0683c8>}},\n",
       "   1940: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068438>}},\n",
       "   1942: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0684a8>}},\n",
       "   1944: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068518>}},\n",
       "   1946: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068588>}},\n",
       "   1948: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0685f8>}},\n",
       "   1950: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068668>}},\n",
       "   1952: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0686d8>}},\n",
       "   1954: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068748>}},\n",
       "   1956: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0687b8>}},\n",
       "   1958: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068828>}},\n",
       "   1960: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068898>}},\n",
       "   1962: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068908>}},\n",
       "   1964: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068978>}},\n",
       "   1966: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b0689e8>}},\n",
       "   1968: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068a58>}},\n",
       "   1970: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068ac8>}},\n",
       "   1972: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068b38>}},\n",
       "   1974: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068ba8>}},\n",
       "   1976: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068c18>}},\n",
       "   1978: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068c88>}},\n",
       "   1980: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068cf8>}},\n",
       "   1982: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068d68>}},\n",
       "   1984: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068dd8>}},\n",
       "   1986: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068e48>}},\n",
       "   1988: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068eb8>}},\n",
       "   1990: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068f28>}},\n",
       "   1992: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b068f98>}},\n",
       "   1994: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b07a048>}},\n",
       "   1996: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b07a0b8>}},\n",
       "   1998: {'worker_0': {'tensor_location': <smdebug.core.locations.TensorLocation at 0x7f463b07a128>}},\n",
       "   ...}},\n",
       " 'index_mode': True,\n",
       " 'last_event_token': None,\n",
       " 'last_index_token': 'debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/index/000000006/000000006998_worker_0.json',\n",
       " 'worker_set': {'worker_0'},\n",
       " 'global_step_to_tensors_map': {0: {'train-error', 'validation-error'},\n",
       "  2: {'train-error', 'validation-error'},\n",
       "  4: {'train-error', 'validation-error'},\n",
       "  6: {'train-error', 'validation-error'},\n",
       "  8: {'train-error', 'validation-error'},\n",
       "  10: {'train-error', 'validation-error'},\n",
       "  12: {'train-error', 'validation-error'},\n",
       "  14: {'train-error', 'validation-error'},\n",
       "  16: {'train-error', 'validation-error'},\n",
       "  18: {'train-error', 'validation-error'},\n",
       "  20: {'train-error', 'validation-error'},\n",
       "  22: {'train-error', 'validation-error'},\n",
       "  24: {'train-error', 'validation-error'},\n",
       "  26: {'train-error', 'validation-error'},\n",
       "  28: {'train-error', 'validation-error'},\n",
       "  30: {'train-error', 'validation-error'},\n",
       "  32: {'train-error', 'validation-error'},\n",
       "  34: {'train-error', 'validation-error'},\n",
       "  36: {'train-error', 'validation-error'},\n",
       "  38: {'train-error', 'validation-error'},\n",
       "  40: {'train-error', 'validation-error'},\n",
       "  42: {'train-error', 'validation-error'},\n",
       "  44: {'train-error', 'validation-error'},\n",
       "  46: {'train-error', 'validation-error'},\n",
       "  48: {'train-error', 'validation-error'},\n",
       "  50: {'train-error', 'validation-error'},\n",
       "  52: {'train-error', 'validation-error'},\n",
       "  54: {'train-error', 'validation-error'},\n",
       "  56: {'train-error', 'validation-error'},\n",
       "  58: {'train-error', 'validation-error'},\n",
       "  60: {'train-error', 'validation-error'},\n",
       "  62: {'train-error', 'validation-error'},\n",
       "  64: {'train-error', 'validation-error'},\n",
       "  66: {'train-error', 'validation-error'},\n",
       "  68: {'train-error', 'validation-error'},\n",
       "  70: {'train-error', 'validation-error'},\n",
       "  72: {'train-error', 'validation-error'},\n",
       "  74: {'train-error', 'validation-error'},\n",
       "  76: {'train-error', 'validation-error'},\n",
       "  78: {'train-error', 'validation-error'},\n",
       "  80: {'train-error', 'validation-error'},\n",
       "  82: {'train-error', 'validation-error'},\n",
       "  84: {'train-error', 'validation-error'},\n",
       "  86: {'train-error', 'validation-error'},\n",
       "  88: {'train-error', 'validation-error'},\n",
       "  90: {'train-error', 'validation-error'},\n",
       "  92: {'train-error', 'validation-error'},\n",
       "  94: {'train-error', 'validation-error'},\n",
       "  96: {'train-error', 'validation-error'},\n",
       "  98: {'train-error', 'validation-error'},\n",
       "  100: {'train-error', 'validation-error'},\n",
       "  102: {'train-error', 'validation-error'},\n",
       "  104: {'train-error', 'validation-error'},\n",
       "  106: {'train-error', 'validation-error'},\n",
       "  108: {'train-error', 'validation-error'},\n",
       "  110: {'train-error', 'validation-error'},\n",
       "  112: {'train-error', 'validation-error'},\n",
       "  114: {'train-error', 'validation-error'},\n",
       "  116: {'train-error', 'validation-error'},\n",
       "  118: {'train-error', 'validation-error'},\n",
       "  120: {'train-error', 'validation-error'},\n",
       "  122: {'train-error', 'validation-error'},\n",
       "  124: {'train-error', 'validation-error'},\n",
       "  126: {'train-error', 'validation-error'},\n",
       "  128: {'train-error', 'validation-error'},\n",
       "  130: {'train-error', 'validation-error'},\n",
       "  132: {'train-error', 'validation-error'},\n",
       "  134: {'train-error', 'validation-error'},\n",
       "  136: {'train-error', 'validation-error'},\n",
       "  138: {'train-error', 'validation-error'},\n",
       "  140: {'train-error', 'validation-error'},\n",
       "  142: {'train-error', 'validation-error'},\n",
       "  144: {'train-error', 'validation-error'},\n",
       "  146: {'train-error', 'validation-error'},\n",
       "  148: {'train-error', 'validation-error'},\n",
       "  150: {'train-error', 'validation-error'},\n",
       "  152: {'train-error', 'validation-error'},\n",
       "  154: {'train-error', 'validation-error'},\n",
       "  156: {'train-error', 'validation-error'},\n",
       "  158: {'train-error', 'validation-error'},\n",
       "  160: {'train-error', 'validation-error'},\n",
       "  162: {'train-error', 'validation-error'},\n",
       "  164: {'train-error', 'validation-error'},\n",
       "  166: {'train-error', 'validation-error'},\n",
       "  168: {'train-error', 'validation-error'},\n",
       "  170: {'train-error', 'validation-error'},\n",
       "  172: {'train-error', 'validation-error'},\n",
       "  174: {'train-error', 'validation-error'},\n",
       "  176: {'train-error', 'validation-error'},\n",
       "  178: {'train-error', 'validation-error'},\n",
       "  180: {'train-error', 'validation-error'},\n",
       "  182: {'train-error', 'validation-error'},\n",
       "  184: {'train-error', 'validation-error'},\n",
       "  186: {'train-error', 'validation-error'},\n",
       "  188: {'train-error', 'validation-error'},\n",
       "  190: {'train-error', 'validation-error'},\n",
       "  192: {'train-error', 'validation-error'},\n",
       "  194: {'train-error', 'validation-error'},\n",
       "  196: {'train-error', 'validation-error'},\n",
       "  198: {'train-error', 'validation-error'},\n",
       "  200: {'train-error', 'validation-error'},\n",
       "  202: {'train-error', 'validation-error'},\n",
       "  204: {'train-error', 'validation-error'},\n",
       "  206: {'train-error', 'validation-error'},\n",
       "  208: {'train-error', 'validation-error'},\n",
       "  210: {'train-error', 'validation-error'},\n",
       "  212: {'train-error', 'validation-error'},\n",
       "  214: {'train-error', 'validation-error'},\n",
       "  216: {'train-error', 'validation-error'},\n",
       "  218: {'train-error', 'validation-error'},\n",
       "  220: {'train-error', 'validation-error'},\n",
       "  222: {'train-error', 'validation-error'},\n",
       "  224: {'train-error', 'validation-error'},\n",
       "  226: {'train-error', 'validation-error'},\n",
       "  228: {'train-error', 'validation-error'},\n",
       "  230: {'train-error', 'validation-error'},\n",
       "  232: {'train-error', 'validation-error'},\n",
       "  234: {'train-error', 'validation-error'},\n",
       "  236: {'train-error', 'validation-error'},\n",
       "  238: {'train-error', 'validation-error'},\n",
       "  240: {'train-error', 'validation-error'},\n",
       "  242: {'train-error', 'validation-error'},\n",
       "  244: {'train-error', 'validation-error'},\n",
       "  246: {'train-error', 'validation-error'},\n",
       "  248: {'train-error', 'validation-error'},\n",
       "  250: {'train-error', 'validation-error'},\n",
       "  252: {'train-error', 'validation-error'},\n",
       "  254: {'train-error', 'validation-error'},\n",
       "  256: {'train-error', 'validation-error'},\n",
       "  258: {'train-error', 'validation-error'},\n",
       "  260: {'train-error', 'validation-error'},\n",
       "  262: {'train-error', 'validation-error'},\n",
       "  264: {'train-error', 'validation-error'},\n",
       "  266: {'train-error', 'validation-error'},\n",
       "  268: {'train-error', 'validation-error'},\n",
       "  270: {'train-error', 'validation-error'},\n",
       "  272: {'train-error', 'validation-error'},\n",
       "  274: {'train-error', 'validation-error'},\n",
       "  276: {'train-error', 'validation-error'},\n",
       "  278: {'train-error', 'validation-error'},\n",
       "  280: {'train-error', 'validation-error'},\n",
       "  282: {'train-error', 'validation-error'},\n",
       "  284: {'train-error', 'validation-error'},\n",
       "  286: {'train-error', 'validation-error'},\n",
       "  288: {'train-error', 'validation-error'},\n",
       "  290: {'train-error', 'validation-error'},\n",
       "  292: {'train-error', 'validation-error'},\n",
       "  294: {'train-error', 'validation-error'},\n",
       "  296: {'train-error', 'validation-error'},\n",
       "  298: {'train-error', 'validation-error'},\n",
       "  300: {'train-error', 'validation-error'},\n",
       "  302: {'train-error', 'validation-error'},\n",
       "  304: {'train-error', 'validation-error'},\n",
       "  306: {'train-error', 'validation-error'},\n",
       "  308: {'train-error', 'validation-error'},\n",
       "  310: {'train-error', 'validation-error'},\n",
       "  312: {'train-error', 'validation-error'},\n",
       "  314: {'train-error', 'validation-error'},\n",
       "  316: {'train-error', 'validation-error'},\n",
       "  318: {'train-error', 'validation-error'},\n",
       "  320: {'train-error', 'validation-error'},\n",
       "  322: {'train-error', 'validation-error'},\n",
       "  324: {'train-error', 'validation-error'},\n",
       "  326: {'train-error', 'validation-error'},\n",
       "  328: {'train-error', 'validation-error'},\n",
       "  330: {'train-error', 'validation-error'},\n",
       "  332: {'train-error', 'validation-error'},\n",
       "  334: {'train-error', 'validation-error'},\n",
       "  336: {'train-error', 'validation-error'},\n",
       "  338: {'train-error', 'validation-error'},\n",
       "  340: {'train-error', 'validation-error'},\n",
       "  342: {'train-error', 'validation-error'},\n",
       "  344: {'train-error', 'validation-error'},\n",
       "  346: {'train-error', 'validation-error'},\n",
       "  348: {'train-error', 'validation-error'},\n",
       "  350: {'train-error', 'validation-error'},\n",
       "  352: {'train-error', 'validation-error'},\n",
       "  354: {'train-error', 'validation-error'},\n",
       "  356: {'train-error', 'validation-error'},\n",
       "  358: {'train-error', 'validation-error'},\n",
       "  360: {'train-error', 'validation-error'},\n",
       "  362: {'train-error', 'validation-error'},\n",
       "  364: {'train-error', 'validation-error'},\n",
       "  366: {'train-error', 'validation-error'},\n",
       "  368: {'train-error', 'validation-error'},\n",
       "  370: {'train-error', 'validation-error'},\n",
       "  372: {'train-error', 'validation-error'},\n",
       "  374: {'train-error', 'validation-error'},\n",
       "  376: {'train-error', 'validation-error'},\n",
       "  378: {'train-error', 'validation-error'},\n",
       "  380: {'train-error', 'validation-error'},\n",
       "  382: {'train-error', 'validation-error'},\n",
       "  384: {'train-error', 'validation-error'},\n",
       "  386: {'train-error', 'validation-error'},\n",
       "  388: {'train-error', 'validation-error'},\n",
       "  390: {'train-error', 'validation-error'},\n",
       "  392: {'train-error', 'validation-error'},\n",
       "  394: {'train-error', 'validation-error'},\n",
       "  396: {'train-error', 'validation-error'},\n",
       "  398: {'train-error', 'validation-error'},\n",
       "  400: {'train-error', 'validation-error'},\n",
       "  402: {'train-error', 'validation-error'},\n",
       "  404: {'train-error', 'validation-error'},\n",
       "  406: {'train-error', 'validation-error'},\n",
       "  408: {'train-error', 'validation-error'},\n",
       "  410: {'train-error', 'validation-error'},\n",
       "  412: {'train-error', 'validation-error'},\n",
       "  414: {'train-error', 'validation-error'},\n",
       "  416: {'train-error', 'validation-error'},\n",
       "  418: {'train-error', 'validation-error'},\n",
       "  420: {'train-error', 'validation-error'},\n",
       "  422: {'train-error', 'validation-error'},\n",
       "  424: {'train-error', 'validation-error'},\n",
       "  426: {'train-error', 'validation-error'},\n",
       "  428: {'train-error', 'validation-error'},\n",
       "  430: {'train-error', 'validation-error'},\n",
       "  432: {'train-error', 'validation-error'},\n",
       "  434: {'train-error', 'validation-error'},\n",
       "  436: {'train-error', 'validation-error'},\n",
       "  438: {'train-error', 'validation-error'},\n",
       "  440: {'train-error', 'validation-error'},\n",
       "  442: {'train-error', 'validation-error'},\n",
       "  444: {'train-error', 'validation-error'},\n",
       "  446: {'train-error', 'validation-error'},\n",
       "  448: {'train-error', 'validation-error'},\n",
       "  450: {'train-error', 'validation-error'},\n",
       "  452: {'train-error', 'validation-error'},\n",
       "  454: {'train-error', 'validation-error'},\n",
       "  456: {'train-error', 'validation-error'},\n",
       "  458: {'train-error', 'validation-error'},\n",
       "  460: {'train-error', 'validation-error'},\n",
       "  462: {'train-error', 'validation-error'},\n",
       "  464: {'train-error', 'validation-error'},\n",
       "  466: {'train-error', 'validation-error'},\n",
       "  468: {'train-error', 'validation-error'},\n",
       "  470: {'train-error', 'validation-error'},\n",
       "  472: {'train-error', 'validation-error'},\n",
       "  474: {'train-error', 'validation-error'},\n",
       "  476: {'train-error', 'validation-error'},\n",
       "  478: {'train-error', 'validation-error'},\n",
       "  480: {'train-error', 'validation-error'},\n",
       "  482: {'train-error', 'validation-error'},\n",
       "  484: {'train-error', 'validation-error'},\n",
       "  486: {'train-error', 'validation-error'},\n",
       "  488: {'train-error', 'validation-error'},\n",
       "  490: {'train-error', 'validation-error'},\n",
       "  492: {'train-error', 'validation-error'},\n",
       "  494: {'train-error', 'validation-error'},\n",
       "  496: {'train-error', 'validation-error'},\n",
       "  498: {'train-error', 'validation-error'},\n",
       "  500: {'train-error', 'validation-error'},\n",
       "  502: {'train-error', 'validation-error'},\n",
       "  504: {'train-error', 'validation-error'},\n",
       "  506: {'train-error', 'validation-error'},\n",
       "  508: {'train-error', 'validation-error'},\n",
       "  510: {'train-error', 'validation-error'},\n",
       "  512: {'train-error', 'validation-error'},\n",
       "  514: {'train-error', 'validation-error'},\n",
       "  516: {'train-error', 'validation-error'},\n",
       "  518: {'train-error', 'validation-error'},\n",
       "  520: {'train-error', 'validation-error'},\n",
       "  522: {'train-error', 'validation-error'},\n",
       "  524: {'train-error', 'validation-error'},\n",
       "  526: {'train-error', 'validation-error'},\n",
       "  528: {'train-error', 'validation-error'},\n",
       "  530: {'train-error', 'validation-error'},\n",
       "  532: {'train-error', 'validation-error'},\n",
       "  534: {'train-error', 'validation-error'},\n",
       "  536: {'train-error', 'validation-error'},\n",
       "  538: {'train-error', 'validation-error'},\n",
       "  540: {'train-error', 'validation-error'},\n",
       "  542: {'train-error', 'validation-error'},\n",
       "  544: {'train-error', 'validation-error'},\n",
       "  546: {'train-error', 'validation-error'},\n",
       "  548: {'train-error', 'validation-error'},\n",
       "  550: {'train-error', 'validation-error'},\n",
       "  552: {'train-error', 'validation-error'},\n",
       "  554: {'train-error', 'validation-error'},\n",
       "  556: {'train-error', 'validation-error'},\n",
       "  558: {'train-error', 'validation-error'},\n",
       "  560: {'train-error', 'validation-error'},\n",
       "  562: {'train-error', 'validation-error'},\n",
       "  564: {'train-error', 'validation-error'},\n",
       "  566: {'train-error', 'validation-error'},\n",
       "  568: {'train-error', 'validation-error'},\n",
       "  570: {'train-error', 'validation-error'},\n",
       "  572: {'train-error', 'validation-error'},\n",
       "  574: {'train-error', 'validation-error'},\n",
       "  576: {'train-error', 'validation-error'},\n",
       "  578: {'train-error', 'validation-error'},\n",
       "  580: {'train-error', 'validation-error'},\n",
       "  582: {'train-error', 'validation-error'},\n",
       "  584: {'train-error', 'validation-error'},\n",
       "  586: {'train-error', 'validation-error'},\n",
       "  588: {'train-error', 'validation-error'},\n",
       "  590: {'train-error', 'validation-error'},\n",
       "  592: {'train-error', 'validation-error'},\n",
       "  594: {'train-error', 'validation-error'},\n",
       "  596: {'train-error', 'validation-error'},\n",
       "  598: {'train-error', 'validation-error'},\n",
       "  600: {'train-error', 'validation-error'},\n",
       "  602: {'train-error', 'validation-error'},\n",
       "  604: {'train-error', 'validation-error'},\n",
       "  606: {'train-error', 'validation-error'},\n",
       "  608: {'train-error', 'validation-error'},\n",
       "  610: {'train-error', 'validation-error'},\n",
       "  612: {'train-error', 'validation-error'},\n",
       "  614: {'train-error', 'validation-error'},\n",
       "  616: {'train-error', 'validation-error'},\n",
       "  618: {'train-error', 'validation-error'},\n",
       "  620: {'train-error', 'validation-error'},\n",
       "  622: {'train-error', 'validation-error'},\n",
       "  624: {'train-error', 'validation-error'},\n",
       "  626: {'train-error', 'validation-error'},\n",
       "  628: {'train-error', 'validation-error'},\n",
       "  630: {'train-error', 'validation-error'},\n",
       "  632: {'train-error', 'validation-error'},\n",
       "  634: {'train-error', 'validation-error'},\n",
       "  636: {'train-error', 'validation-error'},\n",
       "  638: {'train-error', 'validation-error'},\n",
       "  640: {'train-error', 'validation-error'},\n",
       "  642: {'train-error', 'validation-error'},\n",
       "  644: {'train-error', 'validation-error'},\n",
       "  646: {'train-error', 'validation-error'},\n",
       "  648: {'train-error', 'validation-error'},\n",
       "  650: {'train-error', 'validation-error'},\n",
       "  652: {'train-error', 'validation-error'},\n",
       "  654: {'train-error', 'validation-error'},\n",
       "  656: {'train-error', 'validation-error'},\n",
       "  658: {'train-error', 'validation-error'},\n",
       "  660: {'train-error', 'validation-error'},\n",
       "  662: {'train-error', 'validation-error'},\n",
       "  664: {'train-error', 'validation-error'},\n",
       "  666: {'train-error', 'validation-error'},\n",
       "  668: {'train-error', 'validation-error'},\n",
       "  670: {'train-error', 'validation-error'},\n",
       "  672: {'train-error', 'validation-error'},\n",
       "  674: {'train-error', 'validation-error'},\n",
       "  676: {'train-error', 'validation-error'},\n",
       "  678: {'train-error', 'validation-error'},\n",
       "  680: {'train-error', 'validation-error'},\n",
       "  682: {'train-error', 'validation-error'},\n",
       "  684: {'train-error', 'validation-error'},\n",
       "  686: {'train-error', 'validation-error'},\n",
       "  688: {'train-error', 'validation-error'},\n",
       "  690: {'train-error', 'validation-error'},\n",
       "  692: {'train-error', 'validation-error'},\n",
       "  694: {'train-error', 'validation-error'},\n",
       "  696: {'train-error', 'validation-error'},\n",
       "  698: {'train-error', 'validation-error'},\n",
       "  700: {'train-error', 'validation-error'},\n",
       "  702: {'train-error', 'validation-error'},\n",
       "  704: {'train-error', 'validation-error'},\n",
       "  706: {'train-error', 'validation-error'},\n",
       "  708: {'train-error', 'validation-error'},\n",
       "  710: {'train-error', 'validation-error'},\n",
       "  712: {'train-error', 'validation-error'},\n",
       "  714: {'train-error', 'validation-error'},\n",
       "  716: {'train-error', 'validation-error'},\n",
       "  718: {'train-error', 'validation-error'},\n",
       "  720: {'train-error', 'validation-error'},\n",
       "  722: {'train-error', 'validation-error'},\n",
       "  724: {'train-error', 'validation-error'},\n",
       "  726: {'train-error', 'validation-error'},\n",
       "  728: {'train-error', 'validation-error'},\n",
       "  730: {'train-error', 'validation-error'},\n",
       "  732: {'train-error', 'validation-error'},\n",
       "  734: {'train-error', 'validation-error'},\n",
       "  736: {'train-error', 'validation-error'},\n",
       "  738: {'train-error', 'validation-error'},\n",
       "  740: {'train-error', 'validation-error'},\n",
       "  742: {'train-error', 'validation-error'},\n",
       "  744: {'train-error', 'validation-error'},\n",
       "  746: {'train-error', 'validation-error'},\n",
       "  748: {'train-error', 'validation-error'},\n",
       "  750: {'train-error', 'validation-error'},\n",
       "  752: {'train-error', 'validation-error'},\n",
       "  754: {'train-error', 'validation-error'},\n",
       "  756: {'train-error', 'validation-error'},\n",
       "  758: {'train-error', 'validation-error'},\n",
       "  760: {'train-error', 'validation-error'},\n",
       "  762: {'train-error', 'validation-error'},\n",
       "  764: {'train-error', 'validation-error'},\n",
       "  766: {'train-error', 'validation-error'},\n",
       "  768: {'train-error', 'validation-error'},\n",
       "  770: {'train-error', 'validation-error'},\n",
       "  772: {'train-error', 'validation-error'},\n",
       "  774: {'train-error', 'validation-error'},\n",
       "  776: {'train-error', 'validation-error'},\n",
       "  778: {'train-error', 'validation-error'},\n",
       "  780: {'train-error', 'validation-error'},\n",
       "  782: {'train-error', 'validation-error'},\n",
       "  784: {'train-error', 'validation-error'},\n",
       "  786: {'train-error', 'validation-error'},\n",
       "  788: {'train-error', 'validation-error'},\n",
       "  790: {'train-error', 'validation-error'},\n",
       "  792: {'train-error', 'validation-error'},\n",
       "  794: {'train-error', 'validation-error'},\n",
       "  796: {'train-error', 'validation-error'},\n",
       "  798: {'train-error', 'validation-error'},\n",
       "  800: {'train-error', 'validation-error'},\n",
       "  802: {'train-error', 'validation-error'},\n",
       "  804: {'train-error', 'validation-error'},\n",
       "  806: {'train-error', 'validation-error'},\n",
       "  808: {'train-error', 'validation-error'},\n",
       "  810: {'train-error', 'validation-error'},\n",
       "  812: {'train-error', 'validation-error'},\n",
       "  814: {'train-error', 'validation-error'},\n",
       "  816: {'train-error', 'validation-error'},\n",
       "  818: {'train-error', 'validation-error'},\n",
       "  820: {'train-error', 'validation-error'},\n",
       "  822: {'train-error', 'validation-error'},\n",
       "  824: {'train-error', 'validation-error'},\n",
       "  826: {'train-error', 'validation-error'},\n",
       "  828: {'train-error', 'validation-error'},\n",
       "  830: {'train-error', 'validation-error'},\n",
       "  832: {'train-error', 'validation-error'},\n",
       "  834: {'train-error', 'validation-error'},\n",
       "  836: {'train-error', 'validation-error'},\n",
       "  838: {'train-error', 'validation-error'},\n",
       "  840: {'train-error', 'validation-error'},\n",
       "  842: {'train-error', 'validation-error'},\n",
       "  844: {'train-error', 'validation-error'},\n",
       "  846: {'train-error', 'validation-error'},\n",
       "  848: {'train-error', 'validation-error'},\n",
       "  850: {'train-error', 'validation-error'},\n",
       "  852: {'train-error', 'validation-error'},\n",
       "  854: {'train-error', 'validation-error'},\n",
       "  856: {'train-error', 'validation-error'},\n",
       "  858: {'train-error', 'validation-error'},\n",
       "  860: {'train-error', 'validation-error'},\n",
       "  862: {'train-error', 'validation-error'},\n",
       "  864: {'train-error', 'validation-error'},\n",
       "  866: {'train-error', 'validation-error'},\n",
       "  868: {'train-error', 'validation-error'},\n",
       "  870: {'train-error', 'validation-error'},\n",
       "  872: {'train-error', 'validation-error'},\n",
       "  874: {'train-error', 'validation-error'},\n",
       "  876: {'train-error', 'validation-error'},\n",
       "  878: {'train-error', 'validation-error'},\n",
       "  880: {'train-error', 'validation-error'},\n",
       "  882: {'train-error', 'validation-error'},\n",
       "  884: {'train-error', 'validation-error'},\n",
       "  886: {'train-error', 'validation-error'},\n",
       "  888: {'train-error', 'validation-error'},\n",
       "  890: {'train-error', 'validation-error'},\n",
       "  892: {'train-error', 'validation-error'},\n",
       "  894: {'train-error', 'validation-error'},\n",
       "  896: {'train-error', 'validation-error'},\n",
       "  898: {'train-error', 'validation-error'},\n",
       "  900: {'train-error', 'validation-error'},\n",
       "  902: {'train-error', 'validation-error'},\n",
       "  904: {'train-error', 'validation-error'},\n",
       "  906: {'train-error', 'validation-error'},\n",
       "  908: {'train-error', 'validation-error'},\n",
       "  910: {'train-error', 'validation-error'},\n",
       "  912: {'train-error', 'validation-error'},\n",
       "  914: {'train-error', 'validation-error'},\n",
       "  916: {'train-error', 'validation-error'},\n",
       "  918: {'train-error', 'validation-error'},\n",
       "  920: {'train-error', 'validation-error'},\n",
       "  922: {'train-error', 'validation-error'},\n",
       "  924: {'train-error', 'validation-error'},\n",
       "  926: {'train-error', 'validation-error'},\n",
       "  928: {'train-error', 'validation-error'},\n",
       "  930: {'train-error', 'validation-error'},\n",
       "  932: {'train-error', 'validation-error'},\n",
       "  934: {'train-error', 'validation-error'},\n",
       "  936: {'train-error', 'validation-error'},\n",
       "  938: {'train-error', 'validation-error'},\n",
       "  940: {'train-error', 'validation-error'},\n",
       "  942: {'train-error', 'validation-error'},\n",
       "  944: {'train-error', 'validation-error'},\n",
       "  946: {'train-error', 'validation-error'},\n",
       "  948: {'train-error', 'validation-error'},\n",
       "  950: {'train-error', 'validation-error'},\n",
       "  952: {'train-error', 'validation-error'},\n",
       "  954: {'train-error', 'validation-error'},\n",
       "  956: {'train-error', 'validation-error'},\n",
       "  958: {'train-error', 'validation-error'},\n",
       "  960: {'train-error', 'validation-error'},\n",
       "  962: {'train-error', 'validation-error'},\n",
       "  964: {'train-error', 'validation-error'},\n",
       "  966: {'train-error', 'validation-error'},\n",
       "  968: {'train-error', 'validation-error'},\n",
       "  970: {'train-error', 'validation-error'},\n",
       "  972: {'train-error', 'validation-error'},\n",
       "  974: {'train-error', 'validation-error'},\n",
       "  976: {'train-error', 'validation-error'},\n",
       "  978: {'train-error', 'validation-error'},\n",
       "  980: {'train-error', 'validation-error'},\n",
       "  982: {'train-error', 'validation-error'},\n",
       "  984: {'train-error', 'validation-error'},\n",
       "  986: {'train-error', 'validation-error'},\n",
       "  988: {'train-error', 'validation-error'},\n",
       "  990: {'train-error', 'validation-error'},\n",
       "  992: {'train-error', 'validation-error'},\n",
       "  994: {'train-error', 'validation-error'},\n",
       "  996: {'train-error', 'validation-error'},\n",
       "  998: {'train-error', 'validation-error'},\n",
       "  1000: {'train-error', 'validation-error'},\n",
       "  1002: {'train-error', 'validation-error'},\n",
       "  1004: {'train-error', 'validation-error'},\n",
       "  1006: {'train-error', 'validation-error'},\n",
       "  1008: {'train-error', 'validation-error'},\n",
       "  1010: {'train-error', 'validation-error'},\n",
       "  1012: {'train-error', 'validation-error'},\n",
       "  1014: {'train-error', 'validation-error'},\n",
       "  1016: {'train-error', 'validation-error'},\n",
       "  1018: {'train-error', 'validation-error'},\n",
       "  1020: {'train-error', 'validation-error'},\n",
       "  1022: {'train-error', 'validation-error'},\n",
       "  1024: {'train-error', 'validation-error'},\n",
       "  1026: {'train-error', 'validation-error'},\n",
       "  1028: {'train-error', 'validation-error'},\n",
       "  1030: {'train-error', 'validation-error'},\n",
       "  1032: {'train-error', 'validation-error'},\n",
       "  1034: {'train-error', 'validation-error'},\n",
       "  1036: {'train-error', 'validation-error'},\n",
       "  1038: {'train-error', 'validation-error'},\n",
       "  1040: {'train-error', 'validation-error'},\n",
       "  1042: {'train-error', 'validation-error'},\n",
       "  1044: {'train-error', 'validation-error'},\n",
       "  1046: {'train-error', 'validation-error'},\n",
       "  1048: {'train-error', 'validation-error'},\n",
       "  1050: {'train-error', 'validation-error'},\n",
       "  1052: {'train-error', 'validation-error'},\n",
       "  1054: {'train-error', 'validation-error'},\n",
       "  1056: {'train-error', 'validation-error'},\n",
       "  1058: {'train-error', 'validation-error'},\n",
       "  1060: {'train-error', 'validation-error'},\n",
       "  1062: {'train-error', 'validation-error'},\n",
       "  1064: {'train-error', 'validation-error'},\n",
       "  1066: {'train-error', 'validation-error'},\n",
       "  1068: {'train-error', 'validation-error'},\n",
       "  1070: {'train-error', 'validation-error'},\n",
       "  1072: {'train-error', 'validation-error'},\n",
       "  1074: {'train-error', 'validation-error'},\n",
       "  1076: {'train-error', 'validation-error'},\n",
       "  1078: {'train-error', 'validation-error'},\n",
       "  1080: {'train-error', 'validation-error'},\n",
       "  1082: {'train-error', 'validation-error'},\n",
       "  1084: {'train-error', 'validation-error'},\n",
       "  1086: {'train-error', 'validation-error'},\n",
       "  1088: {'train-error', 'validation-error'},\n",
       "  1090: {'train-error', 'validation-error'},\n",
       "  1092: {'train-error', 'validation-error'},\n",
       "  1094: {'train-error', 'validation-error'},\n",
       "  1096: {'train-error', 'validation-error'},\n",
       "  1098: {'train-error', 'validation-error'},\n",
       "  1100: {'train-error', 'validation-error'},\n",
       "  1102: {'train-error', 'validation-error'},\n",
       "  1104: {'train-error', 'validation-error'},\n",
       "  1106: {'train-error', 'validation-error'},\n",
       "  1108: {'train-error', 'validation-error'},\n",
       "  1110: {'train-error', 'validation-error'},\n",
       "  1112: {'train-error', 'validation-error'},\n",
       "  1114: {'train-error', 'validation-error'},\n",
       "  1116: {'train-error', 'validation-error'},\n",
       "  1118: {'train-error', 'validation-error'},\n",
       "  1120: {'train-error', 'validation-error'},\n",
       "  1122: {'train-error', 'validation-error'},\n",
       "  1124: {'train-error', 'validation-error'},\n",
       "  1126: {'train-error', 'validation-error'},\n",
       "  1128: {'train-error', 'validation-error'},\n",
       "  1130: {'train-error', 'validation-error'},\n",
       "  1132: {'train-error', 'validation-error'},\n",
       "  1134: {'train-error', 'validation-error'},\n",
       "  1136: {'train-error', 'validation-error'},\n",
       "  1138: {'train-error', 'validation-error'},\n",
       "  1140: {'train-error', 'validation-error'},\n",
       "  1142: {'train-error', 'validation-error'},\n",
       "  1144: {'train-error', 'validation-error'},\n",
       "  1146: {'train-error', 'validation-error'},\n",
       "  1148: {'train-error', 'validation-error'},\n",
       "  1150: {'train-error', 'validation-error'},\n",
       "  1152: {'train-error', 'validation-error'},\n",
       "  1154: {'train-error', 'validation-error'},\n",
       "  1156: {'train-error', 'validation-error'},\n",
       "  1158: {'train-error', 'validation-error'},\n",
       "  1160: {'train-error', 'validation-error'},\n",
       "  1162: {'train-error', 'validation-error'},\n",
       "  1164: {'train-error', 'validation-error'},\n",
       "  1166: {'train-error', 'validation-error'},\n",
       "  1168: {'train-error', 'validation-error'},\n",
       "  1170: {'train-error', 'validation-error'},\n",
       "  1172: {'train-error', 'validation-error'},\n",
       "  1174: {'train-error', 'validation-error'},\n",
       "  1176: {'train-error', 'validation-error'},\n",
       "  1178: {'train-error', 'validation-error'},\n",
       "  1180: {'train-error', 'validation-error'},\n",
       "  1182: {'train-error', 'validation-error'},\n",
       "  1184: {'train-error', 'validation-error'},\n",
       "  1186: {'train-error', 'validation-error'},\n",
       "  1188: {'train-error', 'validation-error'},\n",
       "  1190: {'train-error', 'validation-error'},\n",
       "  1192: {'train-error', 'validation-error'},\n",
       "  1194: {'train-error', 'validation-error'},\n",
       "  1196: {'train-error', 'validation-error'},\n",
       "  1198: {'train-error', 'validation-error'},\n",
       "  1200: {'train-error', 'validation-error'},\n",
       "  1202: {'train-error', 'validation-error'},\n",
       "  1204: {'train-error', 'validation-error'},\n",
       "  1206: {'train-error', 'validation-error'},\n",
       "  1208: {'train-error', 'validation-error'},\n",
       "  1210: {'train-error', 'validation-error'},\n",
       "  1212: {'train-error', 'validation-error'},\n",
       "  1214: {'train-error', 'validation-error'},\n",
       "  1216: {'train-error', 'validation-error'},\n",
       "  1218: {'train-error', 'validation-error'},\n",
       "  1220: {'train-error', 'validation-error'},\n",
       "  1222: {'train-error', 'validation-error'},\n",
       "  1224: {'train-error', 'validation-error'},\n",
       "  1226: {'train-error', 'validation-error'},\n",
       "  1228: {'train-error', 'validation-error'},\n",
       "  1230: {'train-error', 'validation-error'},\n",
       "  1232: {'train-error', 'validation-error'},\n",
       "  1234: {'train-error', 'validation-error'},\n",
       "  1236: {'train-error', 'validation-error'},\n",
       "  1238: {'train-error', 'validation-error'},\n",
       "  1240: {'train-error', 'validation-error'},\n",
       "  1242: {'train-error', 'validation-error'},\n",
       "  1244: {'train-error', 'validation-error'},\n",
       "  1246: {'train-error', 'validation-error'},\n",
       "  1248: {'train-error', 'validation-error'},\n",
       "  1250: {'train-error', 'validation-error'},\n",
       "  1252: {'train-error', 'validation-error'},\n",
       "  1254: {'train-error', 'validation-error'},\n",
       "  1256: {'train-error', 'validation-error'},\n",
       "  1258: {'train-error', 'validation-error'},\n",
       "  1260: {'train-error', 'validation-error'},\n",
       "  1262: {'train-error', 'validation-error'},\n",
       "  1264: {'train-error', 'validation-error'},\n",
       "  1266: {'train-error', 'validation-error'},\n",
       "  1268: {'train-error', 'validation-error'},\n",
       "  1270: {'train-error', 'validation-error'},\n",
       "  1272: {'train-error', 'validation-error'},\n",
       "  1274: {'train-error', 'validation-error'},\n",
       "  1276: {'train-error', 'validation-error'},\n",
       "  1278: {'train-error', 'validation-error'},\n",
       "  1280: {'train-error', 'validation-error'},\n",
       "  1282: {'train-error', 'validation-error'},\n",
       "  1284: {'train-error', 'validation-error'},\n",
       "  1286: {'train-error', 'validation-error'},\n",
       "  1288: {'train-error', 'validation-error'},\n",
       "  1290: {'train-error', 'validation-error'},\n",
       "  1292: {'train-error', 'validation-error'},\n",
       "  1294: {'train-error', 'validation-error'},\n",
       "  1296: {'train-error', 'validation-error'},\n",
       "  1298: {'train-error', 'validation-error'},\n",
       "  1300: {'train-error', 'validation-error'},\n",
       "  1302: {'train-error', 'validation-error'},\n",
       "  1304: {'train-error', 'validation-error'},\n",
       "  1306: {'train-error', 'validation-error'},\n",
       "  1308: {'train-error', 'validation-error'},\n",
       "  1310: {'train-error', 'validation-error'},\n",
       "  1312: {'train-error', 'validation-error'},\n",
       "  1314: {'train-error', 'validation-error'},\n",
       "  1316: {'train-error', 'validation-error'},\n",
       "  1318: {'train-error', 'validation-error'},\n",
       "  1320: {'train-error', 'validation-error'},\n",
       "  1322: {'train-error', 'validation-error'},\n",
       "  1324: {'train-error', 'validation-error'},\n",
       "  1326: {'train-error', 'validation-error'},\n",
       "  1328: {'train-error', 'validation-error'},\n",
       "  1330: {'train-error', 'validation-error'},\n",
       "  1332: {'train-error', 'validation-error'},\n",
       "  1334: {'train-error', 'validation-error'},\n",
       "  1336: {'train-error', 'validation-error'},\n",
       "  1338: {'train-error', 'validation-error'},\n",
       "  1340: {'train-error', 'validation-error'},\n",
       "  1342: {'train-error', 'validation-error'},\n",
       "  1344: {'train-error', 'validation-error'},\n",
       "  1346: {'train-error', 'validation-error'},\n",
       "  1348: {'train-error', 'validation-error'},\n",
       "  1350: {'train-error', 'validation-error'},\n",
       "  1352: {'train-error', 'validation-error'},\n",
       "  1354: {'train-error', 'validation-error'},\n",
       "  1356: {'train-error', 'validation-error'},\n",
       "  1358: {'train-error', 'validation-error'},\n",
       "  1360: {'train-error', 'validation-error'},\n",
       "  1362: {'train-error', 'validation-error'},\n",
       "  1364: {'train-error', 'validation-error'},\n",
       "  1366: {'train-error', 'validation-error'},\n",
       "  1368: {'train-error', 'validation-error'},\n",
       "  1370: {'train-error', 'validation-error'},\n",
       "  1372: {'train-error', 'validation-error'},\n",
       "  1374: {'train-error', 'validation-error'},\n",
       "  1376: {'train-error', 'validation-error'},\n",
       "  1378: {'train-error', 'validation-error'},\n",
       "  1380: {'train-error', 'validation-error'},\n",
       "  1382: {'train-error', 'validation-error'},\n",
       "  1384: {'train-error', 'validation-error'},\n",
       "  1386: {'train-error', 'validation-error'},\n",
       "  1388: {'train-error', 'validation-error'},\n",
       "  1390: {'train-error', 'validation-error'},\n",
       "  1392: {'train-error', 'validation-error'},\n",
       "  1394: {'train-error', 'validation-error'},\n",
       "  1396: {'train-error', 'validation-error'},\n",
       "  1398: {'train-error', 'validation-error'},\n",
       "  1400: {'train-error', 'validation-error'},\n",
       "  1402: {'train-error', 'validation-error'},\n",
       "  1404: {'train-error', 'validation-error'},\n",
       "  1406: {'train-error', 'validation-error'},\n",
       "  1408: {'train-error', 'validation-error'},\n",
       "  1410: {'train-error', 'validation-error'},\n",
       "  1412: {'train-error', 'validation-error'},\n",
       "  1414: {'train-error', 'validation-error'},\n",
       "  1416: {'train-error', 'validation-error'},\n",
       "  1418: {'train-error', 'validation-error'},\n",
       "  1420: {'train-error', 'validation-error'},\n",
       "  1422: {'train-error', 'validation-error'},\n",
       "  1424: {'train-error', 'validation-error'},\n",
       "  1426: {'train-error', 'validation-error'},\n",
       "  1428: {'train-error', 'validation-error'},\n",
       "  1430: {'train-error', 'validation-error'},\n",
       "  1432: {'train-error', 'validation-error'},\n",
       "  1434: {'train-error', 'validation-error'},\n",
       "  1436: {'train-error', 'validation-error'},\n",
       "  1438: {'train-error', 'validation-error'},\n",
       "  1440: {'train-error', 'validation-error'},\n",
       "  1442: {'train-error', 'validation-error'},\n",
       "  1444: {'train-error', 'validation-error'},\n",
       "  1446: {'train-error', 'validation-error'},\n",
       "  1448: {'train-error', 'validation-error'},\n",
       "  1450: {'train-error', 'validation-error'},\n",
       "  1452: {'train-error', 'validation-error'},\n",
       "  1454: {'train-error', 'validation-error'},\n",
       "  1456: {'train-error', 'validation-error'},\n",
       "  1458: {'train-error', 'validation-error'},\n",
       "  1460: {'train-error', 'validation-error'},\n",
       "  1462: {'train-error', 'validation-error'},\n",
       "  1464: {'train-error', 'validation-error'},\n",
       "  1466: {'train-error', 'validation-error'},\n",
       "  1468: {'train-error', 'validation-error'},\n",
       "  1470: {'train-error', 'validation-error'},\n",
       "  1472: {'train-error', 'validation-error'},\n",
       "  1474: {'train-error', 'validation-error'},\n",
       "  1476: {'train-error', 'validation-error'},\n",
       "  1478: {'train-error', 'validation-error'},\n",
       "  1480: {'train-error', 'validation-error'},\n",
       "  1482: {'train-error', 'validation-error'},\n",
       "  1484: {'train-error', 'validation-error'},\n",
       "  1486: {'train-error', 'validation-error'},\n",
       "  1488: {'train-error', 'validation-error'},\n",
       "  1490: {'train-error', 'validation-error'},\n",
       "  1492: {'train-error', 'validation-error'},\n",
       "  1494: {'train-error', 'validation-error'},\n",
       "  1496: {'train-error', 'validation-error'},\n",
       "  1498: {'train-error', 'validation-error'},\n",
       "  1500: {'train-error', 'validation-error'},\n",
       "  1502: {'train-error', 'validation-error'},\n",
       "  1504: {'train-error', 'validation-error'},\n",
       "  1506: {'train-error', 'validation-error'},\n",
       "  1508: {'train-error', 'validation-error'},\n",
       "  1510: {'train-error', 'validation-error'},\n",
       "  1512: {'train-error', 'validation-error'},\n",
       "  1514: {'train-error', 'validation-error'},\n",
       "  1516: {'train-error', 'validation-error'},\n",
       "  1518: {'train-error', 'validation-error'},\n",
       "  1520: {'train-error', 'validation-error'},\n",
       "  1522: {'train-error', 'validation-error'},\n",
       "  1524: {'train-error', 'validation-error'},\n",
       "  1526: {'train-error', 'validation-error'},\n",
       "  1528: {'train-error', 'validation-error'},\n",
       "  1530: {'train-error', 'validation-error'},\n",
       "  1532: {'train-error', 'validation-error'},\n",
       "  1534: {'train-error', 'validation-error'},\n",
       "  1536: {'train-error', 'validation-error'},\n",
       "  1538: {'train-error', 'validation-error'},\n",
       "  1540: {'train-error', 'validation-error'},\n",
       "  1542: {'train-error', 'validation-error'},\n",
       "  1544: {'train-error', 'validation-error'},\n",
       "  1546: {'train-error', 'validation-error'},\n",
       "  1548: {'train-error', 'validation-error'},\n",
       "  1550: {'train-error', 'validation-error'},\n",
       "  1552: {'train-error', 'validation-error'},\n",
       "  1554: {'train-error', 'validation-error'},\n",
       "  1556: {'train-error', 'validation-error'},\n",
       "  1558: {'train-error', 'validation-error'},\n",
       "  1560: {'train-error', 'validation-error'},\n",
       "  1562: {'train-error', 'validation-error'},\n",
       "  1564: {'train-error', 'validation-error'},\n",
       "  1566: {'train-error', 'validation-error'},\n",
       "  1568: {'train-error', 'validation-error'},\n",
       "  1570: {'train-error', 'validation-error'},\n",
       "  1572: {'train-error', 'validation-error'},\n",
       "  1574: {'train-error', 'validation-error'},\n",
       "  1576: {'train-error', 'validation-error'},\n",
       "  1578: {'train-error', 'validation-error'},\n",
       "  1580: {'train-error', 'validation-error'},\n",
       "  1582: {'train-error', 'validation-error'},\n",
       "  1584: {'train-error', 'validation-error'},\n",
       "  1586: {'train-error', 'validation-error'},\n",
       "  1588: {'train-error', 'validation-error'},\n",
       "  1590: {'train-error', 'validation-error'},\n",
       "  1592: {'train-error', 'validation-error'},\n",
       "  1594: {'train-error', 'validation-error'},\n",
       "  1596: {'train-error', 'validation-error'},\n",
       "  1598: {'train-error', 'validation-error'},\n",
       "  1600: {'train-error', 'validation-error'},\n",
       "  1602: {'train-error', 'validation-error'},\n",
       "  1604: {'train-error', 'validation-error'},\n",
       "  1606: {'train-error', 'validation-error'},\n",
       "  1608: {'train-error', 'validation-error'},\n",
       "  1610: {'train-error', 'validation-error'},\n",
       "  1612: {'train-error', 'validation-error'},\n",
       "  1614: {'train-error', 'validation-error'},\n",
       "  1616: {'train-error', 'validation-error'},\n",
       "  1618: {'train-error', 'validation-error'},\n",
       "  1620: {'train-error', 'validation-error'},\n",
       "  1622: {'train-error', 'validation-error'},\n",
       "  1624: {'train-error', 'validation-error'},\n",
       "  1626: {'train-error', 'validation-error'},\n",
       "  1628: {'train-error', 'validation-error'},\n",
       "  1630: {'train-error', 'validation-error'},\n",
       "  1632: {'train-error', 'validation-error'},\n",
       "  1634: {'train-error', 'validation-error'},\n",
       "  1636: {'train-error', 'validation-error'},\n",
       "  1638: {'train-error', 'validation-error'},\n",
       "  1640: {'train-error', 'validation-error'},\n",
       "  1642: {'train-error', 'validation-error'},\n",
       "  1644: {'train-error', 'validation-error'},\n",
       "  1646: {'train-error', 'validation-error'},\n",
       "  1648: {'train-error', 'validation-error'},\n",
       "  1650: {'train-error', 'validation-error'},\n",
       "  1652: {'train-error', 'validation-error'},\n",
       "  1654: {'train-error', 'validation-error'},\n",
       "  1656: {'train-error', 'validation-error'},\n",
       "  1658: {'train-error', 'validation-error'},\n",
       "  1660: {'train-error', 'validation-error'},\n",
       "  1662: {'train-error', 'validation-error'},\n",
       "  1664: {'train-error', 'validation-error'},\n",
       "  1666: {'train-error', 'validation-error'},\n",
       "  1668: {'train-error', 'validation-error'},\n",
       "  1670: {'train-error', 'validation-error'},\n",
       "  1672: {'train-error', 'validation-error'},\n",
       "  1674: {'train-error', 'validation-error'},\n",
       "  1676: {'train-error', 'validation-error'},\n",
       "  1678: {'train-error', 'validation-error'},\n",
       "  1680: {'train-error', 'validation-error'},\n",
       "  1682: {'train-error', 'validation-error'},\n",
       "  1684: {'train-error', 'validation-error'},\n",
       "  1686: {'train-error', 'validation-error'},\n",
       "  1688: {'train-error', 'validation-error'},\n",
       "  1690: {'train-error', 'validation-error'},\n",
       "  1692: {'train-error', 'validation-error'},\n",
       "  1694: {'train-error', 'validation-error'},\n",
       "  1696: {'train-error', 'validation-error'},\n",
       "  1698: {'train-error', 'validation-error'},\n",
       "  1700: {'train-error', 'validation-error'},\n",
       "  1702: {'train-error', 'validation-error'},\n",
       "  1704: {'train-error', 'validation-error'},\n",
       "  1706: {'train-error', 'validation-error'},\n",
       "  1708: {'train-error', 'validation-error'},\n",
       "  1710: {'train-error', 'validation-error'},\n",
       "  1712: {'train-error', 'validation-error'},\n",
       "  1714: {'train-error', 'validation-error'},\n",
       "  1716: {'train-error', 'validation-error'},\n",
       "  1718: {'train-error', 'validation-error'},\n",
       "  1720: {'train-error', 'validation-error'},\n",
       "  1722: {'train-error', 'validation-error'},\n",
       "  1724: {'train-error', 'validation-error'},\n",
       "  1726: {'train-error', 'validation-error'},\n",
       "  1728: {'train-error', 'validation-error'},\n",
       "  1730: {'train-error', 'validation-error'},\n",
       "  1732: {'train-error', 'validation-error'},\n",
       "  1734: {'train-error', 'validation-error'},\n",
       "  1736: {'train-error', 'validation-error'},\n",
       "  1738: {'train-error', 'validation-error'},\n",
       "  1740: {'train-error', 'validation-error'},\n",
       "  1742: {'train-error', 'validation-error'},\n",
       "  1744: {'train-error', 'validation-error'},\n",
       "  1746: {'train-error', 'validation-error'},\n",
       "  1748: {'train-error', 'validation-error'},\n",
       "  1750: {'train-error', 'validation-error'},\n",
       "  1752: {'train-error', 'validation-error'},\n",
       "  1754: {'train-error', 'validation-error'},\n",
       "  1756: {'train-error', 'validation-error'},\n",
       "  1758: {'train-error', 'validation-error'},\n",
       "  1760: {'train-error', 'validation-error'},\n",
       "  1762: {'train-error', 'validation-error'},\n",
       "  1764: {'train-error', 'validation-error'},\n",
       "  1766: {'train-error', 'validation-error'},\n",
       "  1768: {'train-error', 'validation-error'},\n",
       "  1770: {'train-error', 'validation-error'},\n",
       "  1772: {'train-error', 'validation-error'},\n",
       "  1774: {'train-error', 'validation-error'},\n",
       "  1776: {'train-error', 'validation-error'},\n",
       "  1778: {'train-error', 'validation-error'},\n",
       "  1780: {'train-error', 'validation-error'},\n",
       "  1782: {'train-error', 'validation-error'},\n",
       "  1784: {'train-error', 'validation-error'},\n",
       "  1786: {'train-error', 'validation-error'},\n",
       "  1788: {'train-error', 'validation-error'},\n",
       "  1790: {'train-error', 'validation-error'},\n",
       "  1792: {'train-error', 'validation-error'},\n",
       "  1794: {'train-error', 'validation-error'},\n",
       "  1796: {'train-error', 'validation-error'},\n",
       "  1798: {'train-error', 'validation-error'},\n",
       "  1800: {'train-error', 'validation-error'},\n",
       "  1802: {'train-error', 'validation-error'},\n",
       "  1804: {'train-error', 'validation-error'},\n",
       "  1806: {'train-error', 'validation-error'},\n",
       "  1808: {'train-error', 'validation-error'},\n",
       "  1810: {'train-error', 'validation-error'},\n",
       "  1812: {'train-error', 'validation-error'},\n",
       "  1814: {'train-error', 'validation-error'},\n",
       "  1816: {'train-error', 'validation-error'},\n",
       "  1818: {'train-error', 'validation-error'},\n",
       "  1820: {'train-error', 'validation-error'},\n",
       "  1822: {'train-error', 'validation-error'},\n",
       "  1824: {'train-error', 'validation-error'},\n",
       "  1826: {'train-error', 'validation-error'},\n",
       "  1828: {'train-error', 'validation-error'},\n",
       "  1830: {'train-error', 'validation-error'},\n",
       "  1832: {'train-error', 'validation-error'},\n",
       "  1834: {'train-error', 'validation-error'},\n",
       "  1836: {'train-error', 'validation-error'},\n",
       "  1838: {'train-error', 'validation-error'},\n",
       "  1840: {'train-error', 'validation-error'},\n",
       "  1842: {'train-error', 'validation-error'},\n",
       "  1844: {'train-error', 'validation-error'},\n",
       "  1846: {'train-error', 'validation-error'},\n",
       "  1848: {'train-error', 'validation-error'},\n",
       "  1850: {'train-error', 'validation-error'},\n",
       "  1852: {'train-error', 'validation-error'},\n",
       "  1854: {'train-error', 'validation-error'},\n",
       "  1856: {'train-error', 'validation-error'},\n",
       "  1858: {'train-error', 'validation-error'},\n",
       "  1860: {'train-error', 'validation-error'},\n",
       "  1862: {'train-error', 'validation-error'},\n",
       "  1864: {'train-error', 'validation-error'},\n",
       "  1866: {'train-error', 'validation-error'},\n",
       "  1868: {'train-error', 'validation-error'},\n",
       "  1870: {'train-error', 'validation-error'},\n",
       "  1872: {'train-error', 'validation-error'},\n",
       "  1874: {'train-error', 'validation-error'},\n",
       "  1876: {'train-error', 'validation-error'},\n",
       "  1878: {'train-error', 'validation-error'},\n",
       "  1880: {'train-error', 'validation-error'},\n",
       "  1882: {'train-error', 'validation-error'},\n",
       "  1884: {'train-error', 'validation-error'},\n",
       "  1886: {'train-error', 'validation-error'},\n",
       "  1888: {'train-error', 'validation-error'},\n",
       "  1890: {'train-error', 'validation-error'},\n",
       "  1892: {'train-error', 'validation-error'},\n",
       "  1894: {'train-error', 'validation-error'},\n",
       "  1896: {'train-error', 'validation-error'},\n",
       "  1898: {'train-error', 'validation-error'},\n",
       "  1900: {'train-error', 'validation-error'},\n",
       "  1902: {'train-error', 'validation-error'},\n",
       "  1904: {'train-error', 'validation-error'},\n",
       "  1906: {'train-error', 'validation-error'},\n",
       "  1908: {'train-error', 'validation-error'},\n",
       "  1910: {'train-error', 'validation-error'},\n",
       "  1912: {'train-error', 'validation-error'},\n",
       "  1914: {'train-error', 'validation-error'},\n",
       "  1916: {'train-error', 'validation-error'},\n",
       "  1918: {'train-error', 'validation-error'},\n",
       "  1920: {'train-error', 'validation-error'},\n",
       "  1922: {'train-error', 'validation-error'},\n",
       "  1924: {'train-error', 'validation-error'},\n",
       "  1926: {'train-error', 'validation-error'},\n",
       "  1928: {'train-error', 'validation-error'},\n",
       "  1930: {'train-error', 'validation-error'},\n",
       "  1932: {'train-error', 'validation-error'},\n",
       "  1934: {'train-error', 'validation-error'},\n",
       "  1936: {'train-error', 'validation-error'},\n",
       "  1938: {'train-error', 'validation-error'},\n",
       "  1940: {'train-error', 'validation-error'},\n",
       "  1942: {'train-error', 'validation-error'},\n",
       "  1944: {'train-error', 'validation-error'},\n",
       "  1946: {'train-error', 'validation-error'},\n",
       "  1948: {'train-error', 'validation-error'},\n",
       "  1950: {'train-error', 'validation-error'},\n",
       "  1952: {'train-error', 'validation-error'},\n",
       "  1954: {'train-error', 'validation-error'},\n",
       "  1956: {'train-error', 'validation-error'},\n",
       "  1958: {'train-error', 'validation-error'},\n",
       "  1960: {'train-error', 'validation-error'},\n",
       "  1962: {'train-error', 'validation-error'},\n",
       "  1964: {'train-error', 'validation-error'},\n",
       "  1966: {'train-error', 'validation-error'},\n",
       "  1968: {'train-error', 'validation-error'},\n",
       "  1970: {'train-error', 'validation-error'},\n",
       "  1972: {'train-error', 'validation-error'},\n",
       "  1974: {'train-error', 'validation-error'},\n",
       "  1976: {'train-error', 'validation-error'},\n",
       "  1978: {'train-error', 'validation-error'},\n",
       "  1980: {'train-error', 'validation-error'},\n",
       "  1982: {'train-error', 'validation-error'},\n",
       "  1984: {'train-error', 'validation-error'},\n",
       "  1986: {'train-error', 'validation-error'},\n",
       "  1988: {'train-error', 'validation-error'},\n",
       "  1990: {'train-error', 'validation-error'},\n",
       "  1992: {'train-error', 'validation-error'},\n",
       "  1994: {'train-error', 'validation-error'},\n",
       "  1996: {'train-error', 'validation-error'},\n",
       "  1998: {'train-error', 'validation-error'},\n",
       "  ...},\n",
       " 'mode_to_tensors_map': {},\n",
       " 'num_workers': 1,\n",
       " 'workers_for_global_step': {0: {'worker_0'},\n",
       "  2: {'worker_0'},\n",
       "  4: {'worker_0'},\n",
       "  6: {'worker_0'},\n",
       "  8: {'worker_0'},\n",
       "  10: {'worker_0'},\n",
       "  12: {'worker_0'},\n",
       "  14: {'worker_0'},\n",
       "  16: {'worker_0'},\n",
       "  18: {'worker_0'},\n",
       "  20: {'worker_0'},\n",
       "  22: {'worker_0'},\n",
       "  24: {'worker_0'},\n",
       "  26: {'worker_0'},\n",
       "  28: {'worker_0'},\n",
       "  30: {'worker_0'},\n",
       "  32: {'worker_0'},\n",
       "  34: {'worker_0'},\n",
       "  36: {'worker_0'},\n",
       "  38: {'worker_0'},\n",
       "  40: {'worker_0'},\n",
       "  42: {'worker_0'},\n",
       "  44: {'worker_0'},\n",
       "  46: {'worker_0'},\n",
       "  48: {'worker_0'},\n",
       "  50: {'worker_0'},\n",
       "  52: {'worker_0'},\n",
       "  54: {'worker_0'},\n",
       "  56: {'worker_0'},\n",
       "  58: {'worker_0'},\n",
       "  60: {'worker_0'},\n",
       "  62: {'worker_0'},\n",
       "  64: {'worker_0'},\n",
       "  66: {'worker_0'},\n",
       "  68: {'worker_0'},\n",
       "  70: {'worker_0'},\n",
       "  72: {'worker_0'},\n",
       "  74: {'worker_0'},\n",
       "  76: {'worker_0'},\n",
       "  78: {'worker_0'},\n",
       "  80: {'worker_0'},\n",
       "  82: {'worker_0'},\n",
       "  84: {'worker_0'},\n",
       "  86: {'worker_0'},\n",
       "  88: {'worker_0'},\n",
       "  90: {'worker_0'},\n",
       "  92: {'worker_0'},\n",
       "  94: {'worker_0'},\n",
       "  96: {'worker_0'},\n",
       "  98: {'worker_0'},\n",
       "  100: {'worker_0'},\n",
       "  102: {'worker_0'},\n",
       "  104: {'worker_0'},\n",
       "  106: {'worker_0'},\n",
       "  108: {'worker_0'},\n",
       "  110: {'worker_0'},\n",
       "  112: {'worker_0'},\n",
       "  114: {'worker_0'},\n",
       "  116: {'worker_0'},\n",
       "  118: {'worker_0'},\n",
       "  120: {'worker_0'},\n",
       "  122: {'worker_0'},\n",
       "  124: {'worker_0'},\n",
       "  126: {'worker_0'},\n",
       "  128: {'worker_0'},\n",
       "  130: {'worker_0'},\n",
       "  132: {'worker_0'},\n",
       "  134: {'worker_0'},\n",
       "  136: {'worker_0'},\n",
       "  138: {'worker_0'},\n",
       "  140: {'worker_0'},\n",
       "  142: {'worker_0'},\n",
       "  144: {'worker_0'},\n",
       "  146: {'worker_0'},\n",
       "  148: {'worker_0'},\n",
       "  150: {'worker_0'},\n",
       "  152: {'worker_0'},\n",
       "  154: {'worker_0'},\n",
       "  156: {'worker_0'},\n",
       "  158: {'worker_0'},\n",
       "  160: {'worker_0'},\n",
       "  162: {'worker_0'},\n",
       "  164: {'worker_0'},\n",
       "  166: {'worker_0'},\n",
       "  168: {'worker_0'},\n",
       "  170: {'worker_0'},\n",
       "  172: {'worker_0'},\n",
       "  174: {'worker_0'},\n",
       "  176: {'worker_0'},\n",
       "  178: {'worker_0'},\n",
       "  180: {'worker_0'},\n",
       "  182: {'worker_0'},\n",
       "  184: {'worker_0'},\n",
       "  186: {'worker_0'},\n",
       "  188: {'worker_0'},\n",
       "  190: {'worker_0'},\n",
       "  192: {'worker_0'},\n",
       "  194: {'worker_0'},\n",
       "  196: {'worker_0'},\n",
       "  198: {'worker_0'},\n",
       "  200: {'worker_0'},\n",
       "  202: {'worker_0'},\n",
       "  204: {'worker_0'},\n",
       "  206: {'worker_0'},\n",
       "  208: {'worker_0'},\n",
       "  210: {'worker_0'},\n",
       "  212: {'worker_0'},\n",
       "  214: {'worker_0'},\n",
       "  216: {'worker_0'},\n",
       "  218: {'worker_0'},\n",
       "  220: {'worker_0'},\n",
       "  222: {'worker_0'},\n",
       "  224: {'worker_0'},\n",
       "  226: {'worker_0'},\n",
       "  228: {'worker_0'},\n",
       "  230: {'worker_0'},\n",
       "  232: {'worker_0'},\n",
       "  234: {'worker_0'},\n",
       "  236: {'worker_0'},\n",
       "  238: {'worker_0'},\n",
       "  240: {'worker_0'},\n",
       "  242: {'worker_0'},\n",
       "  244: {'worker_0'},\n",
       "  246: {'worker_0'},\n",
       "  248: {'worker_0'},\n",
       "  250: {'worker_0'},\n",
       "  252: {'worker_0'},\n",
       "  254: {'worker_0'},\n",
       "  256: {'worker_0'},\n",
       "  258: {'worker_0'},\n",
       "  260: {'worker_0'},\n",
       "  262: {'worker_0'},\n",
       "  264: {'worker_0'},\n",
       "  266: {'worker_0'},\n",
       "  268: {'worker_0'},\n",
       "  270: {'worker_0'},\n",
       "  272: {'worker_0'},\n",
       "  274: {'worker_0'},\n",
       "  276: {'worker_0'},\n",
       "  278: {'worker_0'},\n",
       "  280: {'worker_0'},\n",
       "  282: {'worker_0'},\n",
       "  284: {'worker_0'},\n",
       "  286: {'worker_0'},\n",
       "  288: {'worker_0'},\n",
       "  290: {'worker_0'},\n",
       "  292: {'worker_0'},\n",
       "  294: {'worker_0'},\n",
       "  296: {'worker_0'},\n",
       "  298: {'worker_0'},\n",
       "  300: {'worker_0'},\n",
       "  302: {'worker_0'},\n",
       "  304: {'worker_0'},\n",
       "  306: {'worker_0'},\n",
       "  308: {'worker_0'},\n",
       "  310: {'worker_0'},\n",
       "  312: {'worker_0'},\n",
       "  314: {'worker_0'},\n",
       "  316: {'worker_0'},\n",
       "  318: {'worker_0'},\n",
       "  320: {'worker_0'},\n",
       "  322: {'worker_0'},\n",
       "  324: {'worker_0'},\n",
       "  326: {'worker_0'},\n",
       "  328: {'worker_0'},\n",
       "  330: {'worker_0'},\n",
       "  332: {'worker_0'},\n",
       "  334: {'worker_0'},\n",
       "  336: {'worker_0'},\n",
       "  338: {'worker_0'},\n",
       "  340: {'worker_0'},\n",
       "  342: {'worker_0'},\n",
       "  344: {'worker_0'},\n",
       "  346: {'worker_0'},\n",
       "  348: {'worker_0'},\n",
       "  350: {'worker_0'},\n",
       "  352: {'worker_0'},\n",
       "  354: {'worker_0'},\n",
       "  356: {'worker_0'},\n",
       "  358: {'worker_0'},\n",
       "  360: {'worker_0'},\n",
       "  362: {'worker_0'},\n",
       "  364: {'worker_0'},\n",
       "  366: {'worker_0'},\n",
       "  368: {'worker_0'},\n",
       "  370: {'worker_0'},\n",
       "  372: {'worker_0'},\n",
       "  374: {'worker_0'},\n",
       "  376: {'worker_0'},\n",
       "  378: {'worker_0'},\n",
       "  380: {'worker_0'},\n",
       "  382: {'worker_0'},\n",
       "  384: {'worker_0'},\n",
       "  386: {'worker_0'},\n",
       "  388: {'worker_0'},\n",
       "  390: {'worker_0'},\n",
       "  392: {'worker_0'},\n",
       "  394: {'worker_0'},\n",
       "  396: {'worker_0'},\n",
       "  398: {'worker_0'},\n",
       "  400: {'worker_0'},\n",
       "  402: {'worker_0'},\n",
       "  404: {'worker_0'},\n",
       "  406: {'worker_0'},\n",
       "  408: {'worker_0'},\n",
       "  410: {'worker_0'},\n",
       "  412: {'worker_0'},\n",
       "  414: {'worker_0'},\n",
       "  416: {'worker_0'},\n",
       "  418: {'worker_0'},\n",
       "  420: {'worker_0'},\n",
       "  422: {'worker_0'},\n",
       "  424: {'worker_0'},\n",
       "  426: {'worker_0'},\n",
       "  428: {'worker_0'},\n",
       "  430: {'worker_0'},\n",
       "  432: {'worker_0'},\n",
       "  434: {'worker_0'},\n",
       "  436: {'worker_0'},\n",
       "  438: {'worker_0'},\n",
       "  440: {'worker_0'},\n",
       "  442: {'worker_0'},\n",
       "  444: {'worker_0'},\n",
       "  446: {'worker_0'},\n",
       "  448: {'worker_0'},\n",
       "  450: {'worker_0'},\n",
       "  452: {'worker_0'},\n",
       "  454: {'worker_0'},\n",
       "  456: {'worker_0'},\n",
       "  458: {'worker_0'},\n",
       "  460: {'worker_0'},\n",
       "  462: {'worker_0'},\n",
       "  464: {'worker_0'},\n",
       "  466: {'worker_0'},\n",
       "  468: {'worker_0'},\n",
       "  470: {'worker_0'},\n",
       "  472: {'worker_0'},\n",
       "  474: {'worker_0'},\n",
       "  476: {'worker_0'},\n",
       "  478: {'worker_0'},\n",
       "  480: {'worker_0'},\n",
       "  482: {'worker_0'},\n",
       "  484: {'worker_0'},\n",
       "  486: {'worker_0'},\n",
       "  488: {'worker_0'},\n",
       "  490: {'worker_0'},\n",
       "  492: {'worker_0'},\n",
       "  494: {'worker_0'},\n",
       "  496: {'worker_0'},\n",
       "  498: {'worker_0'},\n",
       "  500: {'worker_0'},\n",
       "  502: {'worker_0'},\n",
       "  504: {'worker_0'},\n",
       "  506: {'worker_0'},\n",
       "  508: {'worker_0'},\n",
       "  510: {'worker_0'},\n",
       "  512: {'worker_0'},\n",
       "  514: {'worker_0'},\n",
       "  516: {'worker_0'},\n",
       "  518: {'worker_0'},\n",
       "  520: {'worker_0'},\n",
       "  522: {'worker_0'},\n",
       "  524: {'worker_0'},\n",
       "  526: {'worker_0'},\n",
       "  528: {'worker_0'},\n",
       "  530: {'worker_0'},\n",
       "  532: {'worker_0'},\n",
       "  534: {'worker_0'},\n",
       "  536: {'worker_0'},\n",
       "  538: {'worker_0'},\n",
       "  540: {'worker_0'},\n",
       "  542: {'worker_0'},\n",
       "  544: {'worker_0'},\n",
       "  546: {'worker_0'},\n",
       "  548: {'worker_0'},\n",
       "  550: {'worker_0'},\n",
       "  552: {'worker_0'},\n",
       "  554: {'worker_0'},\n",
       "  556: {'worker_0'},\n",
       "  558: {'worker_0'},\n",
       "  560: {'worker_0'},\n",
       "  562: {'worker_0'},\n",
       "  564: {'worker_0'},\n",
       "  566: {'worker_0'},\n",
       "  568: {'worker_0'},\n",
       "  570: {'worker_0'},\n",
       "  572: {'worker_0'},\n",
       "  574: {'worker_0'},\n",
       "  576: {'worker_0'},\n",
       "  578: {'worker_0'},\n",
       "  580: {'worker_0'},\n",
       "  582: {'worker_0'},\n",
       "  584: {'worker_0'},\n",
       "  586: {'worker_0'},\n",
       "  588: {'worker_0'},\n",
       "  590: {'worker_0'},\n",
       "  592: {'worker_0'},\n",
       "  594: {'worker_0'},\n",
       "  596: {'worker_0'},\n",
       "  598: {'worker_0'},\n",
       "  600: {'worker_0'},\n",
       "  602: {'worker_0'},\n",
       "  604: {'worker_0'},\n",
       "  606: {'worker_0'},\n",
       "  608: {'worker_0'},\n",
       "  610: {'worker_0'},\n",
       "  612: {'worker_0'},\n",
       "  614: {'worker_0'},\n",
       "  616: {'worker_0'},\n",
       "  618: {'worker_0'},\n",
       "  620: {'worker_0'},\n",
       "  622: {'worker_0'},\n",
       "  624: {'worker_0'},\n",
       "  626: {'worker_0'},\n",
       "  628: {'worker_0'},\n",
       "  630: {'worker_0'},\n",
       "  632: {'worker_0'},\n",
       "  634: {'worker_0'},\n",
       "  636: {'worker_0'},\n",
       "  638: {'worker_0'},\n",
       "  640: {'worker_0'},\n",
       "  642: {'worker_0'},\n",
       "  644: {'worker_0'},\n",
       "  646: {'worker_0'},\n",
       "  648: {'worker_0'},\n",
       "  650: {'worker_0'},\n",
       "  652: {'worker_0'},\n",
       "  654: {'worker_0'},\n",
       "  656: {'worker_0'},\n",
       "  658: {'worker_0'},\n",
       "  660: {'worker_0'},\n",
       "  662: {'worker_0'},\n",
       "  664: {'worker_0'},\n",
       "  666: {'worker_0'},\n",
       "  668: {'worker_0'},\n",
       "  670: {'worker_0'},\n",
       "  672: {'worker_0'},\n",
       "  674: {'worker_0'},\n",
       "  676: {'worker_0'},\n",
       "  678: {'worker_0'},\n",
       "  680: {'worker_0'},\n",
       "  682: {'worker_0'},\n",
       "  684: {'worker_0'},\n",
       "  686: {'worker_0'},\n",
       "  688: {'worker_0'},\n",
       "  690: {'worker_0'},\n",
       "  692: {'worker_0'},\n",
       "  694: {'worker_0'},\n",
       "  696: {'worker_0'},\n",
       "  698: {'worker_0'},\n",
       "  700: {'worker_0'},\n",
       "  702: {'worker_0'},\n",
       "  704: {'worker_0'},\n",
       "  706: {'worker_0'},\n",
       "  708: {'worker_0'},\n",
       "  710: {'worker_0'},\n",
       "  712: {'worker_0'},\n",
       "  714: {'worker_0'},\n",
       "  716: {'worker_0'},\n",
       "  718: {'worker_0'},\n",
       "  720: {'worker_0'},\n",
       "  722: {'worker_0'},\n",
       "  724: {'worker_0'},\n",
       "  726: {'worker_0'},\n",
       "  728: {'worker_0'},\n",
       "  730: {'worker_0'},\n",
       "  732: {'worker_0'},\n",
       "  734: {'worker_0'},\n",
       "  736: {'worker_0'},\n",
       "  738: {'worker_0'},\n",
       "  740: {'worker_0'},\n",
       "  742: {'worker_0'},\n",
       "  744: {'worker_0'},\n",
       "  746: {'worker_0'},\n",
       "  748: {'worker_0'},\n",
       "  750: {'worker_0'},\n",
       "  752: {'worker_0'},\n",
       "  754: {'worker_0'},\n",
       "  756: {'worker_0'},\n",
       "  758: {'worker_0'},\n",
       "  760: {'worker_0'},\n",
       "  762: {'worker_0'},\n",
       "  764: {'worker_0'},\n",
       "  766: {'worker_0'},\n",
       "  768: {'worker_0'},\n",
       "  770: {'worker_0'},\n",
       "  772: {'worker_0'},\n",
       "  774: {'worker_0'},\n",
       "  776: {'worker_0'},\n",
       "  778: {'worker_0'},\n",
       "  780: {'worker_0'},\n",
       "  782: {'worker_0'},\n",
       "  784: {'worker_0'},\n",
       "  786: {'worker_0'},\n",
       "  788: {'worker_0'},\n",
       "  790: {'worker_0'},\n",
       "  792: {'worker_0'},\n",
       "  794: {'worker_0'},\n",
       "  796: {'worker_0'},\n",
       "  798: {'worker_0'},\n",
       "  800: {'worker_0'},\n",
       "  802: {'worker_0'},\n",
       "  804: {'worker_0'},\n",
       "  806: {'worker_0'},\n",
       "  808: {'worker_0'},\n",
       "  810: {'worker_0'},\n",
       "  812: {'worker_0'},\n",
       "  814: {'worker_0'},\n",
       "  816: {'worker_0'},\n",
       "  818: {'worker_0'},\n",
       "  820: {'worker_0'},\n",
       "  822: {'worker_0'},\n",
       "  824: {'worker_0'},\n",
       "  826: {'worker_0'},\n",
       "  828: {'worker_0'},\n",
       "  830: {'worker_0'},\n",
       "  832: {'worker_0'},\n",
       "  834: {'worker_0'},\n",
       "  836: {'worker_0'},\n",
       "  838: {'worker_0'},\n",
       "  840: {'worker_0'},\n",
       "  842: {'worker_0'},\n",
       "  844: {'worker_0'},\n",
       "  846: {'worker_0'},\n",
       "  848: {'worker_0'},\n",
       "  850: {'worker_0'},\n",
       "  852: {'worker_0'},\n",
       "  854: {'worker_0'},\n",
       "  856: {'worker_0'},\n",
       "  858: {'worker_0'},\n",
       "  860: {'worker_0'},\n",
       "  862: {'worker_0'},\n",
       "  864: {'worker_0'},\n",
       "  866: {'worker_0'},\n",
       "  868: {'worker_0'},\n",
       "  870: {'worker_0'},\n",
       "  872: {'worker_0'},\n",
       "  874: {'worker_0'},\n",
       "  876: {'worker_0'},\n",
       "  878: {'worker_0'},\n",
       "  880: {'worker_0'},\n",
       "  882: {'worker_0'},\n",
       "  884: {'worker_0'},\n",
       "  886: {'worker_0'},\n",
       "  888: {'worker_0'},\n",
       "  890: {'worker_0'},\n",
       "  892: {'worker_0'},\n",
       "  894: {'worker_0'},\n",
       "  896: {'worker_0'},\n",
       "  898: {'worker_0'},\n",
       "  900: {'worker_0'},\n",
       "  902: {'worker_0'},\n",
       "  904: {'worker_0'},\n",
       "  906: {'worker_0'},\n",
       "  908: {'worker_0'},\n",
       "  910: {'worker_0'},\n",
       "  912: {'worker_0'},\n",
       "  914: {'worker_0'},\n",
       "  916: {'worker_0'},\n",
       "  918: {'worker_0'},\n",
       "  920: {'worker_0'},\n",
       "  922: {'worker_0'},\n",
       "  924: {'worker_0'},\n",
       "  926: {'worker_0'},\n",
       "  928: {'worker_0'},\n",
       "  930: {'worker_0'},\n",
       "  932: {'worker_0'},\n",
       "  934: {'worker_0'},\n",
       "  936: {'worker_0'},\n",
       "  938: {'worker_0'},\n",
       "  940: {'worker_0'},\n",
       "  942: {'worker_0'},\n",
       "  944: {'worker_0'},\n",
       "  946: {'worker_0'},\n",
       "  948: {'worker_0'},\n",
       "  950: {'worker_0'},\n",
       "  952: {'worker_0'},\n",
       "  954: {'worker_0'},\n",
       "  956: {'worker_0'},\n",
       "  958: {'worker_0'},\n",
       "  960: {'worker_0'},\n",
       "  962: {'worker_0'},\n",
       "  964: {'worker_0'},\n",
       "  966: {'worker_0'},\n",
       "  968: {'worker_0'},\n",
       "  970: {'worker_0'},\n",
       "  972: {'worker_0'},\n",
       "  974: {'worker_0'},\n",
       "  976: {'worker_0'},\n",
       "  978: {'worker_0'},\n",
       "  980: {'worker_0'},\n",
       "  982: {'worker_0'},\n",
       "  984: {'worker_0'},\n",
       "  986: {'worker_0'},\n",
       "  988: {'worker_0'},\n",
       "  990: {'worker_0'},\n",
       "  992: {'worker_0'},\n",
       "  994: {'worker_0'},\n",
       "  996: {'worker_0'},\n",
       "  998: {'worker_0'},\n",
       "  1000: {'worker_0'},\n",
       "  1002: {'worker_0'},\n",
       "  1004: {'worker_0'},\n",
       "  1006: {'worker_0'},\n",
       "  1008: {'worker_0'},\n",
       "  1010: {'worker_0'},\n",
       "  1012: {'worker_0'},\n",
       "  1014: {'worker_0'},\n",
       "  1016: {'worker_0'},\n",
       "  1018: {'worker_0'},\n",
       "  1020: {'worker_0'},\n",
       "  1022: {'worker_0'},\n",
       "  1024: {'worker_0'},\n",
       "  1026: {'worker_0'},\n",
       "  1028: {'worker_0'},\n",
       "  1030: {'worker_0'},\n",
       "  1032: {'worker_0'},\n",
       "  1034: {'worker_0'},\n",
       "  1036: {'worker_0'},\n",
       "  1038: {'worker_0'},\n",
       "  1040: {'worker_0'},\n",
       "  1042: {'worker_0'},\n",
       "  1044: {'worker_0'},\n",
       "  1046: {'worker_0'},\n",
       "  1048: {'worker_0'},\n",
       "  1050: {'worker_0'},\n",
       "  1052: {'worker_0'},\n",
       "  1054: {'worker_0'},\n",
       "  1056: {'worker_0'},\n",
       "  1058: {'worker_0'},\n",
       "  1060: {'worker_0'},\n",
       "  1062: {'worker_0'},\n",
       "  1064: {'worker_0'},\n",
       "  1066: {'worker_0'},\n",
       "  1068: {'worker_0'},\n",
       "  1070: {'worker_0'},\n",
       "  1072: {'worker_0'},\n",
       "  1074: {'worker_0'},\n",
       "  1076: {'worker_0'},\n",
       "  1078: {'worker_0'},\n",
       "  1080: {'worker_0'},\n",
       "  1082: {'worker_0'},\n",
       "  1084: {'worker_0'},\n",
       "  1086: {'worker_0'},\n",
       "  1088: {'worker_0'},\n",
       "  1090: {'worker_0'},\n",
       "  1092: {'worker_0'},\n",
       "  1094: {'worker_0'},\n",
       "  1096: {'worker_0'},\n",
       "  1098: {'worker_0'},\n",
       "  1100: {'worker_0'},\n",
       "  1102: {'worker_0'},\n",
       "  1104: {'worker_0'},\n",
       "  1106: {'worker_0'},\n",
       "  1108: {'worker_0'},\n",
       "  1110: {'worker_0'},\n",
       "  1112: {'worker_0'},\n",
       "  1114: {'worker_0'},\n",
       "  1116: {'worker_0'},\n",
       "  1118: {'worker_0'},\n",
       "  1120: {'worker_0'},\n",
       "  1122: {'worker_0'},\n",
       "  1124: {'worker_0'},\n",
       "  1126: {'worker_0'},\n",
       "  1128: {'worker_0'},\n",
       "  1130: {'worker_0'},\n",
       "  1132: {'worker_0'},\n",
       "  1134: {'worker_0'},\n",
       "  1136: {'worker_0'},\n",
       "  1138: {'worker_0'},\n",
       "  1140: {'worker_0'},\n",
       "  1142: {'worker_0'},\n",
       "  1144: {'worker_0'},\n",
       "  1146: {'worker_0'},\n",
       "  1148: {'worker_0'},\n",
       "  1150: {'worker_0'},\n",
       "  1152: {'worker_0'},\n",
       "  1154: {'worker_0'},\n",
       "  1156: {'worker_0'},\n",
       "  1158: {'worker_0'},\n",
       "  1160: {'worker_0'},\n",
       "  1162: {'worker_0'},\n",
       "  1164: {'worker_0'},\n",
       "  1166: {'worker_0'},\n",
       "  1168: {'worker_0'},\n",
       "  1170: {'worker_0'},\n",
       "  1172: {'worker_0'},\n",
       "  1174: {'worker_0'},\n",
       "  1176: {'worker_0'},\n",
       "  1178: {'worker_0'},\n",
       "  1180: {'worker_0'},\n",
       "  1182: {'worker_0'},\n",
       "  1184: {'worker_0'},\n",
       "  1186: {'worker_0'},\n",
       "  1188: {'worker_0'},\n",
       "  1190: {'worker_0'},\n",
       "  1192: {'worker_0'},\n",
       "  1194: {'worker_0'},\n",
       "  1196: {'worker_0'},\n",
       "  1198: {'worker_0'},\n",
       "  1200: {'worker_0'},\n",
       "  1202: {'worker_0'},\n",
       "  1204: {'worker_0'},\n",
       "  1206: {'worker_0'},\n",
       "  1208: {'worker_0'},\n",
       "  1210: {'worker_0'},\n",
       "  1212: {'worker_0'},\n",
       "  1214: {'worker_0'},\n",
       "  1216: {'worker_0'},\n",
       "  1218: {'worker_0'},\n",
       "  1220: {'worker_0'},\n",
       "  1222: {'worker_0'},\n",
       "  1224: {'worker_0'},\n",
       "  1226: {'worker_0'},\n",
       "  1228: {'worker_0'},\n",
       "  1230: {'worker_0'},\n",
       "  1232: {'worker_0'},\n",
       "  1234: {'worker_0'},\n",
       "  1236: {'worker_0'},\n",
       "  1238: {'worker_0'},\n",
       "  1240: {'worker_0'},\n",
       "  1242: {'worker_0'},\n",
       "  1244: {'worker_0'},\n",
       "  1246: {'worker_0'},\n",
       "  1248: {'worker_0'},\n",
       "  1250: {'worker_0'},\n",
       "  1252: {'worker_0'},\n",
       "  1254: {'worker_0'},\n",
       "  1256: {'worker_0'},\n",
       "  1258: {'worker_0'},\n",
       "  1260: {'worker_0'},\n",
       "  1262: {'worker_0'},\n",
       "  1264: {'worker_0'},\n",
       "  1266: {'worker_0'},\n",
       "  1268: {'worker_0'},\n",
       "  1270: {'worker_0'},\n",
       "  1272: {'worker_0'},\n",
       "  1274: {'worker_0'},\n",
       "  1276: {'worker_0'},\n",
       "  1278: {'worker_0'},\n",
       "  1280: {'worker_0'},\n",
       "  1282: {'worker_0'},\n",
       "  1284: {'worker_0'},\n",
       "  1286: {'worker_0'},\n",
       "  1288: {'worker_0'},\n",
       "  1290: {'worker_0'},\n",
       "  1292: {'worker_0'},\n",
       "  1294: {'worker_0'},\n",
       "  1296: {'worker_0'},\n",
       "  1298: {'worker_0'},\n",
       "  1300: {'worker_0'},\n",
       "  1302: {'worker_0'},\n",
       "  1304: {'worker_0'},\n",
       "  1306: {'worker_0'},\n",
       "  1308: {'worker_0'},\n",
       "  1310: {'worker_0'},\n",
       "  1312: {'worker_0'},\n",
       "  1314: {'worker_0'},\n",
       "  1316: {'worker_0'},\n",
       "  1318: {'worker_0'},\n",
       "  1320: {'worker_0'},\n",
       "  1322: {'worker_0'},\n",
       "  1324: {'worker_0'},\n",
       "  1326: {'worker_0'},\n",
       "  1328: {'worker_0'},\n",
       "  1330: {'worker_0'},\n",
       "  1332: {'worker_0'},\n",
       "  1334: {'worker_0'},\n",
       "  1336: {'worker_0'},\n",
       "  1338: {'worker_0'},\n",
       "  1340: {'worker_0'},\n",
       "  1342: {'worker_0'},\n",
       "  1344: {'worker_0'},\n",
       "  1346: {'worker_0'},\n",
       "  1348: {'worker_0'},\n",
       "  1350: {'worker_0'},\n",
       "  1352: {'worker_0'},\n",
       "  1354: {'worker_0'},\n",
       "  1356: {'worker_0'},\n",
       "  1358: {'worker_0'},\n",
       "  1360: {'worker_0'},\n",
       "  1362: {'worker_0'},\n",
       "  1364: {'worker_0'},\n",
       "  1366: {'worker_0'},\n",
       "  1368: {'worker_0'},\n",
       "  1370: {'worker_0'},\n",
       "  1372: {'worker_0'},\n",
       "  1374: {'worker_0'},\n",
       "  1376: {'worker_0'},\n",
       "  1378: {'worker_0'},\n",
       "  1380: {'worker_0'},\n",
       "  1382: {'worker_0'},\n",
       "  1384: {'worker_0'},\n",
       "  1386: {'worker_0'},\n",
       "  1388: {'worker_0'},\n",
       "  1390: {'worker_0'},\n",
       "  1392: {'worker_0'},\n",
       "  1394: {'worker_0'},\n",
       "  1396: {'worker_0'},\n",
       "  1398: {'worker_0'},\n",
       "  1400: {'worker_0'},\n",
       "  1402: {'worker_0'},\n",
       "  1404: {'worker_0'},\n",
       "  1406: {'worker_0'},\n",
       "  1408: {'worker_0'},\n",
       "  1410: {'worker_0'},\n",
       "  1412: {'worker_0'},\n",
       "  1414: {'worker_0'},\n",
       "  1416: {'worker_0'},\n",
       "  1418: {'worker_0'},\n",
       "  1420: {'worker_0'},\n",
       "  1422: {'worker_0'},\n",
       "  1424: {'worker_0'},\n",
       "  1426: {'worker_0'},\n",
       "  1428: {'worker_0'},\n",
       "  1430: {'worker_0'},\n",
       "  1432: {'worker_0'},\n",
       "  1434: {'worker_0'},\n",
       "  1436: {'worker_0'},\n",
       "  1438: {'worker_0'},\n",
       "  1440: {'worker_0'},\n",
       "  1442: {'worker_0'},\n",
       "  1444: {'worker_0'},\n",
       "  1446: {'worker_0'},\n",
       "  1448: {'worker_0'},\n",
       "  1450: {'worker_0'},\n",
       "  1452: {'worker_0'},\n",
       "  1454: {'worker_0'},\n",
       "  1456: {'worker_0'},\n",
       "  1458: {'worker_0'},\n",
       "  1460: {'worker_0'},\n",
       "  1462: {'worker_0'},\n",
       "  1464: {'worker_0'},\n",
       "  1466: {'worker_0'},\n",
       "  1468: {'worker_0'},\n",
       "  1470: {'worker_0'},\n",
       "  1472: {'worker_0'},\n",
       "  1474: {'worker_0'},\n",
       "  1476: {'worker_0'},\n",
       "  1478: {'worker_0'},\n",
       "  1480: {'worker_0'},\n",
       "  1482: {'worker_0'},\n",
       "  1484: {'worker_0'},\n",
       "  1486: {'worker_0'},\n",
       "  1488: {'worker_0'},\n",
       "  1490: {'worker_0'},\n",
       "  1492: {'worker_0'},\n",
       "  1494: {'worker_0'},\n",
       "  1496: {'worker_0'},\n",
       "  1498: {'worker_0'},\n",
       "  1500: {'worker_0'},\n",
       "  1502: {'worker_0'},\n",
       "  1504: {'worker_0'},\n",
       "  1506: {'worker_0'},\n",
       "  1508: {'worker_0'},\n",
       "  1510: {'worker_0'},\n",
       "  1512: {'worker_0'},\n",
       "  1514: {'worker_0'},\n",
       "  1516: {'worker_0'},\n",
       "  1518: {'worker_0'},\n",
       "  1520: {'worker_0'},\n",
       "  1522: {'worker_0'},\n",
       "  1524: {'worker_0'},\n",
       "  1526: {'worker_0'},\n",
       "  1528: {'worker_0'},\n",
       "  1530: {'worker_0'},\n",
       "  1532: {'worker_0'},\n",
       "  1534: {'worker_0'},\n",
       "  1536: {'worker_0'},\n",
       "  1538: {'worker_0'},\n",
       "  1540: {'worker_0'},\n",
       "  1542: {'worker_0'},\n",
       "  1544: {'worker_0'},\n",
       "  1546: {'worker_0'},\n",
       "  1548: {'worker_0'},\n",
       "  1550: {'worker_0'},\n",
       "  1552: {'worker_0'},\n",
       "  1554: {'worker_0'},\n",
       "  1556: {'worker_0'},\n",
       "  1558: {'worker_0'},\n",
       "  1560: {'worker_0'},\n",
       "  1562: {'worker_0'},\n",
       "  1564: {'worker_0'},\n",
       "  1566: {'worker_0'},\n",
       "  1568: {'worker_0'},\n",
       "  1570: {'worker_0'},\n",
       "  1572: {'worker_0'},\n",
       "  1574: {'worker_0'},\n",
       "  1576: {'worker_0'},\n",
       "  1578: {'worker_0'},\n",
       "  1580: {'worker_0'},\n",
       "  1582: {'worker_0'},\n",
       "  1584: {'worker_0'},\n",
       "  1586: {'worker_0'},\n",
       "  1588: {'worker_0'},\n",
       "  1590: {'worker_0'},\n",
       "  1592: {'worker_0'},\n",
       "  1594: {'worker_0'},\n",
       "  1596: {'worker_0'},\n",
       "  1598: {'worker_0'},\n",
       "  1600: {'worker_0'},\n",
       "  1602: {'worker_0'},\n",
       "  1604: {'worker_0'},\n",
       "  1606: {'worker_0'},\n",
       "  1608: {'worker_0'},\n",
       "  1610: {'worker_0'},\n",
       "  1612: {'worker_0'},\n",
       "  1614: {'worker_0'},\n",
       "  1616: {'worker_0'},\n",
       "  1618: {'worker_0'},\n",
       "  1620: {'worker_0'},\n",
       "  1622: {'worker_0'},\n",
       "  1624: {'worker_0'},\n",
       "  1626: {'worker_0'},\n",
       "  1628: {'worker_0'},\n",
       "  1630: {'worker_0'},\n",
       "  1632: {'worker_0'},\n",
       "  1634: {'worker_0'},\n",
       "  1636: {'worker_0'},\n",
       "  1638: {'worker_0'},\n",
       "  1640: {'worker_0'},\n",
       "  1642: {'worker_0'},\n",
       "  1644: {'worker_0'},\n",
       "  1646: {'worker_0'},\n",
       "  1648: {'worker_0'},\n",
       "  1650: {'worker_0'},\n",
       "  1652: {'worker_0'},\n",
       "  1654: {'worker_0'},\n",
       "  1656: {'worker_0'},\n",
       "  1658: {'worker_0'},\n",
       "  1660: {'worker_0'},\n",
       "  1662: {'worker_0'},\n",
       "  1664: {'worker_0'},\n",
       "  1666: {'worker_0'},\n",
       "  1668: {'worker_0'},\n",
       "  1670: {'worker_0'},\n",
       "  1672: {'worker_0'},\n",
       "  1674: {'worker_0'},\n",
       "  1676: {'worker_0'},\n",
       "  1678: {'worker_0'},\n",
       "  1680: {'worker_0'},\n",
       "  1682: {'worker_0'},\n",
       "  1684: {'worker_0'},\n",
       "  1686: {'worker_0'},\n",
       "  1688: {'worker_0'},\n",
       "  1690: {'worker_0'},\n",
       "  1692: {'worker_0'},\n",
       "  1694: {'worker_0'},\n",
       "  1696: {'worker_0'},\n",
       "  1698: {'worker_0'},\n",
       "  1700: {'worker_0'},\n",
       "  1702: {'worker_0'},\n",
       "  1704: {'worker_0'},\n",
       "  1706: {'worker_0'},\n",
       "  1708: {'worker_0'},\n",
       "  1710: {'worker_0'},\n",
       "  1712: {'worker_0'},\n",
       "  1714: {'worker_0'},\n",
       "  1716: {'worker_0'},\n",
       "  1718: {'worker_0'},\n",
       "  1720: {'worker_0'},\n",
       "  1722: {'worker_0'},\n",
       "  1724: {'worker_0'},\n",
       "  1726: {'worker_0'},\n",
       "  1728: {'worker_0'},\n",
       "  1730: {'worker_0'},\n",
       "  1732: {'worker_0'},\n",
       "  1734: {'worker_0'},\n",
       "  1736: {'worker_0'},\n",
       "  1738: {'worker_0'},\n",
       "  1740: {'worker_0'},\n",
       "  1742: {'worker_0'},\n",
       "  1744: {'worker_0'},\n",
       "  1746: {'worker_0'},\n",
       "  1748: {'worker_0'},\n",
       "  1750: {'worker_0'},\n",
       "  1752: {'worker_0'},\n",
       "  1754: {'worker_0'},\n",
       "  1756: {'worker_0'},\n",
       "  1758: {'worker_0'},\n",
       "  1760: {'worker_0'},\n",
       "  1762: {'worker_0'},\n",
       "  1764: {'worker_0'},\n",
       "  1766: {'worker_0'},\n",
       "  1768: {'worker_0'},\n",
       "  1770: {'worker_0'},\n",
       "  1772: {'worker_0'},\n",
       "  1774: {'worker_0'},\n",
       "  1776: {'worker_0'},\n",
       "  1778: {'worker_0'},\n",
       "  1780: {'worker_0'},\n",
       "  1782: {'worker_0'},\n",
       "  1784: {'worker_0'},\n",
       "  1786: {'worker_0'},\n",
       "  1788: {'worker_0'},\n",
       "  1790: {'worker_0'},\n",
       "  1792: {'worker_0'},\n",
       "  1794: {'worker_0'},\n",
       "  1796: {'worker_0'},\n",
       "  1798: {'worker_0'},\n",
       "  1800: {'worker_0'},\n",
       "  1802: {'worker_0'},\n",
       "  1804: {'worker_0'},\n",
       "  1806: {'worker_0'},\n",
       "  1808: {'worker_0'},\n",
       "  1810: {'worker_0'},\n",
       "  1812: {'worker_0'},\n",
       "  1814: {'worker_0'},\n",
       "  1816: {'worker_0'},\n",
       "  1818: {'worker_0'},\n",
       "  1820: {'worker_0'},\n",
       "  1822: {'worker_0'},\n",
       "  1824: {'worker_0'},\n",
       "  1826: {'worker_0'},\n",
       "  1828: {'worker_0'},\n",
       "  1830: {'worker_0'},\n",
       "  1832: {'worker_0'},\n",
       "  1834: {'worker_0'},\n",
       "  1836: {'worker_0'},\n",
       "  1838: {'worker_0'},\n",
       "  1840: {'worker_0'},\n",
       "  1842: {'worker_0'},\n",
       "  1844: {'worker_0'},\n",
       "  1846: {'worker_0'},\n",
       "  1848: {'worker_0'},\n",
       "  1850: {'worker_0'},\n",
       "  1852: {'worker_0'},\n",
       "  1854: {'worker_0'},\n",
       "  1856: {'worker_0'},\n",
       "  1858: {'worker_0'},\n",
       "  1860: {'worker_0'},\n",
       "  1862: {'worker_0'},\n",
       "  1864: {'worker_0'},\n",
       "  1866: {'worker_0'},\n",
       "  1868: {'worker_0'},\n",
       "  1870: {'worker_0'},\n",
       "  1872: {'worker_0'},\n",
       "  1874: {'worker_0'},\n",
       "  1876: {'worker_0'},\n",
       "  1878: {'worker_0'},\n",
       "  1880: {'worker_0'},\n",
       "  1882: {'worker_0'},\n",
       "  1884: {'worker_0'},\n",
       "  1886: {'worker_0'},\n",
       "  1888: {'worker_0'},\n",
       "  1890: {'worker_0'},\n",
       "  1892: {'worker_0'},\n",
       "  1894: {'worker_0'},\n",
       "  1896: {'worker_0'},\n",
       "  1898: {'worker_0'},\n",
       "  1900: {'worker_0'},\n",
       "  1902: {'worker_0'},\n",
       "  1904: {'worker_0'},\n",
       "  1906: {'worker_0'},\n",
       "  1908: {'worker_0'},\n",
       "  1910: {'worker_0'},\n",
       "  1912: {'worker_0'},\n",
       "  1914: {'worker_0'},\n",
       "  1916: {'worker_0'},\n",
       "  1918: {'worker_0'},\n",
       "  1920: {'worker_0'},\n",
       "  1922: {'worker_0'},\n",
       "  1924: {'worker_0'},\n",
       "  1926: {'worker_0'},\n",
       "  1928: {'worker_0'},\n",
       "  1930: {'worker_0'},\n",
       "  1932: {'worker_0'},\n",
       "  1934: {'worker_0'},\n",
       "  1936: {'worker_0'},\n",
       "  1938: {'worker_0'},\n",
       "  1940: {'worker_0'},\n",
       "  1942: {'worker_0'},\n",
       "  1944: {'worker_0'},\n",
       "  1946: {'worker_0'},\n",
       "  1948: {'worker_0'},\n",
       "  1950: {'worker_0'},\n",
       "  1952: {'worker_0'},\n",
       "  1954: {'worker_0'},\n",
       "  1956: {'worker_0'},\n",
       "  1958: {'worker_0'},\n",
       "  1960: {'worker_0'},\n",
       "  1962: {'worker_0'},\n",
       "  1964: {'worker_0'},\n",
       "  1966: {'worker_0'},\n",
       "  1968: {'worker_0'},\n",
       "  1970: {'worker_0'},\n",
       "  1972: {'worker_0'},\n",
       "  1974: {'worker_0'},\n",
       "  1976: {'worker_0'},\n",
       "  1978: {'worker_0'},\n",
       "  1980: {'worker_0'},\n",
       "  1982: {'worker_0'},\n",
       "  1984: {'worker_0'},\n",
       "  1986: {'worker_0'},\n",
       "  1988: {'worker_0'},\n",
       "  1990: {'worker_0'},\n",
       "  1992: {'worker_0'},\n",
       "  1994: {'worker_0'},\n",
       "  1996: {'worker_0'},\n",
       "  1998: {'worker_0'},\n",
       "  ...},\n",
       " 'last_complete_step': 6998,\n",
       " '_incomplete_wait_for_step_window': 1000,\n",
       " 'dynamic_refresh': True,\n",
       " '_training_end_delay_refresh': 1,\n",
       " 'bucket_name': 'sagemaker-cookbook-bucket',\n",
       " 'prefix_name': 'debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14543ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', '_tensors', '_mode_to_global', '_global_to_mode', 'logger', 'parallel', 'check', 'range_steps', 'collection_manager', 'loaded_all_steps', 'cache', 'path', 'index_reader', 'index_tensors_dict', 'index_mode', 'last_event_token', 'last_index_token', 'worker_set', 'global_step_to_tensors_map', 'mode_to_tensors_map', 'num_workers', 'workers_for_global_step', 'last_complete_step', '_incomplete_wait_for_step_window', 'dynamic_refresh', '_training_end_delay_refresh', 'bucket_name', 'prefix_name'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d06b380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-cookbook-bucket'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5d71cef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.prefix_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a719c242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.prefix_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6eb6dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_path = f\"s3://{trial.bucket_name}/{trial.prefix_name}\"\n",
    "target_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af1db884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021-04-23 12:49:35       5504 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/collections/000000000/worker_0_collections.json',\n",
       " '2021-04-23 12:49:35        222 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000000/000000000000_worker_0.tfevents',\n",
       " '2021-04-23 12:49:35        226 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000002/000000000002_worker_0.tfevents',\n",
       " '2021-04-23 12:49:35        226 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000004/000000000004_worker_0.tfevents',\n",
       " '2021-04-23 12:49:35        226 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000006/000000000006_worker_0.tfevents',\n",
       " '2021-04-23 12:49:35        226 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000008/000000000008_worker_0.tfevents',\n",
       " '2021-04-23 12:49:35        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000010/000000000010_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000012/000000000012_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000014/000000000014_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000016/000000000016_worker_0.tfevents',\n",
       " '2021-04-23 12:49:35        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000018/000000000018_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000020/000000000020_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000022/000000000022_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000024/000000000024_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000026/000000000026_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000028/000000000028_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000030/000000000030_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000032/000000000032_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000034/000000000034_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000036/000000000036_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000038/000000000038_worker_0.tfevents',\n",
       " '2021-04-23 12:49:35        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000040/000000000040_worker_0.tfevents',\n",
       " '2021-04-23 12:49:35        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000042/000000000042_worker_0.tfevents',\n",
       " '2021-04-23 12:49:35        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000044/000000000044_worker_0.tfevents',\n",
       " '2021-04-23 12:49:35        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000046/000000000046_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000048/000000000048_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000050/000000000050_worker_0.tfevents',\n",
       " '2021-04-23 12:49:35        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000052/000000000052_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000054/000000000054_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000056/000000000056_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000058/000000000058_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000060/000000000060_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000062/000000000062_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000064/000000000064_worker_0.tfevents',\n",
       " '2021-04-23 12:49:35        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000066/000000000066_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000068/000000000068_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000070/000000000070_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000072/000000000072_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000074/000000000074_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000076/000000000076_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000078/000000000078_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000080/000000000080_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000082/000000000082_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000084/000000000084_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000086/000000000086_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000088/000000000088_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000090/000000000090_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000092/000000000092_worker_0.tfevents',\n",
       " '2021-04-23 12:49:36        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000094/000000000094_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000096/000000000096_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        228 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000098/000000000098_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000100/000000000100_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000102/000000000102_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000104/000000000104_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000106/000000000106_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000108/000000000108_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000110/000000000110_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000112/000000000112_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000114/000000000114_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000116/000000000116_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000118/000000000118_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000120/000000000120_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000122/000000000122_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000124/000000000124_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        230 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000126/000000000126_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000128/000000000128_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000130/000000000130_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000132/000000000132_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000134/000000000134_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000136/000000000136_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000138/000000000138_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000140/000000000140_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000142/000000000142_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000144/000000000144_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000146/000000000146_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000148/000000000148_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000150/000000000150_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000152/000000000152_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000154/000000000154_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000156/000000000156_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000158/000000000158_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000160/000000000160_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000162/000000000162_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000164/000000000164_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000166/000000000166_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000168/000000000168_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000170/000000000170_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000172/000000000172_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000174/000000000174_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000176/000000000176_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000178/000000000178_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000180/000000000180_worker_0.tfevents',\n",
       " '2021-04-23 12:49:37        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000182/000000000182_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000184/000000000184_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000186/000000000186_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000188/000000000188_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000190/000000000190_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000192/000000000192_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000194/000000000194_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000196/000000000196_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000198/000000000198_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000200/000000000200_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000202/000000000202_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000204/000000000204_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000206/000000000206_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000208/000000000208_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000210/000000000210_worker_0.tfevents',\n",
       " '2021-04-23 12:49:38        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000212/000000000212_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000214/000000000214_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000216/000000000216_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000218/000000000218_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000220/000000000220_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000222/000000000222_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000224/000000000224_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000226/000000000226_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000228/000000000228_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000230/000000000230_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000232/000000000232_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000234/000000000234_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000236/000000000236_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000238/000000000238_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000240/000000000240_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000242/000000000242_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000244/000000000244_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000246/000000000246_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000248/000000000248_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000250/000000000250_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000252/000000000252_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000254/000000000254_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000256/000000000256_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000258/000000000258_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000260/000000000260_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000262/000000000262_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000264/000000000264_worker_0.tfevents',\n",
       " '2021-04-23 12:49:39        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000266/000000000266_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000268/000000000268_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000270/000000000270_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000272/000000000272_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000274/000000000274_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000276/000000000276_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000278/000000000278_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000280/000000000280_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000282/000000000282_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000284/000000000284_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000286/000000000286_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000288/000000000288_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000290/000000000290_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000292/000000000292_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000294/000000000294_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000296/000000000296_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000298/000000000298_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000300/000000000300_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000302/000000000302_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000304/000000000304_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000306/000000000306_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000308/000000000308_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000310/000000000310_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000312/000000000312_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000314/000000000314_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000316/000000000316_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000318/000000000318_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000320/000000000320_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000322/000000000322_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000324/000000000324_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000326/000000000326_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000328/000000000328_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000330/000000000330_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000332/000000000332_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000334/000000000334_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000336/000000000336_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000338/000000000338_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000340/000000000340_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000342/000000000342_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000344/000000000344_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000346/000000000346_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000348/000000000348_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000350/000000000350_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000352/000000000352_worker_0.tfevents',\n",
       " '2021-04-23 12:49:40        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000354/000000000354_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000356/000000000356_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000358/000000000358_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000360/000000000360_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000362/000000000362_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000364/000000000364_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000366/000000000366_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000368/000000000368_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000370/000000000370_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000372/000000000372_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000374/000000000374_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000376/000000000376_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000378/000000000378_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000380/000000000380_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000382/000000000382_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000384/000000000384_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000386/000000000386_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000388/000000000388_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000390/000000000390_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000392/000000000392_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000394/000000000394_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000396/000000000396_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000398/000000000398_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000400/000000000400_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000402/000000000402_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000404/000000000404_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000406/000000000406_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000408/000000000408_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000410/000000000410_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000412/000000000412_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000414/000000000414_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000416/000000000416_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000418/000000000418_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000420/000000000420_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000422/000000000422_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000424/000000000424_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000426/000000000426_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000428/000000000428_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000430/000000000430_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000432/000000000432_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000434/000000000434_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000436/000000000436_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000438/000000000438_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000440/000000000440_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000442/000000000442_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000444/000000000444_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000446/000000000446_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000448/000000000448_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000450/000000000450_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000452/000000000452_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000454/000000000454_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000456/000000000456_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000458/000000000458_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000460/000000000460_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000462/000000000462_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000464/000000000464_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000466/000000000466_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000468/000000000468_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000470/000000000470_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000472/000000000472_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000474/000000000474_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000476/000000000476_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000478/000000000478_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000480/000000000480_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000482/000000000482_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000484/000000000484_worker_0.tfevents',\n",
       " '2021-04-23 12:49:41        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000486/000000000486_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000488/000000000488_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000490/000000000490_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000492/000000000492_worker_0.tfevents',\n",
       " '2021-04-23 12:49:42        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000494/000000000494_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000496/000000000496_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000498/000000000498_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000500/000000000500_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000502/000000000502_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000504/000000000504_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000506/000000000506_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000508/000000000508_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000510/000000000510_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000512/000000000512_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000514/000000000514_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000516/000000000516_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000518/000000000518_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000520/000000000520_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000522/000000000522_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000524/000000000524_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000526/000000000526_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000528/000000000528_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000530/000000000530_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000532/000000000532_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000534/000000000534_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000536/000000000536_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000538/000000000538_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000540/000000000540_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000542/000000000542_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000544/000000000544_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000546/000000000546_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000548/000000000548_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000550/000000000550_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000552/000000000552_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000554/000000000554_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000556/000000000556_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000558/000000000558_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000560/000000000560_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000562/000000000562_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000564/000000000564_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000566/000000000566_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000568/000000000568_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000570/000000000570_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000572/000000000572_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000574/000000000574_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000576/000000000576_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000578/000000000578_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000580/000000000580_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000582/000000000582_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000584/000000000584_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000586/000000000586_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000588/000000000588_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000590/000000000590_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000592/000000000592_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000594/000000000594_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000596/000000000596_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000598/000000000598_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000600/000000000600_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000602/000000000602_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000604/000000000604_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000606/000000000606_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000608/000000000608_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000610/000000000610_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000612/000000000612_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000614/000000000614_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000616/000000000616_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000618/000000000618_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000620/000000000620_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000622/000000000622_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000624/000000000624_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000626/000000000626_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000628/000000000628_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000630/000000000630_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000632/000000000632_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000634/000000000634_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000636/000000000636_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000638/000000000638_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000640/000000000640_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000642/000000000642_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000644/000000000644_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000646/000000000646_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000648/000000000648_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000650/000000000650_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000652/000000000652_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000654/000000000654_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000656/000000000656_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000658/000000000658_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000660/000000000660_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000662/000000000662_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000664/000000000664_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000666/000000000666_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000668/000000000668_worker_0.tfevents',\n",
       " '2021-04-23 12:49:44        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000670/000000000670_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000672/000000000672_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000674/000000000674_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000676/000000000676_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000678/000000000678_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000680/000000000680_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000682/000000000682_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000684/000000000684_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000686/000000000686_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000688/000000000688_worker_0.tfevents',\n",
       " '2021-04-23 12:49:43        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000690/000000000690_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000692/000000000692_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000694/000000000694_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000696/000000000696_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000698/000000000698_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000700/000000000700_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000702/000000000702_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000704/000000000704_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000706/000000000706_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000708/000000000708_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000710/000000000710_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000712/000000000712_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000714/000000000714_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000716/000000000716_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000718/000000000718_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000720/000000000720_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000722/000000000722_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000724/000000000724_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000726/000000000726_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000728/000000000728_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000730/000000000730_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000732/000000000732_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000734/000000000734_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000736/000000000736_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000738/000000000738_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000740/000000000740_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000742/000000000742_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000744/000000000744_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000746/000000000746_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000748/000000000748_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000750/000000000750_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000752/000000000752_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000754/000000000754_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000756/000000000756_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000758/000000000758_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000760/000000000760_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000762/000000000762_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000764/000000000764_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000766/000000000766_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000768/000000000768_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000770/000000000770_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000772/000000000772_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000774/000000000774_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000776/000000000776_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000778/000000000778_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000780/000000000780_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000782/000000000782_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000784/000000000784_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000786/000000000786_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000788/000000000788_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000790/000000000790_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000792/000000000792_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000794/000000000794_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000796/000000000796_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000798/000000000798_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000800/000000000800_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000802/000000000802_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000804/000000000804_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000806/000000000806_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000808/000000000808_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000810/000000000810_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000812/000000000812_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000814/000000000814_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000816/000000000816_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000818/000000000818_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000820/000000000820_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000822/000000000822_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000824/000000000824_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000826/000000000826_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000828/000000000828_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000830/000000000830_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000832/000000000832_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000834/000000000834_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000836/000000000836_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000838/000000000838_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000840/000000000840_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000842/000000000842_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000844/000000000844_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000846/000000000846_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000848/000000000848_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000850/000000000850_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000852/000000000852_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000854/000000000854_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000856/000000000856_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000858/000000000858_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000860/000000000860_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000862/000000000862_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000864/000000000864_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000866/000000000866_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000868/000000000868_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000870/000000000870_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000872/000000000872_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000874/000000000874_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000876/000000000876_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000878/000000000878_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000880/000000000880_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000882/000000000882_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000884/000000000884_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000886/000000000886_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000888/000000000888_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000890/000000000890_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000892/000000000892_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000894/000000000894_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000896/000000000896_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000898/000000000898_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000900/000000000900_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000902/000000000902_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000904/000000000904_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000906/000000000906_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000908/000000000908_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000910/000000000910_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000912/000000000912_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000914/000000000914_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000916/000000000916_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000918/000000000918_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000920/000000000920_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000922/000000000922_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000924/000000000924_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000926/000000000926_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000928/000000000928_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000930/000000000930_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000932/000000000932_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000934/000000000934_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000936/000000000936_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000938/000000000938_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000940/000000000940_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000942/000000000942_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000944/000000000944_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000946/000000000946_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000948/000000000948_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000950/000000000950_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000952/000000000952_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000954/000000000954_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000956/000000000956_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000958/000000000958_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000960/000000000960_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000962/000000000962_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000964/000000000964_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000966/000000000966_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000968/000000000968_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000970/000000000970_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000972/000000000972_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000974/000000000974_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000976/000000000976_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000978/000000000978_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000980/000000000980_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000982/000000000982_worker_0.tfevents',\n",
       " '2021-04-23 12:49:45        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000984/000000000984_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000986/000000000986_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000988/000000000988_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000990/000000000990_worker_0.tfevents',\n",
       " '2021-04-23 12:49:46        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000992/000000000992_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000994/000000000994_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000996/000000000996_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        232 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000000998/000000000998_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001000/000000001000_worker_0.tfevents',\n",
       " '2021-04-23 12:49:47        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001002/000000001002_worker_0.tfevents',\n",
       " '2021-04-23 12:49:48        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001004/000000001004_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001006/000000001006_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001008/000000001008_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001010/000000001010_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001012/000000001012_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001014/000000001014_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001016/000000001016_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001018/000000001018_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001020/000000001020_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001022/000000001022_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001024/000000001024_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001026/000000001026_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001028/000000001028_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001030/000000001030_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001032/000000001032_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001034/000000001034_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001036/000000001036_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001038/000000001038_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001040/000000001040_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001042/000000001042_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001044/000000001044_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001046/000000001046_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001048/000000001048_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001050/000000001050_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001052/000000001052_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001054/000000001054_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001056/000000001056_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001058/000000001058_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001060/000000001060_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001062/000000001062_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001064/000000001064_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001066/000000001066_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001068/000000001068_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001070/000000001070_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001072/000000001072_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001074/000000001074_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001076/000000001076_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001078/000000001078_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001080/000000001080_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001082/000000001082_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001084/000000001084_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001086/000000001086_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001088/000000001088_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001090/000000001090_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001092/000000001092_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001094/000000001094_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001096/000000001096_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001098/000000001098_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001100/000000001100_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001102/000000001102_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001104/000000001104_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001106/000000001106_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001108/000000001108_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001110/000000001110_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001112/000000001112_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001114/000000001114_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001116/000000001116_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001118/000000001118_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001120/000000001120_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001122/000000001122_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001124/000000001124_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001126/000000001126_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001128/000000001128_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001130/000000001130_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001132/000000001132_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001134/000000001134_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001136/000000001136_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001138/000000001138_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001140/000000001140_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001142/000000001142_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001144/000000001144_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001146/000000001146_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001148/000000001148_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001150/000000001150_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001152/000000001152_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001154/000000001154_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001156/000000001156_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001158/000000001158_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001160/000000001160_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001162/000000001162_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001164/000000001164_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001166/000000001166_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001168/000000001168_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001170/000000001170_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001172/000000001172_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001174/000000001174_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001176/000000001176_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001178/000000001178_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001180/000000001180_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001182/000000001182_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001184/000000001184_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001186/000000001186_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001188/000000001188_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001190/000000001190_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001192/000000001192_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001194/000000001194_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001196/000000001196_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001198/000000001198_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001200/000000001200_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001202/000000001202_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001204/000000001204_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001206/000000001206_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001208/000000001208_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001210/000000001210_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001212/000000001212_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001214/000000001214_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001216/000000001216_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001218/000000001218_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001220/000000001220_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001222/000000001222_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001224/000000001224_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001226/000000001226_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001228/000000001228_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001230/000000001230_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001232/000000001232_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001234/000000001234_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001236/000000001236_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001238/000000001238_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001240/000000001240_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001242/000000001242_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001244/000000001244_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001246/000000001246_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001248/000000001248_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001250/000000001250_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001252/000000001252_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001254/000000001254_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001256/000000001256_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001258/000000001258_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001260/000000001260_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001262/000000001262_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001264/000000001264_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001266/000000001266_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001268/000000001268_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001270/000000001270_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001272/000000001272_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001274/000000001274_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001276/000000001276_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001278/000000001278_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001280/000000001280_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001282/000000001282_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001284/000000001284_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001286/000000001286_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001288/000000001288_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001290/000000001290_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001292/000000001292_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001294/000000001294_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001296/000000001296_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001298/000000001298_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001300/000000001300_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001302/000000001302_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001304/000000001304_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001306/000000001306_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001308/000000001308_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001310/000000001310_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001312/000000001312_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001314/000000001314_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001316/000000001316_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001318/000000001318_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001320/000000001320_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001322/000000001322_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001324/000000001324_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001326/000000001326_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001328/000000001328_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001330/000000001330_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001332/000000001332_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001334/000000001334_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001336/000000001336_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001338/000000001338_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001340/000000001340_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001342/000000001342_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001344/000000001344_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001346/000000001346_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001348/000000001348_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001350/000000001350_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001352/000000001352_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001354/000000001354_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001356/000000001356_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001358/000000001358_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001360/000000001360_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001362/000000001362_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001364/000000001364_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001366/000000001366_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001368/000000001368_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001370/000000001370_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001372/000000001372_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001374/000000001374_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001376/000000001376_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001378/000000001378_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001380/000000001380_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001382/000000001382_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001384/000000001384_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001386/000000001386_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001388/000000001388_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001390/000000001390_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001392/000000001392_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001394/000000001394_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001396/000000001396_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001398/000000001398_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001400/000000001400_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001402/000000001402_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001404/000000001404_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001406/000000001406_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001408/000000001408_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001410/000000001410_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001412/000000001412_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001414/000000001414_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001416/000000001416_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001418/000000001418_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001420/000000001420_worker_0.tfevents',\n",
       " '2021-04-23 12:49:49        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001422/000000001422_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001424/000000001424_worker_0.tfevents',\n",
       " '2021-04-23 12:49:52        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001426/000000001426_worker_0.tfevents',\n",
       " '2021-04-23 12:49:50        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001428/000000001428_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001430/000000001430_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001432/000000001432_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001434/000000001434_worker_0.tfevents',\n",
       " '2021-04-23 12:49:51        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001436/000000001436_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001438/000000001438_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001440/000000001440_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001442/000000001442_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001444/000000001444_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001446/000000001446_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001448/000000001448_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001450/000000001450_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001452/000000001452_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001454/000000001454_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001456/000000001456_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001458/000000001458_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001460/000000001460_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001462/000000001462_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001464/000000001464_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001466/000000001466_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001468/000000001468_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001470/000000001470_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001472/000000001472_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001474/000000001474_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001476/000000001476_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001478/000000001478_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001480/000000001480_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001482/000000001482_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001484/000000001484_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001486/000000001486_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001488/000000001488_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001490/000000001490_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001492/000000001492_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001494/000000001494_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001496/000000001496_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001498/000000001498_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001500/000000001500_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001502/000000001502_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001504/000000001504_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001506/000000001506_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001508/000000001508_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001510/000000001510_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001512/000000001512_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001514/000000001514_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001516/000000001516_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001518/000000001518_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001520/000000001520_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001522/000000001522_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001524/000000001524_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001526/000000001526_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001528/000000001528_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001530/000000001530_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001532/000000001532_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001534/000000001534_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001536/000000001536_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001538/000000001538_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001540/000000001540_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001542/000000001542_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001544/000000001544_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001546/000000001546_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001548/000000001548_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001550/000000001550_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001552/000000001552_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001554/000000001554_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001556/000000001556_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001558/000000001558_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001560/000000001560_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001562/000000001562_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001564/000000001564_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001566/000000001566_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001568/000000001568_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001570/000000001570_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001572/000000001572_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001574/000000001574_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001576/000000001576_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001578/000000001578_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001580/000000001580_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001582/000000001582_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001584/000000001584_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001586/000000001586_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001588/000000001588_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001590/000000001590_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001592/000000001592_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001594/000000001594_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001596/000000001596_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001598/000000001598_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001600/000000001600_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001602/000000001602_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001604/000000001604_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001606/000000001606_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001608/000000001608_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001610/000000001610_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001612/000000001612_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001614/000000001614_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001616/000000001616_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001618/000000001618_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001620/000000001620_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001622/000000001622_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001624/000000001624_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001626/000000001626_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001628/000000001628_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001630/000000001630_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001632/000000001632_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001634/000000001634_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001636/000000001636_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001638/000000001638_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001640/000000001640_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001642/000000001642_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001644/000000001644_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001646/000000001646_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001648/000000001648_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001650/000000001650_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001652/000000001652_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001654/000000001654_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001656/000000001656_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001658/000000001658_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001660/000000001660_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001662/000000001662_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001664/000000001664_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001666/000000001666_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001668/000000001668_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001670/000000001670_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001672/000000001672_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001674/000000001674_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001676/000000001676_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001678/000000001678_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001680/000000001680_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001682/000000001682_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001684/000000001684_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001686/000000001686_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001688/000000001688_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001690/000000001690_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001692/000000001692_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001694/000000001694_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001696/000000001696_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001698/000000001698_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001700/000000001700_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001702/000000001702_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001704/000000001704_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001706/000000001706_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001708/000000001708_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001710/000000001710_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001712/000000001712_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001714/000000001714_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001716/000000001716_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001718/000000001718_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001720/000000001720_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001722/000000001722_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001724/000000001724_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001726/000000001726_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001728/000000001728_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001730/000000001730_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001732/000000001732_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001734/000000001734_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001736/000000001736_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001738/000000001738_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001740/000000001740_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001742/000000001742_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001744/000000001744_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001746/000000001746_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001748/000000001748_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001750/000000001750_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001752/000000001752_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001754/000000001754_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001756/000000001756_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001758/000000001758_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001760/000000001760_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001762/000000001762_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001764/000000001764_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001766/000000001766_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001768/000000001768_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001770/000000001770_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001772/000000001772_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001774/000000001774_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001776/000000001776_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001778/000000001778_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001780/000000001780_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001782/000000001782_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001784/000000001784_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001786/000000001786_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001788/000000001788_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001790/000000001790_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001792/000000001792_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001794/000000001794_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001796/000000001796_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001798/000000001798_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001800/000000001800_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001802/000000001802_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001804/000000001804_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001806/000000001806_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001808/000000001808_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001810/000000001810_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001812/000000001812_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001814/000000001814_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001816/000000001816_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001818/000000001818_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001820/000000001820_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001822/000000001822_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001824/000000001824_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001826/000000001826_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001828/000000001828_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001830/000000001830_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001832/000000001832_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001834/000000001834_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001836/000000001836_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001838/000000001838_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001840/000000001840_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001842/000000001842_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001844/000000001844_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001846/000000001846_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001848/000000001848_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001850/000000001850_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001852/000000001852_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001854/000000001854_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001856/000000001856_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001858/000000001858_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001860/000000001860_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001862/000000001862_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001864/000000001864_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001866/000000001866_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001868/000000001868_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001870/000000001870_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001872/000000001872_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001874/000000001874_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001876/000000001876_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001878/000000001878_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001880/000000001880_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001882/000000001882_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001884/000000001884_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001886/000000001886_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001888/000000001888_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001890/000000001890_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001892/000000001892_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001894/000000001894_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001896/000000001896_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001898/000000001898_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001900/000000001900_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001902/000000001902_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001904/000000001904_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001906/000000001906_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001908/000000001908_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001910/000000001910_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001912/000000001912_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001914/000000001914_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001916/000000001916_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001918/000000001918_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001920/000000001920_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001922/000000001922_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001924/000000001924_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001926/000000001926_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001928/000000001928_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001930/000000001930_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001932/000000001932_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001934/000000001934_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001936/000000001936_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001938/000000001938_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001940/000000001940_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001942/000000001942_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001944/000000001944_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001946/000000001946_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001948/000000001948_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001950/000000001950_worker_0.tfevents',\n",
       " '2021-04-23 12:49:53        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001952/000000001952_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001954/000000001954_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001956/000000001956_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001958/000000001958_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001960/000000001960_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001962/000000001962_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001964/000000001964_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001966/000000001966_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001968/000000001968_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001970/000000001970_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001972/000000001972_worker_0.tfevents',\n",
       " '2021-04-23 12:49:57        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001974/000000001974_worker_0.tfevents',\n",
       " '2021-04-23 12:50:00        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001976/000000001976_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001978/000000001978_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001980/000000001980_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001982/000000001982_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001984/000000001984_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001986/000000001986_worker_0.tfevents',\n",
       " '2021-04-23 12:49:59        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001988/000000001988_worker_0.tfevents',\n",
       " '2021-04-23 12:49:56        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001990/000000001990_worker_0.tfevents',\n",
       " '2021-04-23 12:49:58        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001992/000000001992_worker_0.tfevents',\n",
       " '2021-04-23 12:49:54        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001994/000000001994_worker_0.tfevents',\n",
       " '2021-04-23 12:49:55        234 debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/events/000000001996/000000001996_worker_0.tfevents',\n",
       " ...]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_contents = !aws s3 ls {target_path} --recursive\n",
    "s3_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "34895240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/index/000000006/000000006998_worker_0.json'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_path = s3_contents[-1].split(\" \")[-1]\n",
    "chosen_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0126539a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-cookbook-bucket/debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/index/000000006/000000006998_worker_0.json'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_fullpath = f\"s3://{trial.bucket_name}/{chosen_path}\"\n",
    "chosen_fullpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "59fca370",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cf5f2d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 271 Bytes/271 Bytes (1.9 KiB/s) with 1 file(s) remaining\r",
      "download: s3://sagemaker-cookbook-bucket/debugger/sagemaker-xgboost-2021-04-23-12-46-34-431/debug-output/index/000000006/000000006998_worker_0.json to tmp/worker_0.json\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {chosen_fullpath} tmp/worker_0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb6cc96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"meta\": {\"mode\": \"GLOBAL\", \"mode_step\": 6998, \"event_file_name\": \"events/000000006998/000000006998_worker_0.tfevents\"}, \"tensor_payload\": [{\"tensorname\": \"train-error\", \"start_idx\": 0, \"length\": 112}, {\"tensorname\": \"validation-error\", \"start_idx\": 112, \"length\": 122}]}"
     ]
    }
   ],
   "source": [
    "!cat tmp/worker_0.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24cfec7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worker_0']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.tensor(\"train-error\").workers(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0b7037fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: [TRAIN ERROR]=0.1053 [VALIDATION ERROR]={validation_error}\n",
      "STEP 4: [TRAIN ERROR]=0.0893 [VALIDATION ERROR]={validation_error}\n",
      "STEP 6: [TRAIN ERROR]=0.0800 [VALIDATION ERROR]={validation_error}\n",
      "STEP 8: [TRAIN ERROR]=0.0670 [VALIDATION ERROR]={validation_error}\n",
      "STEP 10: [TRAIN ERROR]=0.0633 [VALIDATION ERROR]={validation_error}\n",
      "STEP 12: [TRAIN ERROR]=0.0613 [VALIDATION ERROR]={validation_error}\n",
      "STEP 14: [TRAIN ERROR]=0.0567 [VALIDATION ERROR]={validation_error}\n",
      "STEP 16: [TRAIN ERROR]=0.0540 [VALIDATION ERROR]={validation_error}\n"
     ]
    }
   ],
   "source": [
    "for i in [2, 4, 6, 8, 10, 12, 14, 16]:\n",
    "    traint = trial.tensor(\"train-error\")\n",
    "    train_error = traint.value(i)[0]\n",
    "    train_error = \"{0:.4f}\".format(train_error)\n",
    "    \n",
    "    valt = trial.tensor(\"validation-error\")\n",
    "    validation_error = valt.value(i)[0]\n",
    "    validation_error = \"{0:.4f}\".format(validation_error)\n",
    "    print(f\"STEP {i}: [TRAIN ERROR]={train_error} \" +\n",
    "          \"[VALIDATION ERROR]={validation_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fbc7288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r rule_job_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5ea9b9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RuleConfigurationName': 'LossNotDecreasing',\n",
       " 'RuleEvaluationJobArn': 'arn:aws:sagemaker:us-east-1:581320662326:processing-job/sagemaker-xgboost-2021-04--lossnotdecreasing-ee3df9a4',\n",
       " 'RuleEvaluationStatus': 'IssuesFound',\n",
       " 'StatusDetails': 'RuleEvaluationConditionMet: Evaluation of the rule LossNotDecreasing at step 8 resulted in the condition being met\\n',\n",
       " 'LastModifiedTime': datetime.datetime(2021, 4, 23, 12, 57, 6, 274000, tzinfo=tzlocal())}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lfx(r):\n",
    "    return r[\"RuleConfigurationName\"] == \"LossNotDecreasing\"\n",
    "\n",
    "loss_not_decreasing_summary = list(filter(\n",
    "    lfx, \n",
    "    rule_job_summary))[0]\n",
    "\n",
    "loss_not_decreasing_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fcdcf06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:sagemaker:us-east-1:581320662326:processing-job/sagemaker-xgboost-2021-04--lossnotdecreasing-ee3df9a4'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = loss_not_decreasing_summary\n",
    "rule_evaluation_job_arn = summary['RuleEvaluationJobArn']\n",
    "rule_evaluation_job_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1551792e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7f46388b6438>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker \n",
    "from sagemaker.processing import ProcessingJob\n",
    "\n",
    "session = sagemaker.Session()\n",
    "processing_job = ProcessingJob.from_processing_arn(\n",
    "    sagemaker_session=session, \n",
    "    processing_job_arn=rule_evaluation_job_arn)\n",
    "\n",
    "processing_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "387fd18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': [<sagemaker.processing.ProcessingInput at 0x7f46388b6710>],\n",
       " 'outputs': [<sagemaker.processing.ProcessingOutput at 0x7f46388b63c8>],\n",
       " 'output_kms_key': None,\n",
       " 'sagemaker_session': <sagemaker.session.Session at 0x7f4638680b70>,\n",
       " 'job_name': 'sagemaker-xgboost-2021-04--lossnotdecreasing-ee3df9a4'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_job.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c5ed51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '/opt/ml/processing/output/rule',\n",
       " 'destination': 's3://sagemaker-us-east-1-581320662326/sagemaker-xgboost-2021-04-23-12-46-34-431/rule-output/LossNotDecreasing',\n",
       " 'output_name': 'RuleOutput',\n",
       " 's3_upload_mode': 'EndOfJob',\n",
       " 'app_managed': False,\n",
       " 'feature_store_output': None}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_job.outputs[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3d455cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-xgboost-2021-04--lossnotdecreasing-ee3df9a4'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_job.job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aa7a12f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"us-east-1\"\n",
    "group = \"/aws/sagemaker/ProcessingJobs\"\n",
    "prefix = processing_job.job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ada0495f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: awslogs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.14.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awslogs) (2.8.1)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awslogs) (1.1.0)\r\n",
      "Requirement already satisfied: boto3>=1.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awslogs) (1.17.35)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awslogs) (0.10.0)\r\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.5.0->awslogs) (0.3.4)\r\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.5.0->awslogs) (1.20.35)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.35->boto3>=1.5.0->awslogs) (1.26.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.4.0->awslogs) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install awslogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5970cb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:14.058 ip-10-0-109-175.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192043\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:14.524 ip-10-0-109-175.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.530 ip-10-0-109-175.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.531 ip-10-0-109-175.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.531 ip-10-0-109-175.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.531 ip-10-0-109-175.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.531 ip-10-0-109-175.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.531 ip-10-0-109-175.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.531 ip-10-0-109-175.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.532 ip-10-0-109-175.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.532 ip-10-0-109-175.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.532 ip-10-0-109-175.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.532 ip-10-0-109-175.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.532 ip-10-0-109-175.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.532 ip-10-0-109-175.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.532 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.532 ip-10-0-109-175.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.533 ip-10-0-109-175.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.573 ip-10-0-109-175.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.573 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619192160000000 to timestamp_end:1619192220000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.574 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619192160000000 to timestamp_end:1619192220000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.575 ip-10-0-109-175.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.575 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619192160000000 to timestamp_end:1619192220000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.575 ip-10-0-109-175.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.576 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619192160000000 to timestamp_end:1619192220000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.576 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619192160000000 to timestamp_end:1619192220000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.577 ip-10-0-109-175.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.577 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619192160000000 to timestamp_end:1619192220000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.577 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619192160000000 to timestamp_end:1619192220000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.577 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619192160000000 to timestamp_end:1619192220000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.578 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619192160000000 to timestamp_end:1619192220000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.581 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619192160000000 to timestamp_end:1619192220000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:24.581 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619192160000000 to timestamp_end:1619192220000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:28.656 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:25.464257', 'end_time': '2021-04-23T15:38:25.483437', 'duration': 0.01918, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:25.501358', 'end_time': '2021-04-23T15:38:26.326017', 'duration': 0.824659, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:25.525801Z', 'iopub.execute_input': '2021-04-23T15:38:25.526372Z', 'iopub.status.idle': '2021-04-23T15:38:26.325371Z', 'shell.execute_reply': '2021-04-23T15:38:26.325783Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 15:38:26.317 ip-10-0-109-175.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192043\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:26.345605', 'end_time': '2021-04-23T15:38:26.556956', 'duration': 0.211351, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:26.367584Z', 'iopub.execute_input': '2021-04-23T15:38:26.368100Z', 'shell.execute_reply': '2021-04-23T15:38:26.556407Z', 'iopub.status.idle': '2021-04-23T15:38:26.556847Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n   \r\n",
      "   if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:26.575472', 'end_time': '2021-04-23T15:38:26.602724', 'duration': 0.027252, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:26.600420Z', 'iopub.execute_input': '2021-04-23T15:38:26.600977Z', 'iopub.status.idle': '2021-04-23T15:38:26.602168Z', 'shell.execute_reply': '2021-04-23T15:38:26.602549Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:26.621076', 'end_time': '2021-04-23T15:38:26.645400', 'duration': 0.024324, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:26.643100Z', 'iopub.execute_input': '2021-04-23T15:38:26.643611Z', 'shell.execute_reply': '2021-04-23T15:38:26.644879Z', 'iopub.status.idle': '2021-04-23T15:38:26.645292Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:26.663639', 'end_time': '2021-04-23T15:38:26.681782', 'duration': 0.018143, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:26.700399', 'end_time': '2021-04-23T15:38:26.724698', 'duration': 0.024299, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:26.722311Z', 'iopub.execute_input': '2021-04-23T15:38:26.722855Z', 'shell.execute_reply': '2021-04-23T15:38:26.724103Z', 'iopub.status.idle': '2021-04-23T15:38:26.724590Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:26.743174', 'end_time': '2021-04-23T15:38:26.774978', 'duration': 0.031804, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:26.770923Z', 'iopub.execute_input': '2021-04-23T15:38:26.771484Z', 'shell.execute_reply': '2021-04-23T15:38:26.774407Z', 'iopub.status.idle': '2021-04-23T15:38:26.774863Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:26.793996', 'end_time': '2021-04-23T15:38:26.836901', 'duration': 0.042905, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:26.823073Z', 'iopub.execute_input': '2021-04-23T15:38:26.826771Z', 'shell.execute_reply': '2021-04-23T15:38:26.836360Z', 'iopub.status.idle': '2021-04-23T15:38:26.836790Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"01b97422-b5be-4537-aff5-e224765c0ce7\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"760f5c70-e0c3-4fb1-850a-52133f2e4866\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1009\"},\"field\":\"0\",\"formatter\":{\"id\":\"1010\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringFormatter\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 15:36:43 and ran for 16 seconds. \\\\n Your training job started on 04/23/2021 at 15:36:43 and ran for 16 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"Selection\"},{\"attributes\":{\"editor\":{\"id\":\"1011\"},\"field\":\"1\",\"formatter\":{\"id\":\"1012\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"15:36:43 04/23/2021\",\"15:36:59 04/23/2021\",\"16 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1013\"},\"selection_policy\":{\"id\":\"1014\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringEditor\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"760f5c70-e0c3-4fb1-850a-52133f2e4866\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"01b97422-b5be-4537-aff5-e224765c0ce7\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:26.856422', 'end_time': '2021-04-23T15:38:26.876007', 'duration': 0.019585, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:26.895485', 'end_time': '2021-04-23T15:38:26.920570', 'duration': 0.025085, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:26.917983Z', 'iopub.execute_input': '2021-04-23T15:38:26.918506Z', 'shell.execute_reply': '2021-04-23T15:38:26.919974Z', 'iopub.status.idle': '2021-04-23T15:38:26.920463Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-in\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m put'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:26.940088', 'end_time': '2021-04-23T15:38:26.969159', 'duration': 0.029071, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:26.966798Z', 'iopub.execute_input': '2021-04-23T15:38:26.967348Z', 'iopub.status.idle': '2021-04-23T15:38:26.968645Z', 'shell.execute_reply': '2021-04-23T15:38:26.969023Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:26.988881', 'end_time': '2021-04-23T15:38:27.036289', 'duration': 0.047408, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.017564Z', 'iopub.execute_input': '2021-04-23T15:38:27.032347Z', 'iopub.status.idle': '2021-04-23T15:38:27.035771Z', 'shell.execute_reply': '2021-04-23T15:38:27.036150Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"040321d5-4233-4f2f-a536-f3a20dc8e6e5\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"72e67c5e-cfe7-4c23-bdd0-5c9db4fc6269\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1068\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1069\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringFormatter\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringEditor\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{\"editor\":{\"id\":\"1070\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1071\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1074\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1075\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQClcj8L1qDFArkfhehROREA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAB7FK5H4fpXQBSuR+F6lCpAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQIXrUbgehS9AAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQJqZmZmZmTFAZmZmZmZmLEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQGZmZmZmpjFAmpmZmZm5QUA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1082\"},\"selection_policy\":{\"id\":\"1083\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringFormatter\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{\"editor\":{\"id\":\"1078\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1079\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1076\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1077\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1066\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1067\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1072\"},\"field\":\"max\",\"formatter\":{\"id\":\"1073\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1080\"},\"field\":\"min\",\"formatter\":{\"id\":\"1081\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"72e67c5e-cfe7-4c23-bdd0-5c9db4fc6269\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"040321d5-4233-4f2f-a536-f3a20dc8e6e5\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.056697', 'end_time': '2021-04-23T15:38:27.097094', 'duration': 0.040397, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.091168Z', 'iopub.execute_input': '2021-04-23T15:38:27.091781Z', 'shell.execute_reply': '2021-04-23T15:38:27.096556Z', 'iopub.status.idle': '2021-04-23T15:38:27.096982Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.118282', 'end_time': '2021-04-23T15:38:27.150060', 'duration': 0.031778, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.147632Z', 'iopub.execute_input': '2021-04-23T15:38:27.148192Z', 'shell.execute_reply': '2021-04-23T15:38:27.149520Z', 'iopub.status.idle': '2021-04-23T1\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m 5:38:27.149948Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.171121', 'end_time': '2021-04-23T15:38:27.202262', 'duration': 0.031141, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.200129Z', 'iopub.execute_input': '2021-04-23T15:38:27.200638Z', 'iopub.status.idle': '2021-04-23T15:38:27.201744Z', 'shell.execute_reply': '2021-04-23T15:38:27.202123Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.223649', 'end_time': '2021-04-23T15:38:27.244967', 'duration': 0.021318, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.266103', 'end_time': '2021-04-23T15:38:27.294428', 'duration': 0.028325, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.292231Z', 'iopub.execute_input': '2021-04-23T15:38:27.292740Z', 'shell.execute_reply': '2021-04-23T15:38:27.293834Z', 'iopub.status.idle': '2021-04-23T15:38:27.294318Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.315647', 'end_time': '2021-04-23T15:38:27.343726', 'duration': 0.028079, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.341547Z', 'iopub.execute_input': '2021-04-23T15:38:27.342041Z', 'iopub.status.idle': '2021-04-23T15:38:27.343209Z', 'shell.execute_reply': '2021-04-23T15:38:27.343579Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.365243', 'end_time': '2021-04-23T15:38:27.403713', 'duration': 0.03847, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.392699Z', 'iopub.execute_input': '2021-04-23T15:38:27.393201Z', 'iopub.status.idle': '2021-04-23T15:38:27.403178Z', 'shell.execute_reply': '2021-04-23T15:38:27.403561Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the LoadBalancing rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>43</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>31</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>43</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n   \r\n",
      " summary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.426040', 'end_time': '2021-04-23T15:38:27.456510', 'duration': 0.03047, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.453686Z', 'iopub.execute_input': '2021-04-23T15:38:27.454236Z', 'shell.execute_reply': '2021-04-23T15:38:27.455995Z', 'iopub.status.idle': '2021-04-23T15:38:27.456402Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.479645', 'end_time': '2021-04-23T15:38:27.508740', 'duration': 0.029095, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.506503Z', 'iopub.execute_input': '2021-04-23T15:38:27.507037Z', 'shell.execute_reply': '2021-04-23T15:38:27.508226Z', 'iopub.status.idle': '2021-04-23T15:38:27.508633Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.531792', 'end_time': '2021-04-23T15:38:27.584629', 'duration': 0.052837, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.572830Z', 'iopub.execute_input': '2021-04-23T15:38:27.576745Z', 'iopub.status.idle': '2021-04-23T15:38:27.584100Z', 'shell.execute_reply': '2021-04-23T15:38:27.584485Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"73091388-c210-4428-9bdd-3fd6d9bc38a4\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"0531ba30-04fc-457a-8d91-0cf9f5a4fb9e\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"0531ba30-04fc-457a-8d91-0cf9f5a4fb9e\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"73091388-c210-4428-9bdd-3fd6d9bc38a4\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.609034', 'end_time': '2021-04-23T15:38:27.656890', 'duration': 0.047856, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.645827Z', 'iopub.execute_input': '2021-04-23T15:38:27.648576Z', 'shell.execute_reply': '2021-04-23T15:38:27.656341Z', 'iopub.status.idle': '2021-04-\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m 23T15:38:27.656780Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d5d767a8-5ae5-4522-88f2-270d9944bf6a\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"d8276a8f-2c2a-4efd-a560-8352b303f027\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"d8276a8f-2c2a-4efd-a560-8352b303f027\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"d5d767a8-5ae5-4522-88f2-270d9944bf6a\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.682760', 'end_time': '2021-04-23T15:38:27.727257', 'duration': 0.044497, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.719310Z', 'iopub.execute_input': '2021-04-23T15:38:27.719863Z', 'shell.execute_reply': '2021-04-23T15:38:27.726693Z', 'iopub.status.idle': '2021-04-23T15:38:27.727145Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"921ae2b0-7a89-4ac9-912c-5aa0eb2e2216\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"cefe45a0-0d7b-4ba0-bf91-e8db602052f1\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"cefe45a0-0d7b-4ba0-bf91-e8db602052f1\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"921ae2b0-7a89-4ac9-912c-5aa0eb2e2216\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.754580', 'end_time': '2021-04-23T15:38:27.804502', 'duration': 0.049922, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.790653Z', 'iopub.execute_input': '2021-04-23T15:38:27.792630Z', 'iopub.status.idle': '2021-04-23T15:38:27.803981Z', 'shell.execute_reply': '2021-04-23T15:38:27.804362Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"ad817f17-88af-41e9-9082-f02a5349a754\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"9e54ec13-037b-47c7-ae0a-e29129a2aae0\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"w\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m idth\":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"9e54ec13-037b-47c7-ae0a-e29129a2aae0\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"ad817f17-88af-41e9-9082-f02a5349a754\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8a4bc81f-5a50-4de4-b6f7-13b704842e37\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"fb9e6f86-e910-40c8-9ee4-fa9a2f8b7caf\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"fb9e6f86-e910-40c8-9ee4-fa9a2f8b7caf\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"8a4bc81f-5a50-4de4-b6f7-13b704842e37\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.833405', 'end_time': '2021-04-23T15:38:27.882626', 'duration': 0.049221, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.874606Z', 'iopub.execute_input': '2021-04-23T15:38:27.875184Z', 'iopub.status.idle': '2021-04-23T15:38:27.882087Z', 'shell.execute_reply': '2021-04-23T15:38:27.882461Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"15da963f-7db8-44c6-8d72-9b429f456fdb\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"79884b2f-ec8b-4ca5-88f5-46dfa12acfa8\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 31 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"79884b2f-ec8b-4ca5-88f5-46dfa12acfa8\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"15da963f-7db8-44c6-8d72-9b429f456fdb\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:27.913043', 'end_time': '2021-04-23T15:38:27.969425', 'duration': 0.056382, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:27.961155Z', 'iopub.execute_input': '2021-04-23T15:38:27.961775Z', 'shell.execute_reply': '2021-04-23T15:38:27.968900Z', 'iopub.status.idle': '2021-04-23T15:38:27.969312Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"a7659d60-b91e-4fa8-88f0-2921ef7f0225\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"0987b2ee-4a4a-478c-b1e6-b04842e3e3ed\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 43 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"0987b2ee-4a4a-478c-b1e6-b04842e3e3ed\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"a7659d60-b91e-4fa8-88f0-2921ef7f0225\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if\r\n",
      " (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:28.000941', 'end_time': '2021-04-23T15:38:28.058856', 'duration': 0.057915, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:28.044851Z', 'iopub.execute_input': '2021-04-23T15:38:28.050666Z', 'shell.execute_reply': '2021-04-23T15:38:28.058297Z', 'iopub.status.idle': '2021-04-23T15:38:28.058741Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"56ef24ef-e542-416f-a5e1-5c2ebb57904e\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"b445341a-7c84-4338-b337-dd5d4eb3211c\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 43 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"b445341a-7c84-4338-b337-dd5d4eb3211c\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"56ef24ef-e542-416f-a5e1-5c2ebb57904e\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n     \r\n",
      "                                       height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:38:28.091691', 'end_time': '2021-04-23T15:38:28.146805', 'duration': 0.055114, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:38:28.136000Z', 'iopub.execute_input': '2021-04-23T15:38:28.138801Z', 'iopub.status.idle': '2021-04-23T15:38:28.146259Z', 'shell.execute_reply': '2021-04-23T15:38:28.146663Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"b3e2d8f5-f104-4fd2-8c0b-f534987a5c3b\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"b7cd8b67-2dd6-4114-a355-447452b0feed\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"b7cd8b67-2dd6-4114-a355-447452b0feed\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"b3e2d8f5-f104-4fd2-8c0b-f534987a5c3b\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T15:38:24.607673', 'end_time': '2021-04-23T15:38:28.587535', 'duration': 3.979862, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:28.657 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:28.657 ip-10-0-109-175.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:29.055 ip-10-0-109-175.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619192280000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m [2021-04-23 15:38:29.055 ip-10-0-109-175.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZQRUWYEXTU-ProfilerReport-1619192043-24a126b6/algo-1-1619192262\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:25,  1.12cell/s]#015Executing:   7%|         | 2/30 [00:01<00:24,  1.16cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.74cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.51cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.18cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01,  9.97cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 11.07cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.57cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.87cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 15.48cell/s]#015Executing:  77%|  | 23/30 [00:03<00:00, 15.48cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.94cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 14.19cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 13.17cell/s]#015Executing: 100%|| 30/30 [00:03<00:00,  7.54cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:39:56.067 ip-10-0-244-241.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192161\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:39:56.548 ip-10-0-244-241.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:39:57.330 ip-10-0-246-154.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192159\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:39:57.827 ip-10-0-246-154.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:00.316 ip-10-0-109-143.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192154\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:00.818 ip-10-0-109-143.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:00.391 ip-10-0-244-44.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192151\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:00.878 ip-10-0-244-44.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:00.162 ip-10-2-234-68.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192149\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:00.656 ip-10-2-234-68.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.558 ip-10-0-244-241.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.558 ip-10-0-244-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.559 ip-10-0-244-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.559 ip-10-0-244-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.559 ip-10-0-244-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.559 ip-10-0-244-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.559 ip-10-0-244-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.559 ip-10-0-244-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.559 ip-10-0-244-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.559 ip-10-0-244-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.559 ip-10-0-244-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.559 ip-10-0-244-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.559 ip-10-0-244-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.560 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.560 ip-10-0-244-241.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.561 ip-10-0-244-241.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.594 ip-10-0-244-241.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.594 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.594 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.595 ip-10-0-244-241.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.595 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.595 ip-10-0-244-241.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.596 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.596 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.596 ip-10-0-244-241.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.597 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.597 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.597 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.598 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.600 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:06.600 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:06.456 ip-10-0-148-20.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192156\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:06.946 ip-10-0-148-20.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.838 ip-10-0-246-154.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.838 ip-10-0-246-154.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.838 ip-10-0-246-154.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.838 ip-10-0-246-154.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.839 ip-10-0-246-154.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.839 ip-10-0-246-154.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.839 ip-10-0-246-154.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.839 ip-10-0-246-154.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.839 ip-10-0-246-154.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.839 ip-10-0-246-154.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.839 ip-10-0-246-154.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.839 ip-10-0-246-154.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.839 ip-10-0-246-154.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.840 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.840 ip-10-0-246-154.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.840 ip-10-0-246-154.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.875 ip-10-0-246-154.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.876 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.876 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.878 ip-10-0-246-154.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.878 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.878 ip-10-0-246-154.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.878 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.879 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.880 ip-10-0-246-154.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.880 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.881 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.881 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.882 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.885 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:07.885 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.828 ip-10-0-109-143.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.828 ip-10-0-109-143.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.829 ip-10-0-109-143.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.829 ip-10-0-109-143.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.829 ip-10-0-109-143.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.829 ip-10-0-109-143.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.829 ip-10-0-109-143.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.829 ip-10-0-109-143.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.829 ip-10-0-109-143.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.829 ip-10-0-109-143.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.829 ip-10-0-109-143.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.829 ip-10-0-109-143.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.829 ip-10-0-109-143.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.830 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.830 ip-10-0-109-143.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.831 ip-10-0-109-143.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.871 ip-10-0-109-143.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.871 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.872 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.874 ip-10-0-109-143.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.874 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.874 ip-10-0-109-143.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.874 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.875 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.877 ip-10-0-109-143.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.877 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.877 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.878 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.878 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.882 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:10.882 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:10.558 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:07.460705', 'end_time': '2021-04-23T15:40:07.479705', 'duration': 0.019, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:07.497565', 'end_time': '2021-04-23T15:40:08.292053', 'duration': 0.794488, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:07.521846Z', 'iopub.execute_input': '2021-04-23T15:40:07.522342Z', 'shell.execute_reply': '2021-04-23T15:40:08.291357Z', 'iopub.status.idle': '2021-04-23T15:40:08.291916Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 15:40:08.284 ip-10-0-244-241.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192161\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:08.312238', 'end_time': '2021-04-23T15:40:08.520848', 'duration': 0.20861, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:08.334459Z', 'iopub.execute_input': '2021-04-23T15:40:08.334963Z', 'shell.execute_reply': '2021-04-23T15:40:08.520267Z', 'iopub.status.idle': '2021-04-23T15:40:08.520727Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:08.540111', 'end_time': '2021-04-23T15:40:08.567809', 'duration': 0.027698, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:08.565521Z', 'iopub.execute_input': '2021-04-23T15:40:08.566006Z', 'shell.execute_reply': '2021-04-23T15:40:08.567226Z', 'iopub.status.idle': '2021-04-23T15:40:08.567707Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:08.586825', 'end_time': '2021-04-23T15:40:08.611393', 'duration': 0.024568, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:08.609118Z', 'iopub.execute_input': '2021-04-23T15:40:08.609585Z', 'shell.execute_reply': '2021-04-23T15:40:08.610826Z', 'iopub.status.idle': '2021-04-23T15:40:08.611294Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:08.630308', 'end_time': '2021-04-23T15:40:08.649127', 'duration': 0.018819, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:08.668458', 'end_time': '2021-04-23T15:40:08.693192', 'duration': 0.024734, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:08.690915Z', 'iopub.execute_input': '2021-04-23T15:40:08.691454Z', 'iopub.status.idle': '2021-04-23T15:40:08.692646Z', 'shell.execute_reply': '2021-04-23T15:40:08.693068Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:08.712574', 'end_time': '2021-04-23T15:40:08.744334', 'duration': 0.03176, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:08.738366Z', 'iopub.execute_input': '2021-04-23T15:40:08.740535Z', 'shell.execute_reply': '2021-04-23T15:40:08.743717Z', 'iopub.status.idle': '2021-04-23T15:40:08.744231Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:08.763506', 'end_time': '2021-04-23T15:40:08.805995', 'duration': 0.042489, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:08.790025Z', 'iopub.execute_input': '2021-04-23T15:40:08.796134Z', 'shell.execute_reply': '2021-04-23T15:40:08.805404Z', 'iopub.status.idle': '2021-04-23T15:40:08.805896Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d798d764-77c8-4824-9085-396d8e9dcc35\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"9ac2a8b8-d8d7-4052-b6c3-8619a0071d39\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1013\"},\"field\":\"1\",\"formatter\":{\"id\":\"1014\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"StringEditor\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 15:38:49 and ran for 10 seconds. \\\\n Your training job started on 04/23/2021 at 15:38:49 and ran for 10 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"15:38:49 04/23/2021\",\"15:38:59 04/23/2021\",\"10 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1010\"},\"selection_policy\":{\"id\":\"1009\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1011\"},\"field\":\"0\",\"formatter\":{\"id\":\"1012\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"StringFormatter\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringEditor\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"9ac2a8b8-d8d7-4052-b6c3-8619a0071d39\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"d798d764-77c8-4824-9085-396d8e9dcc35\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:08.826254', 'end_time': '2021-04-23T15:40:08.846224', 'duration': 0.01997, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:08.866181', 'end_time': '2021-04-23T15:40:08.891617', 'duration': 0.025436, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:08.889042Z', 'iopub.execute_input': '2021-04-23T15:40:08.889558Z', 'iopub.status.idle': '2021-04-23T15:40:08.891118Z', 'shell.execute_reply': '2021-04-23T15:40:08.891501Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-input']\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m , 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:08.911591', 'end_time': '2021-04-23T15:40:08.940661', 'duration': 0.02907, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:08.938328Z', 'iopub.execute_input': '2021-04-23T15:40:08.938832Z', 'shell.execute_reply': '2021-04-23T15:40:08.940159Z', 'iopub.status.idle': '2021-04-23T15:40:08.940564Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:08.960747', 'end_time': '2021-04-23T15:40:09.007488', 'duration': 0.046741, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:08.988735Z', 'iopub.execute_input': '2021-04-23T15:40:09.003477Z', 'shell.execute_reply': '2021-04-23T15:40:09.006981Z', 'iopub.status.idle': '2021-04-23T15:40:09.007390Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"b5351335-5a94-4e71-811b-112ee0b461d2\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"203ee729-bb8e-42d5-b549-99d146b75241\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{\"editor\":{\"id\":\"1080\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1081\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1070\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1071\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1072\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1073\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1076\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1077\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1068\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1069\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1078\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1079\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringEditor\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1082\"},\"field\":\"min\",\"formatter\":{\"id\":\"1083\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQDMzMzMzMzFASOF6FK5HLEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAIBYQDMzMzMzsyxAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQLgehetRuC9AAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQLgehetR+DBAmpmZmZkZK0A=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQClcj8L1KDFACtejcD0KLEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1067\"},\"selection_policy\":{\"id\":\"1066\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringFormatter\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1074\"},\"field\":\"max\",\"formatter\":{\"id\":\"1075\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringEditor\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"203ee729-bb8e-42d5-b549-99d146b75241\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"b5351335-5a94-4e71-811b-112ee0b461d2\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.028566', 'end_time': '2021-04-23T15:40:09.068274', 'duration': 0.039708, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.062557Z', 'iopub.execute_input': '2021-04-23T15:40:09.063115Z', 'shell.execute_reply': '2021-04-23T15:40:09.067741Z', 'iopub.status.idle': '2021-04-23T15:40:09.068175Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.090075', 'end_time': '2021-04-23T15:40:09.122273', 'duration': 0.032198, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.119700Z', 'iopub.execute_input': '2021-04-23T15:40:09.120230Z', 'iopub.status.idle': '2021-04-23T15:40:09.121775Z', 'shell.execute_reply': '2021-04-23T15:40:0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m 9.122160Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.144226', 'end_time': '2021-04-23T15:40:09.175624', 'duration': 0.031398, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.173307Z', 'iopub.execute_input': '2021-04-23T15:40:09.173803Z', 'iopub.status.idle': '2021-04-23T15:40:09.175122Z', 'shell.execute_reply': '2021-04-23T15:40:09.175511Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.197598', 'end_time': '2021-04-23T15:40:09.219521', 'duration': 0.021923, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.241360', 'end_time': '2021-04-23T15:40:09.269627', 'duration': 0.028267, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.267258Z', 'iopub.execute_input': '2021-04-23T15:40:09.267763Z', 'shell.execute_reply': '2021-04-23T15:40:09.269045Z', 'iopub.status.idle': '2021-04-23T15:40:09.269528Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.291643', 'end_time': '2021-04-23T15:40:09.319381', 'duration': 0.027738, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.317069Z', 'iopub.execute_input': '2021-04-23T15:40:09.317559Z', 'shell.execute_reply': '2021-04-23T15:40:09.318880Z', 'iopub.status.idle': '2021-04-23T15:40:09.319283Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.341713', 'end_time': '2021-04-23T15:40:09.381145', 'duration': 0.039432, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.370049Z', 'iopub.execute_input': '2021-04-23T15:40:09.370530Z', 'shell.execute_reply': '2021-04-23T15:40:09.380558Z', 'iopub.status.idle': '2021-04-23T15:40:09.381046Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the CPUBottleneck rule\\nwas the most frequently triggered. It processed 25 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>25</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>21</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>25</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n    sum\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m mary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.404681', 'end_time': '2021-04-23T15:40:09.434971', 'duration': 0.03029, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.431959Z', 'iopub.execute_input': '2021-04-23T15:40:09.432446Z', 'iopub.status.idle': '2021-04-23T15:40:09.434484Z', 'shell.execute_reply': '2021-04-23T15:40:09.434858Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.458717', 'end_time': '2021-04-23T15:40:09.487243', 'duration': 0.028526, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.485054Z', 'iopub.execute_input': '2021-04-23T15:40:09.485520Z', 'shell.execute_reply': '2021-04-23T15:40:09.486727Z', 'iopub.status.idle': '2021-04-23T15:40:09.487147Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.511032', 'end_time': '2021-04-23T15:40:09.563590', 'duration': 0.052558, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.546866Z', 'iopub.execute_input': '2021-04-23T15:40:09.555365Z', 'iopub.status.idle': '2021-04-23T15:40:09.563100Z', 'shell.execute_reply': '2021-04-23T15:40:09.563478Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8773efc0-df52-4ac3-92e1-fb7b9a4456c0\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"4603adf1-7d44-4bdb-9743-763ababeb347\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"4603adf1-7d44-4bdb-9743-763ababeb347\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"8773efc0-df52-4ac3-92e1-fb7b9a4456c0\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.588909', 'end_time': '2021-04-23T15:40:09.636370', 'duration': 0.047461, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.625368Z', 'iopub.execute_input': '2021-04-23T15:40:09.628015Z', 'shell.execute_reply': '2021-04-23T15:40:09.635841Z', 'iopub.status.idle': '2021-04-23T1\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m 5:40:09.636269Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"44434775-0f21-4be2-b18b-a0c1dffa018e\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"c7d4317f-98be-4ba4-956a-c43c5f46c62b\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"c7d4317f-98be-4ba4-956a-c43c5f46c62b\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"44434775-0f21-4be2-b18b-a0c1dffa018e\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.663342', 'end_time': '2021-04-23T15:40:09.708476', 'duration': 0.045134, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.698800Z', 'iopub.execute_input': '2021-04-23T15:40:09.700733Z', 'iopub.status.idle': '2021-04-23T15:40:09.707967Z', 'shell.execute_reply': '2021-04-23T15:40:09.708353Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"cfe81ca5-7b3d-4e56-8708-7ce06dd3044c\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"c5953c73-8737-4d1c-bee5-9cafafd2f617\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"c5953c73-8737-4d1c-bee5-9cafafd2f617\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"cfe81ca5-7b3d-4e56-8708-7ce06dd3044c\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.737233', 'end_time': '2021-04-23T15:40:09.789168', 'duration': 0.051935, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.775391Z', 'iopub.execute_input': '2021-04-23T15:40:09.775925Z', 'iopub.status.idle': '2021-04-23T15:40:09.788660Z', 'shell.execute_reply': '2021-04-23T15:40:09.789046Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"9d3f08bf-1368-49a2-bbc5-dcfff68efb91\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"f5fb3242-a8df-48d8-abe1-76c67a741b82\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m \":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"f5fb3242-a8df-48d8-abe1-76c67a741b82\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"9d3f08bf-1368-49a2-bbc5-dcfff68efb91\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"f8ac8cf8-3189-40a3-93ad-e1be2c779bfb\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"1645631a-065e-4b53-aa72-bb5ff55ff1f1\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"1645631a-065e-4b53-aa72-bb5ff55ff1f1\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"f8ac8cf8-3189-40a3-93ad-e1be2c779bfb\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.819463', 'end_time': '2021-04-23T15:40:09.870804', 'duration': 0.051341, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.859708Z', 'iopub.execute_input': '2021-04-23T15:40:09.862389Z', 'iopub.status.idle': '2021-04-23T15:40:09.870315Z', 'shell.execute_reply': '2021-04-23T15:40:09.870679Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"807f94b9-2874-4eba-a130-3580b9d64907\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"189a3f5b-2a54-48d2-a041-6e7d59458276\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 21 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"189a3f5b-2a54-48d2-a041-6e7d59458276\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"807f94b9-2874-4eba-a130-3580b9d64907\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.902876', 'end_time': '2021-04-23T15:40:09.961153', 'duration': 0.058277, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.952165Z', 'iopub.execute_input': '2021-04-23T15:40:09.953057Z', 'shell.execute_reply': '2021-04-23T15:40:09.960565Z', 'iopub.status.idle': '2021-04-23T15:40:09.961054Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"c294e1ef-4f60-44c2-acf6-f968f9f3d92e\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"4c12e205-f7e4-4f12-8cff-6775e7e89a2c\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 25 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"4c12e205-f7e4-4f12-8cff-6775e7e89a2c\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"c294e1ef-4f60-44c2-acf6-f968f9f3d92e\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (at\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m tempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.993972', 'end_time': '2021-04-23T15:40:10.054345', 'duration': 0.060373, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.042168Z', 'iopub.execute_input': '2021-04-23T15:40:10.045851Z', 'shell.execute_reply': '2021-04-23T15:40:10.053802Z', 'iopub.status.idle': '2021-04-23T15:40:10.054246Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"68dff5a4-2c75-4cb4-84f5-adc9482fe57f\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"270cdf74-8bb1-4b63-acad-2b1ff362b52c\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 25 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"270cdf74-8bb1-4b63-acad-2b1ff362b52c\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"68dff5a4-2c75-4cb4-84f5-adc9482fe57f\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n         \r\n",
      "                                   height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.088594', 'end_time': '2021-04-23T15:40:10.145494', 'duration': 0.0569, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.134329Z', 'iopub.execute_input': '2021-04-23T15:40:10.137271Z', 'iopub.status.idle': '2021-04-23T15:40:10.144998Z', 'shell.execute_reply': '2021-04-23T15:40:10.145380Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"4a7ea1bc-1065-45f0-a3ae-a393324e9fe6\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"b901141f-2e08-4a1d-8aa1-004154663d59\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"b901141f-2e08-4a1d-8aa1-004154663d59\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"4a7ea1bc-1065-45f0-a3ae-a393324e9fe6\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T15:40:06.626739', 'end_time': '2021-04-23T15:40:10.486015', 'duration': 3.859276, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:10.559 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:10.559 ip-10-0-244-241.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:10.977 ip-10-0-244-241.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m [2021-04-23 15:40:10.977 ip-10-0-244-241.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:25,  1.15cell/s]#015Executing:   7%|         | 2/30 [00:01<00:23,  1.19cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.79cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.60cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.28cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01, 10.06cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 11.15cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.60cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.82cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 15.36cell/s]#015Executing:  77%|  | 23/30 [00:02<00:00, 15.36cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.78cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 13.89cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.81cell/s]#015Executing: 100%|| 30/30 [00:03<00:00,  7.77cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HLMHVSHJLD-ProfilerReport-1619192161-0cfb91a9/algo-1-1619192364\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.889 ip-10-0-244-44.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.889 ip-10-0-244-44.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.889 ip-10-0-244-44.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.889 ip-10-0-244-44.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.890 ip-10-0-244-44.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.890 ip-10-0-244-44.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.890 ip-10-0-244-44.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.890 ip-10-0-244-44.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.890 ip-10-0-244-44.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.890 ip-10-0-244-44.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.890 ip-10-0-244-44.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.890 ip-10-0-244-44.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.890 ip-10-0-244-44.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.890 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.891 ip-10-0-244-44.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:10.891 ip-10-0-244-44.ec2.internal:1 INFO profiler_trial.py:93] Current timestamp 1619192400000000 latest timestamp 1619192340000000: waiting for new profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.663 ip-10-2-234-68.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.664 ip-10-2-234-68.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.664 ip-10-2-234-68.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.664 ip-10-2-234-68.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.664 ip-10-2-234-68.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.664 ip-10-2-234-68.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.664 ip-10-2-234-68.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.664 ip-10-2-234-68.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.664 ip-10-2-234-68.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.664 ip-10-2-234-68.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.665 ip-10-2-234-68.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.665 ip-10-2-234-68.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.665 ip-10-2-234-68.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.665 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.665 ip-10-2-234-68.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.666 ip-10-2-234-68.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.702 ip-10-2-234-68.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.702 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.702 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.704 ip-10-2-234-68.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.704 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.704 ip-10-2-234-68.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.704 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.704 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.705 ip-10-2-234-68.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.705 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.706 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.706 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.707 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.710 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:10.710 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:11.867 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:08.751500', 'end_time': '2021-04-23T15:40:08.770486', 'duration': 0.018986, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:08.788454', 'end_time': '2021-04-23T15:40:09.595592', 'duration': 0.807138, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:08.813349Z', 'iopub.execute_input': '2021-04-23T15:40:08.813852Z', 'shell.execute_reply': '2021-04-23T15:40:09.594947Z', 'iopub.status.idle': '2021-04-23T15:40:09.595465Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 15:40:09.587 ip-10-0-246-154.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192159\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.615616', 'end_time': '2021-04-23T15:40:09.826405', 'duration': 0.210789, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.637864Z', 'iopub.execute_input': '2021-04-23T15:40:09.638361Z', 'shell.execute_reply': '2021-04-23T15:40:09.825859Z', 'iopub.status.idle': '2021-04-23T15:40:09.826292Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n  \r\n",
      "    if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.845709', 'end_time': '2021-04-23T15:40:09.873317', 'duration': 0.027608, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.870825Z', 'iopub.execute_input': '2021-04-23T15:40:09.871321Z', 'shell.execute_reply': '2021-04-23T15:40:09.872776Z', 'iopub.status.idle': '2021-04-23T15:40:09.873212Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.892317', 'end_time': '2021-04-23T15:40:09.916795', 'duration': 0.024478, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.914314Z', 'iopub.execute_input': '2021-04-23T15:40:09.914788Z', 'iopub.status.idle': '2021-04-23T15:40:09.916279Z', 'shell.execute_reply': '2021-04-23T15:40:09.916672Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.935568', 'end_time': '2021-04-23T15:40:09.954377', 'duration': 0.018809, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:09.973490', 'end_time': '2021-04-23T15:40:09.998168', 'duration': 0.024678, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:09.995695Z', 'iopub.execute_input': '2021-04-23T15:40:09.996204Z', 'iopub.status.idle': '2021-04-23T15:40:09.997664Z', 'shell.execute_reply': '2021-04-23T15:40:09.998045Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.017003', 'end_time': '2021-04-23T15:40:10.048952', 'duration': 0.031949, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.044962Z', 'iopub.execute_input': '2021-04-23T15:40:10.046541Z', 'iopub.status.idle': '2021-04-23T15:40:10.048446Z', 'shell.execute_reply': '2021-04-23T15:40:10.048830Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.068080', 'end_time': '2021-04-23T15:40:10.110759', 'duration': 0.042679, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.098238Z', 'iopub.execute_input': '2021-04-23T15:40:10.100925Z', 'iopub.status.idle': '2021-04-23T15:40:10.110255Z', 'shell.execute_reply': '2021-04-23T15:40:10.110636Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8bc7afec-ae63-4f60-ba78-d1051dd2f16b\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"057780ab-9db9-40ba-b59a-0b8fc04f71de\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"StringEditor\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"editor\":{\"id\":\"1011\"},\"field\":\"1\",\"formatter\":{\"id\":\"1012\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"Selection\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"15:38:22 04/23/2021\",\"15:38:59 04/23/2021\",\"37 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1014\"},\"selection_policy\":{\"id\":\"1013\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 15:38:22 and ran for 37 seconds. \\\\n Your training job started on 04/23/2021 at 15:38:22 and ran for 37 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"editor\":{\"id\":\"1009\"},\"field\":\"0\",\"formatter\":{\"id\":\"1010\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"057780ab-9db9-40ba-b59a-0b8fc04f71de\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"8bc7afec-ae63-4f60-ba78-d1051dd2f16b\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.130996', 'end_time': '2021-04-23T15:40:10.150906', 'duration': 0.01991, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.171019', 'end_time': '2021-04-23T15:40:10.196944', 'duration': 0.025925, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.194272Z', 'iopub.execute_input': '2021-04-23T15:40:10.194804Z', 'iopub.status.idle': '2021-04-23T15:40:10.196435Z', 'shell.execute_reply': '2021-04-23T15:40:10.196816Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-in\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m put'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.217275', 'end_time': '2021-04-23T15:40:10.246464', 'duration': 0.029189, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.244124Z', 'iopub.execute_input': '2021-04-23T15:40:10.244642Z', 'shell.execute_reply': '2021-04-23T15:40:10.245957Z', 'iopub.status.idle': '2021-04-23T15:40:10.246365Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.266645', 'end_time': '2021-04-23T15:40:10.313629', 'duration': 0.046984, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.294982Z', 'iopub.execute_input': '2021-04-23T15:40:10.300937Z', 'shell.execute_reply': '2021-04-23T15:40:10.313123Z', 'iopub.status.idle': '2021-04-23T15:40:10.313530Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"f877555e-9296-4db6-8a03-f79fb3c25b39\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"937b028f-8cee-40be-9f27-4bff3db82bd9\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQKRwPQrX4zJAAAAAAADAWEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAB7FK5H4fpJQHsUrkfh+ihAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAADhehSuR8FYQOF6FK5HITFAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQKRwPQrXozJAhetRuB4VWEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQFyPwvUo3DJA4XoUrkeRWEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1083\"},\"selection_policy\":{\"id\":\"1082\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringEditor\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{\"editor\":{\"id\":\"1072\"},\"field\":\"max\",\"formatter\":{\"id\":\"1073\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"Selection\"},{\"attributes\":{\"editor\":{\"id\":\"1074\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1075\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1078\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1079\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringEditor\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{\"editor\":{\"id\":\"1070\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1071\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1066\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1067\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1068\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1069\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringFormatter\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1076\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1077\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"StringEditor\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{\"editor\":{\"id\":\"1080\"},\"field\":\"min\",\"formatter\":{\"id\":\"1081\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"937b028f-8cee-40be-9f27-4bff3db82bd9\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"f877555e-9296-4db6-8a03-f79fb3c25b39\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.334822', 'end_time': '2021-04-23T15:40:10.375654', 'duration': 0.040832, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.369733Z', 'iopub.execute_input': '2021-04-23T15:40:10.372499Z', 'iopub.status.idle': '2021-04-23T15:40:10.375147Z', 'shell.execute_reply': '2021-04-23T15:40:10.375529Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.397577', 'end_time': '2021-04-23T15:40:10.428674', 'duration': 0.031097, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.426377Z', 'iopub.execute_input': '2021-04-23T15:40:10.426877Z', 'iopub.status.idle': '2021-04-23T15:40:10.428168Z', 'shell.execute_reply': '2021-04-23T1\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m 5:40:10.428554Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.450468', 'end_time': '2021-04-23T15:40:10.481740', 'duration': 0.031272, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.479387Z', 'iopub.execute_input': '2021-04-23T15:40:10.479888Z', 'iopub.status.idle': '2021-04-23T15:40:10.481237Z', 'shell.execute_reply': '2021-04-23T15:40:10.481621Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.503890', 'end_time': '2021-04-23T15:40:10.525802', 'duration': 0.021912, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.547715', 'end_time': '2021-04-23T15:40:10.576794', 'duration': 0.029079, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.574436Z', 'iopub.execute_input': '2021-04-23T15:40:10.574961Z', 'iopub.status.idle': '2021-04-23T15:40:10.576276Z', 'shell.execute_reply': '2021-04-23T15:40:10.576666Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.598876', 'end_time': '2021-04-23T15:40:10.627125', 'duration': 0.028249, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.624833Z', 'iopub.execute_input': '2021-04-23T15:40:10.625337Z', 'shell.execute_reply': '2021-04-23T15:40:10.626598Z', 'iopub.status.idle': '2021-04-23T15:40:10.627019Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.649452', 'end_time': '2021-04-23T15:40:10.688544', 'duration': 0.039092, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.677206Z', 'iopub.execute_input': '2021-04-23T15:40:10.677699Z', 'iopub.status.idle': '2021-04-23T15:40:10.688000Z', 'shell.execute_reply': '2021-04-23T15:40:10.688415Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the GPUMemoryIncrease rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>82</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>74</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>82</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\']\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m \\n    summary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.711986', 'end_time': '2021-04-23T15:40:10.743129', 'duration': 0.031143, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.740163Z', 'iopub.execute_input': '2021-04-23T15:40:10.740688Z', 'iopub.status.idle': '2021-04-23T15:40:10.742625Z', 'shell.execute_reply': '2021-04-23T15:40:10.743007Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.767152', 'end_time': '2021-04-23T15:40:10.795931', 'duration': 0.028779, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.793679Z', 'iopub.execute_input': '2021-04-23T15:40:10.794152Z', 'iopub.status.idle': '2021-04-23T15:40:10.795435Z', 'shell.execute_reply': '2021-04-23T15:40:10.795814Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.819517', 'end_time': '2021-04-23T15:40:10.872469', 'duration': 0.052952, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.860778Z', 'iopub.execute_input': '2021-04-23T15:40:10.864521Z', 'shell.execute_reply': '2021-04-23T15:40:10.871851Z', 'iopub.status.idle': '2021-04-23T15:40:10.872362Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d0e537ad-26b9-4ac0-a796-b830d855ccbb\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"70b59640-499e-458f-9415-d666b8b9ab67\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"70b59640-499e-458f-9415-d666b8b9ab67\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"d0e537ad-26b9-4ac0-a796-b830d855ccbb\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.897686', 'end_time': '2021-04-23T15:40:10.946074', 'duration': 0.048388, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:10.937615Z', 'iopub.execute_input': '2021-04-23T15:40:10.938178Z', 'iopub.status.idle': '2021-04-23T15:40:10.945562Z', 'shell.execute_reply': '20\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m 21-04-23T15:40:10.945944Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"54d43c04-cebe-441b-baf4-af5ac3bdfc87\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"feb1965c-1067-41c1-a260-da4375d0b804\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"feb1965c-1067-41c1-a260-da4375d0b804\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"54d43c04-cebe-441b-baf4-af5ac3bdfc87\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:10.973217', 'end_time': '2021-04-23T15:40:11.018504', 'duration': 0.045287, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:11.008947Z', 'iopub.execute_input': '2021-04-23T15:40:11.010986Z', 'iopub.status.idle': '2021-04-23T15:40:11.017996Z', 'shell.execute_reply': '2021-04-23T15:40:11.018377Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"eb29f165-3623-4fdb-b3b8-072b07110fe0\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"69fd7a72-1d1c-4439-babc-5fafecee1530\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"69fd7a72-1d1c-4439-babc-5fafecee1530\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"eb29f165-3623-4fdb-b3b8-072b07110fe0\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:11.047044', 'end_time': '2021-04-23T15:40:11.098199', 'duration': 0.051155, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:11.085772Z', 'iopub.execute_input': '2021-04-23T15:40:11.086324Z', 'shell.execute_reply': '2021-04-23T15:40:11.097593Z', 'iopub.status.idle': '2021-04-23T15:40:11.098096Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"4c2c5977-183c-4d66-b3db-b9f4c4a3443b\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"65c05f11-85a3-4b51-bd69-ca36c9e095d1\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 time\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m s.\",\"width\":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"65c05f11-85a3-4b51-bd69-ca36c9e095d1\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"4c2c5977-183c-4d66-b3db-b9f4c4a3443b\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"b2229794-9027-49e1-a611-f874a11956f7\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"cdd6531b-a193-406c-b80b-50dbc926ce8a\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"cdd6531b-a193-406c-b80b-50dbc926ce8a\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"b2229794-9027-49e1-a611-f874a11956f7\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:11.128259', 'end_time': '2021-04-23T15:40:11.178153', 'duration': 0.049894, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:11.168347Z', 'iopub.execute_input': '2021-04-23T15:40:11.170654Z', 'shell.execute_reply': '2021-04-23T15:40:11.177577Z', 'iopub.status.idle': '2021-04-23T15:40:11.178053Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"2affab93-a358-4566-a747-57196ba0a798\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"15667407-f012-48bb-89ba-9fe94ee3fc92\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 74 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"15667407-f012-48bb-89ba-9fe94ee3fc92\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"2affab93-a358-4566-a747-57196ba0a798\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:11.209649', 'end_time': '2021-04-23T15:40:11.268159', 'duration': 0.05851, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:11.253399Z', 'iopub.execute_input': '2021-04-23T15:40:11.259553Z', 'shell.execute_reply': '2021-04-23T15:40:11.267592Z', 'iopub.status.idle': '2021-04-23T15:40:11.268009Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"145e9976-3ea1-4941-a90b-14e8378a2062\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"aaef8a8a-dfd1-4ddc-8016-2121c086906f\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 82 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"aaef8a8a-dfd1-4ddc-8016-2121c086906f\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"145e9976-3ea1-4941-a90b-14e8378a2062\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n     \r\n",
      "   if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:11.300945', 'end_time': '2021-04-23T15:40:11.361867', 'duration': 0.060922, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:11.352441Z', 'iopub.execute_input': '2021-04-23T15:40:11.353365Z', 'iopub.status.idle': '2021-04-23T15:40:11.361120Z', 'shell.execute_reply': '2021-04-23T15:40:11.361722Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"a95d52ff-dc09-4c64-9358-fde716bd6534\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"b5f020bf-4c5e-4f83-97a0-f6115a15af57\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 82 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"b5f020bf-4c5e-4f83-97a0-f6115a15af57\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"a95d52ff-dc09-4c64-9358-fde716bd6534\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n\r\n",
      "                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:11.396183', 'end_time': '2021-04-23T15:40:11.453588', 'duration': 0.057405, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:11.444294Z', 'iopub.execute_input': '2021-04-23T15:40:11.445130Z', 'shell.execute_reply': '2021-04-23T15:40:11.452991Z', 'iopub.status.idle': '2021-04-23T15:40:11.453485Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"9b985625-36cd-457d-9f56-d1bcaa1f2710\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"099df3a8-f867-4d08-9d27-93f9c2246ae6\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"099df3a8-f867-4d08-9d27-93f9c2246ae6\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"9b985625-36cd-457d-9f56-d1bcaa1f2710\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T15:40:07.912222', 'end_time': '2021-04-23T15:40:11.795053', 'duration': 3.882831, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:11.867 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:11.868 ip-10-0-246-154.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:12.282 ip-10-0-246-154.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m [2021-04-23 15:40:12.282 ip-10-0-246-154.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-IGYOEWLLKC-ProfilerReport-1619192159-fc3f96b2/algo-1-1619192360\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:25,  1.14cell/s]#015Executing:   7%|         | 2/30 [00:01<00:23,  1.18cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.77cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.56cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.23cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01,  9.98cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 11.06cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.54cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.73cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 15.27cell/s]#015Executing:  77%|  | 23/30 [00:02<00:00, 15.29cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.70cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 13.92cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.81cell/s]#015Executing: 100%|| 30/30 [00:03<00:00,  7.73cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:14.842 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:11.586364', 'end_time': '2021-04-23T15:40:11.605565', 'duration': 0.019201, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:11.623430', 'end_time': '2021-04-23T15:40:12.436132', 'duration': 0.812702, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:11.648782Z', 'iopub.execute_input': '2021-04-23T15:40:11.649290Z', 'shell.execute_reply': '2021-04-23T15:40:12.435431Z', 'iopub.status.idle': '2021-04-23T15:40:12.436007Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 15:40:12.428 ip-10-2-234-68.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192149\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:12.456342', 'end_time': '2021-04-23T15:40:12.668478', 'duration': 0.212136, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:12.478747Z', 'iopub.execute_input': '2021-04-23T15:40:12.479238Z', 'shell.execute_reply': '2021-04-23T15:40:12.667928Z', 'iopub.status.idle': '2021-04-23T15:40:12.668364Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n    \r\n",
      "  if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:12.687998', 'end_time': '2021-04-23T15:40:12.715918', 'duration': 0.02792, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:12.713716Z', 'iopub.execute_input': '2021-04-23T15:40:12.714215Z', 'iopub.status.idle': '2021-04-23T15:40:12.715305Z', 'shell.execute_reply': '2021-04-23T15:40:12.715784Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:12.735059', 'end_time': '2021-04-23T15:40:12.760032', 'duration': 0.024973, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:12.757701Z', 'iopub.execute_input': '2021-04-23T15:40:12.758180Z', 'shell.execute_reply': '2021-04-23T15:40:12.759407Z', 'iopub.status.idle': '2021-04-23T15:40:12.759927Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:12.779092', 'end_time': '2021-04-23T15:40:12.798073', 'duration': 0.018981, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:12.817555', 'end_time': '2021-04-23T15:40:12.842775', 'duration': 0.02522, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:12.840429Z', 'iopub.execute_input': '2021-04-23T15:40:12.840916Z', 'shell.execute_reply': '2021-04-23T15:40:12.842237Z', 'iopub.status.idle': '2021-04-23T15:40:12.842657Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:12.861849', 'end_time': '2021-04-23T15:40:12.894456', 'duration': 0.032607, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:12.890520Z', 'iopub.execute_input': '2021-04-23T15:40:12.891034Z', 'shell.execute_reply': '2021-04-23T15:40:12.893937Z', 'iopub.status.idle': '2021-04-23T15:40:12.894351Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:12.913909', 'end_time': '2021-04-23T15:40:12.956931', 'duration': 0.043022, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:12.940748Z', 'iopub.execute_input': '2021-04-23T15:40:12.947133Z', 'iopub.status.idle': '2021-04-23T15:40:12.956417Z', 'shell.execute_reply': '2021-04-23T15:40:12.956801Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"2eaae132-5d88-45fb-b74b-3372b2a38868\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"9dda1bde-f5a1-4a52-93af-64fc2adc2185\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"15:38:42 04/23/2021\",\"15:38:59 04/23/2021\",\"17 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1014\"},\"selection_policy\":{\"id\":\"1013\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringEditor\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 15:38:42 and ran for 17 seconds. \\\\n Your training job started on 04/23/2021 at 15:38:42 and ran for 17 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"editor\":{\"id\":\"1011\"},\"field\":\"1\",\"formatter\":{\"id\":\"1012\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{\"editor\":{\"id\":\"1009\"},\"field\":\"0\",\"formatter\":{\"id\":\"1010\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"9dda1bde-f5a1-4a52-93af-64fc2adc2185\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"2eaae132-5d88-45fb-b74b-3372b2a38868\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:12.977341', 'end_time': '2021-04-23T15:40:12.997722', 'duration': 0.020381, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.017991', 'end_time': '2021-04-23T15:40:13.044063', 'duration': 0.026072, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.041420Z', 'iopub.execute_input': '2021-04-23T15:40:13.041913Z', 'iopub.status.idle': '2021-04-23T15:40:13.043490Z', 'shell.execute_reply': '2021-04-23T15:40:13.043925Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-input\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m '], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.064400', 'end_time': '2021-04-23T15:40:13.094322', 'duration': 0.029922, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.092030Z', 'iopub.execute_input': '2021-04-23T15:40:13.092543Z', 'iopub.status.idle': '2021-04-23T15:40:13.093791Z', 'shell.execute_reply': '2021-04-23T15:40:13.094190Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.114875', 'end_time': '2021-04-23T15:40:13.162513', 'duration': 0.047638, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.143999Z', 'iopub.execute_input': '2021-04-23T15:40:13.144520Z', 'shell.execute_reply': '2021-04-23T15:40:13.161971Z', 'iopub.status.idle': '2021-04-23T15:40:13.162402Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"0c02af86-809b-4fb1-aaae-37cccf5ec793\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"fce05c73-2268-4354-ba79-ac51a50c31bb\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1066\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1067\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1078\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1079\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringEditor\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{\"editor\":{\"id\":\"1074\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1075\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringFormatter\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"StringFormatter\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1076\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1077\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQOxRuB6F6zFAuB6F61H4TEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAIBYQHsUrkfh+ipAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQOxRuB6F6y9AAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQGZmZmZm5jFAexSuR+G6NEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQOxRuB6F6zFA4XoUrkdBSUA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1083\"},\"selection_policy\":{\"id\":\"1082\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringEditor\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{\"editor\":{\"id\":\"1068\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1069\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1080\"},\"field\":\"min\",\"formatter\":{\"id\":\"1081\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"Selection\"},{\"attributes\":{\"editor\":{\"id\":\"1072\"},\"field\":\"max\",\"formatter\":{\"id\":\"1073\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringFormatter\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1070\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1071\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"fce05c73-2268-4354-ba79-ac51a50c31bb\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"0c02af86-809b-4fb1-aaae-37cccf5ec793\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.184004', 'end_time': '2021-04-23T15:40:13.225049', 'duration': 0.041045, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.207439Z', 'iopub.execute_input': '2021-04-23T15:40:13.219538Z', 'shell.execute_reply': '2021-04-23T15:40:13.224527Z', 'iopub.status.idle': '2021-04-23T15:40:13.224945Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.247182', 'end_time': '2021-04-23T15:40:13.278899', 'duration': 0.031717, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.276149Z', 'iopub.execute_input': '2021-04-23T15:40:13.276675Z', 'iopub.status.idle': '2021-04-23T15:40:13.278397Z', 'shell.execute_reply': '2021-04-23T15:4\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m 0:13.278777Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.300881', 'end_time': '2021-04-23T15:40:13.332448', 'duration': 0.031567, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.330089Z', 'iopub.execute_input': '2021-04-23T15:40:13.330581Z', 'iopub.status.idle': '2021-04-23T15:40:13.331951Z', 'shell.execute_reply': '2021-04-23T15:40:13.332330Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.354610', 'end_time': '2021-04-23T15:40:13.376593', 'duration': 0.021983, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.399046', 'end_time': '2021-04-23T15:40:13.428677', 'duration': 0.029631, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.423953Z', 'iopub.execute_input': '2021-04-23T15:40:13.426337Z', 'shell.execute_reply': '2021-04-23T15:40:13.428146Z', 'iopub.status.idle': '2021-04-23T15:40:13.428571Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.450878', 'end_time': '2021-04-23T15:40:13.479725', 'duration': 0.028847, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.477470Z', 'iopub.execute_input': '2021-04-23T15:40:13.478005Z', 'iopub.status.idle': '2021-04-23T15:40:13.479188Z', 'shell.execute_reply': '2021-04-23T15:40:13.479598Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.502271', 'end_time': '2021-04-23T15:40:13.541621', 'duration': 0.03935, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.530467Z', 'iopub.execute_input': '2021-04-23T15:40:13.530981Z', 'shell.execute_reply': '2021-04-23T15:40:13.541025Z', 'iopub.status.idle': '2021-04-23T15:40:13.541514Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the StepOutlier rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>35</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>38</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>38</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n    summ\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m ary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.565195', 'end_time': '2021-04-23T15:40:13.596634', 'duration': 0.031439, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.593633Z', 'iopub.execute_input': '2021-04-23T15:40:13.594143Z', 'shell.execute_reply': '2021-04-23T15:40:13.596119Z', 'iopub.status.idle': '2021-04-23T15:40:13.596536Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.620822', 'end_time': '2021-04-23T15:40:13.650817', 'duration': 0.029995, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.648500Z', 'iopub.execute_input': '2021-04-23T15:40:13.649114Z', 'iopub.status.idle': '2021-04-23T15:40:13.650292Z', 'shell.execute_reply': '2021-04-23T15:40:13.650687Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.674917', 'end_time': '2021-04-23T15:40:13.732147', 'duration': 0.05723, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.718825Z', 'iopub.execute_input': '2021-04-23T15:40:13.723097Z', 'iopub.status.idle': '2021-04-23T15:40:13.731602Z', 'shell.execute_reply': '2021-04-23T15:40:13.732006Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"bc1f1ec0-06c5-4df3-aaa0-cdf73f44d47a\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"ab3ce451-bfc0-4c0c-b3d6-13a0d21a09c8\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"ab3ce451-bfc0-4c0c-b3d6-13a0d21a09c8\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"bc1f1ec0-06c5-4df3-aaa0-cdf73f44d47a\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.757833', 'end_time': '2021-04-23T15:40:13.808940', 'duration': 0.051107, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.795828Z', 'iopub.execute_input': '2021-04-23T15:40:13.798908Z', 'iopub.status.idle': '2021-04-23T15:40:13.808413Z', 'shell.execute_reply': '2021-04-23T15\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m :40:13.808805Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"7929b2b0-9536-47b0-854d-24ebd51a646f\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"59678393-3fc3-4eab-97a9-eafead45f7fb\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"59678393-3fc3-4eab-97a9-eafead45f7fb\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"7929b2b0-9536-47b0-854d-24ebd51a646f\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.836263', 'end_time': '2021-04-23T15:40:13.884102', 'duration': 0.047839, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.872815Z', 'iopub.execute_input': '2021-04-23T15:40:13.875257Z', 'iopub.status.idle': '2021-04-23T15:40:13.883539Z', 'shell.execute_reply': '2021-04-23T15:40:13.883963Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"ee214a6b-964e-4dac-b126-e552a7fa2bfd\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"4c5b4f01-2750-4e7c-a929-4cb0b65a8b52\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"4c5b4f01-2750-4e7c-a929-4cb0b65a8b52\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"ee214a6b-964e-4dac-b126-e552a7fa2bfd\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.912848', 'end_time': '2021-04-23T15:40:13.966920', 'duration': 0.054072, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.952107Z', 'iopub.execute_input': '2021-04-23T15:40:13.952950Z', 'shell.execute_reply': '2021-04-23T15:40:13.966386Z', 'iopub.status.idle': '2021-04-23T15:40:13.966810Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"ca1dd535-9486-4d36-a686-0764585b173f\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"a06a51f1-60db-43b6-adca-670bb6e17f8a\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\"\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m :900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"a06a51f1-60db-43b6-adca-670bb6e17f8a\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"ca1dd535-9486-4d36-a686-0764585b173f\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"f28f102f-c182-48dc-b33b-26216bf2a0ee\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"4712768a-dffd-4d10-ab3b-a53e2ae59d1b\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"4712768a-dffd-4d10-ab3b-a53e2ae59d1b\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"f28f102f-c182-48dc-b33b-26216bf2a0ee\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.997282', 'end_time': '2021-04-23T15:40:14.050094', 'duration': 0.052812, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:14.040469Z', 'iopub.execute_input': '2021-04-23T15:40:14.041300Z', 'shell.execute_reply': '2021-04-23T15:40:14.049553Z', 'iopub.status.idle': '2021-04-23T15:40:14.049983Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"a222c0e3-fd9f-4002-8d05-3adc158d25d7\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"5e080ef7-aa80-41cd-be4d-9f7a9e512d16\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 35 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"5e080ef7-aa80-41cd-be4d-9f7a9e512d16\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"a222c0e3-fd9f-4002-8d05-3adc158d25d7\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:14.082108', 'end_time': '2021-04-23T15:40:14.141820', 'duration': 0.059712, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:14.116126Z', 'iopub.execute_input': '2021-04-23T15:40:14.132522Z', 'shell.execute_reply': '2021-04-23T15:40:14.141288Z', 'iopub.status.idle': '2021-04-23T15:40:14.141712Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"9e93ff0c-1fe7-43f9-8aa1-7655710475a2\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"fadf05dd-c734-4e58-9849-db5bfbac4812\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 38 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"fadf05dd-c734-4e58-9849-db5bfbac4812\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"9e93ff0c-1fe7-43f9-8aa1-7655710475a2\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (att\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m empts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:14.174708', 'end_time': '2021-04-23T15:40:14.235591', 'duration': 0.060883, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:14.225993Z', 'iopub.execute_input': '2021-04-23T15:40:14.226844Z', 'iopub.status.idle': '2021-04-23T15:40:14.235046Z', 'shell.execute_reply': '2021-04-23T15:40:14.235437Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"b85a380b-e360-4b3f-b4da-ba542a37cb3f\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"deba9f10-8a51-444b-bc23-7194de57a7a1\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 38 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"deba9f10-8a51-444b-bc23-7194de57a7a1\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"b85a380b-e360-4b3f-b4da-ba542a37cb3f\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n          \r\n",
      "                                  height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:14.269993', 'end_time': '2021-04-23T15:40:14.328132', 'duration': 0.058139, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:14.316445Z', 'iopub.execute_input': '2021-04-23T15:40:14.319519Z', 'iopub.status.idle': '2021-04-23T15:40:14.327596Z', 'shell.execute_reply': '2021-04-23T15:40:14.327999Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"3e0c5d20-e1ea-48bf-a504-a890c11f4700\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"e045dc67-fe2f-4f30-8a9e-f978ac6a4a14\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"e045dc67-fe2f-4f30-8a9e-f978ac6a4a14\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"3e0c5d20-e1ea-48bf-a504-a890c11f4700\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T15:40:10.737196', 'end_time': '2021-04-23T15:40:14.769627', 'duration': 4.032431, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:14.843 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:14.843 ip-10-2-234-68.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:15.261 ip-10-2-234-68.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m [2021-04-23 15:40:15.261 ip-10-2-234-68.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:25,  1.13cell/s]#015Executing:   7%|         | 2/30 [00:01<00:23,  1.17cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.75cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.52cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.16cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01,  9.88cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 10.94cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.41cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.56cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 15.11cell/s]#015Executing:  77%|  | 23/30 [00:03<00:00, 14.97cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.32cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 13.50cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.52cell/s]#015Executing: 100%|| 30/30 [00:04<00:00,  7.44cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-UUMPHCNNKS-ProfilerReport-1619192149-2b7264b6/algo-1-1619192357\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:15.051 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:11.774604', 'end_time': '2021-04-23T15:40:11.793867', 'duration': 0.019263, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:11.813558', 'end_time': '2021-04-23T15:40:12.630556', 'duration': 0.816998, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:11.838358Z', 'iopub.execute_input': '2021-04-23T15:40:11.838881Z', 'iopub.status.idle': '2021-04-23T15:40:12.629903Z', 'shell.execute_reply': '2021-04-23T15:40:12.630323Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 15:40:12.622 ip-10-0-109-143.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192154\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:12.650096', 'end_time': '2021-04-23T15:40:12.863575', 'duration': 0.213479, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:12.673271Z', 'iopub.execute_input': '2021-04-23T15:40:12.673773Z', 'iopub.status.idle': '2021-04-23T15:40:12.863060Z', 'shell.execute_reply': '2021-04-23T15:40:12.863427Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n  \r\n",
      "    if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:12.883106', 'end_time': '2021-04-23T15:40:12.911054', 'duration': 0.027948, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:12.908770Z', 'iopub.execute_input': '2021-04-23T15:40:12.909292Z', 'iopub.status.idle': '2021-04-23T15:40:12.910539Z', 'shell.execute_reply': '2021-04-23T15:40:12.910920Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:12.930409', 'end_time': '2021-04-23T15:40:12.955340', 'duration': 0.024931, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:12.953120Z', 'iopub.execute_input': '2021-04-23T15:40:12.953636Z', 'iopub.status.idle': '2021-04-23T15:40:12.954824Z', 'shell.execute_reply': '2021-04-23T15:40:12.955208Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:12.974550', 'end_time': '2021-04-23T15:40:12.993788', 'duration': 0.019238, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.015324', 'end_time': '2021-04-23T15:40:13.040551', 'duration': 0.025227, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.038333Z', 'iopub.execute_input': '2021-04-23T15:40:13.038814Z', 'iopub.status.idle': '2021-04-23T15:40:13.040019Z', 'shell.execute_reply': '2021-04-23T15:40:13.040423Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.059781', 'end_time': '2021-04-23T15:40:13.092303', 'duration': 0.032522, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.088438Z', 'iopub.execute_input': '2021-04-23T15:40:13.088970Z', 'iopub.status.idle': '2021-04-23T15:40:13.091767Z', 'shell.execute_reply': '2021-04-23T15:40:13.092170Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.111768', 'end_time': '2021-04-23T15:40:13.154951', 'duration': 0.043183, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.138931Z', 'iopub.execute_input': '2021-04-23T15:40:13.145275Z', 'iopub.status.idle': '2021-04-23T15:40:13.154434Z', 'shell.execute_reply': '2021-04-23T15:40:13.154816Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"518f7c39-1834-4390-ab93-d417b33ebb6c\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"d61e9ccc-adb1-434c-ad60-7280fbb12701\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"StringFormatter\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{\"editor\":{\"id\":\"1012\"},\"field\":\"0\",\"formatter\":{\"id\":\"1011\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 15:38:18 and ran for 41 seconds. \\\\n Your training job started on 04/23/2021 at 15:38:18 and ran for 41 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"StringEditor\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"15:38:18 04/23/2021\",\"15:38:59 04/23/2021\",\"41 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1010\"},\"selection_policy\":{\"id\":\"1009\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{\"editor\":{\"id\":\"1014\"},\"field\":\"1\",\"formatter\":{\"id\":\"1013\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringFormatter\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"d61e9ccc-adb1-434c-ad60-7280fbb12701\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"518f7c39-1834-4390-ab93-d417b33ebb6c\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.175592', 'end_time': '2021-04-23T15:40:13.196219', 'duration': 0.020627, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.216598', 'end_time': '2021-04-23T15:40:13.242665', 'duration': 0.026067, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.240089Z', 'iopub.execute_input': '2021-04-23T15:40:13.240605Z', 'iopub.status.idle': '2021-04-23T15:40:13.242161Z', 'shell.execute_reply': '2021-04-23T15:40:13.242539Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-i\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m nput'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.263352', 'end_time': '2021-04-23T15:40:13.294134', 'duration': 0.030782, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.291875Z', 'iopub.execute_input': '2021-04-23T15:40:13.292436Z', 'shell.execute_reply': '2021-04-23T15:40:13.293547Z', 'iopub.status.idle': '2021-04-23T15:40:13.294028Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.315064', 'end_time': '2021-04-23T15:40:13.363322', 'duration': 0.048258, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.344876Z', 'iopub.execute_input': '2021-04-23T15:40:13.359578Z', 'shell.execute_reply': '2021-04-23T15:40:13.362799Z', 'iopub.status.idle': '2021-04-23T15:40:13.363214Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"29587880-d393-4dfc-adf6-e5d344e4bdc8\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"febf7489-1f03-4678-b23f-1a86f94a3d5e\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1079\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1078\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1075\"},\"field\":\"max\",\"formatter\":{\"id\":\"1074\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1083\"},\"field\":\"min\",\"formatter\":{\"id\":\"1082\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1073\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1072\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"Selection\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringEditor\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{\"editor\":{\"id\":\"1069\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1068\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringEditor\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringEditor\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1077\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1076\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1081\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1080\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringEditor\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{\"editor\":{\"id\":\"1071\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1070\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringFormatter\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQFyPwvUo3DJAPQrXo3D9V0A=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAIBNQAAAAAAAACpAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAEBYQEjhehSuBzFApHA9Ctej8D8=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQEjhehSuhzJA16NwPQo3UUA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQMP1KFyPwjJAXI/C9Sj8VkA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1067\"},\"selection_policy\":{\"id\":\"1066\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"febf7489-1f03-4678-b23f-1a86f94a3d5e\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"29587880-d393-4dfc-adf6-e5d344e4bdc8\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.385041', 'end_time': '2021-04-23T15:40:13.426613', 'duration': 0.041572, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.420960Z', 'iopub.execute_input': '2021-04-23T15:40:13.423560Z', 'shell.execute_reply': '2021-04-23T15:40:13.426018Z', 'iopub.status.idle': '2021-04-23T15:40:13.426506Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.449033', 'end_time': '2021-04-23T15:40:13.481826', 'duration': 0.032793, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.479540Z', 'iopub.execute_input': '2021-04-23T15:40:13.480054Z', 'iopub.status.idle': '2021-04-23T15:40:13.481315Z', 'shell.execute_reply': '2021-04-23T\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m 15:40:13.481695Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.504207', 'end_time': '2021-04-23T15:40:13.537117', 'duration': 0.03291, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.534864Z', 'iopub.execute_input': '2021-04-23T15:40:13.535382Z', 'iopub.status.idle': '2021-04-23T15:40:13.536589Z', 'shell.execute_reply': '2021-04-23T15:40:13.536970Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.559830', 'end_time': '2021-04-23T15:40:13.582441', 'duration': 0.022611, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.604747', 'end_time': '2021-04-23T15:40:13.634193', 'duration': 0.029446, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.631678Z', 'iopub.execute_input': '2021-04-23T15:40:13.632210Z', 'iopub.status.idle': '2021-04-23T15:40:13.633668Z', 'shell.execute_reply': '2021-04-23T15:40:13.634059Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.656719', 'end_time': '2021-04-23T15:40:13.686208', 'duration': 0.029489, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.683676Z', 'iopub.execute_input': '2021-04-23T15:40:13.684316Z', 'iopub.status.idle': '2021-04-23T15:40:13.685694Z', 'shell.execute_reply': '2021-04-23T15:40:13.686073Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.708888', 'end_time': '2021-04-23T15:40:13.748405', 'duration': 0.039517, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.737141Z', 'iopub.execute_input': '2021-04-23T15:40:13.737663Z', 'shell.execute_reply': '2021-04-23T15:40:13.747747Z', 'iopub.status.idle': '2021-04-23T15:40:13.748296Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the LowGPUUtilization rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>92</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>82</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>92</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\']\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m \\n    summary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.772230', 'end_time': '2021-04-23T15:40:13.803843', 'duration': 0.031613, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.800860Z', 'iopub.execute_input': '2021-04-23T15:40:13.801418Z', 'iopub.status.idle': '2021-04-23T15:40:13.803332Z', 'shell.execute_reply': '2021-04-23T15:40:13.803713Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.828362', 'end_time': '2021-04-23T15:40:13.858221', 'duration': 0.029859, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.855927Z', 'iopub.execute_input': '2021-04-23T15:40:13.856476Z', 'shell.execute_reply': '2021-04-23T15:40:13.857624Z', 'iopub.status.idle': '2021-04-23T15:40:13.858114Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.882717', 'end_time': '2021-04-23T15:40:13.937008', 'duration': 0.054291, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:13.925190Z', 'iopub.execute_input': '2021-04-23T15:40:13.928897Z', 'iopub.status.idle': '2021-04-23T15:40:13.936461Z', 'shell.execute_reply': '2021-04-23T15:40:13.936865Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"fb1792d9-3ac6-4154-9e4d-ccb8a19f8b30\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"2577eba8-f6e8-43f6-8f01-001198417a15\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"2577eba8-f6e8-43f6-8f01-001198417a15\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"fb1792d9-3ac6-4154-9e4d-ccb8a19f8b30\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:13.963063', 'end_time': '2021-04-23T15:40:14.012378', 'duration': 0.049315, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:14.003413Z', 'iopub.execute_input': '2021-04-23T15:40:14.003976Z', 'shell.execute_reply': '2021-04-23T15:40:14.011812Z', 'iopub.status.idle': '20\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m 21-04-23T15:40:14.012263Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"a0cc7e0e-31fd-47eb-a9e4-8d22123347ec\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"0b915e97-ccb7-4c7f-be4a-297a5bb2186c\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"0b915e97-ccb7-4c7f-be4a-297a5bb2186c\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"a0cc7e0e-31fd-47eb-a9e4-8d22123347ec\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:14.039965', 'end_time': '2021-04-23T15:40:14.086041', 'duration': 0.046076, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:14.077803Z', 'iopub.execute_input': '2021-04-23T15:40:14.078335Z', 'iopub.status.idle': '2021-04-23T15:40:14.085523Z', 'shell.execute_reply': '2021-04-23T15:40:14.085904Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"db37969c-ff1a-47fc-9bd6-8a6bad521685\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"7f081f2e-c19b-4085-b213-05f843dea5d2\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"7f081f2e-c19b-4085-b213-05f843dea5d2\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"db37969c-ff1a-47fc-9bd6-8a6bad521685\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:14.114999', 'end_time': '2021-04-23T15:40:14.167335', 'duration': 0.052336, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:14.154561Z', 'iopub.execute_input': '2021-04-23T15:40:14.155095Z', 'shell.execute_reply': '2021-04-23T15:40:14.166809Z', 'iopub.status.idle': '2021-04-23T15:40:14.167226Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"74d71405-d714-4587-892b-a7b3970cfc19\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"f6b0ac85-6abe-49ff-8672-cff4a6df26eb\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 time\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m s.\",\"width\":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"f6b0ac85-6abe-49ff-8672-cff4a6df26eb\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"74d71405-d714-4587-892b-a7b3970cfc19\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"4d5f8249-7d84-4f3d-9644-475b5a7ac8f7\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"943f9ede-5660-4865-8cb4-8ff47e1c3b34\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"943f9ede-5660-4865-8cb4-8ff47e1c3b34\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"4d5f8249-7d84-4f3d-9644-475b5a7ac8f7\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:14.198747', 'end_time': '2021-04-23T15:40:14.251177', 'duration': 0.05243, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:14.242615Z', 'iopub.execute_input': '2021-04-23T15:40:14.243201Z', 'shell.execute_reply': '2021-04-23T15:40:14.250642Z', 'iopub.status.idle': '2021-04-23T15:40:14.251063Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"afb0f064-07dd-43f7-b9c1-82e338fc14f7\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"9aba99a2-555e-4ef0-8653-597c6b918683\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 82 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"9aba99a2-555e-4ef0-8653-597c6b918683\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"afb0f064-07dd-43f7-b9c1-82e338fc14f7\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:14.283504', 'end_time': '2021-04-23T15:40:14.344944', 'duration': 0.06144, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:14.335824Z', 'iopub.execute_input': '2021-04-23T15:40:14.336626Z', 'shell.execute_reply': '2021-04-23T15:40:14.344323Z', 'iopub.status.idle': '2021-04-23T15:40:14.344831Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"76f24a7e-595c-451b-b142-970f7296ae3a\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"b226bb60-1f37-45ef-bbc5-c4c74771fc96\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 92 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"b226bb60-1f37-45ef-bbc5-c4c74771fc96\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"76f24a7e-595c-451b-b142-970f7296ae3a\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n      \r\n",
      "  if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:14.378643', 'end_time': '2021-04-23T15:40:14.440273', 'duration': 0.06163, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:14.428150Z', 'iopub.execute_input': '2021-04-23T15:40:14.431863Z', 'iopub.status.idle': '2021-04-23T15:40:14.439715Z', 'shell.execute_reply': '2021-04-23T15:40:14.440111Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"5220fa44-ad9b-44cf-952b-144d45519cae\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"202af08e-cbfd-45f1-a423-b29095249900\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 92 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"202af08e-cbfd-45f1-a423-b29095249900\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"5220fa44-ad9b-44cf-952b-144d45519cae\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n  \r\n",
      "                                          height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:14.475391', 'end_time': '2021-04-23T15:40:14.533943', 'duration': 0.058552, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:14.524987Z', 'iopub.execute_input': '2021-04-23T15:40:14.525692Z', 'shell.execute_reply': '2021-04-23T15:40:14.533399Z', 'iopub.status.idle': '2021-04-23T15:40:14.533830Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"6e207a88-ee48-4327-94a4-fab1c64b74bb\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"1b7a4792-9183-47d6-a7fb-dc909c5b0a22\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"1b7a4792-9183-47d6-a7fb-dc909c5b0a22\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"6e207a88-ee48-4327-94a4-fab1c64b74bb\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T15:40:10.910375', 'end_time': '2021-04-23T15:40:14.976436', 'duration': 4.066061, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:15.051 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:15.051 ip-10-0-109-143.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:15.478 ip-10-0-109-143.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m [2021-04-23 15:40:15.478 ip-10-0-109-143.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:26,  1.11cell/s]#015Executing:   7%|         | 2/30 [00:01<00:24,  1.16cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.74cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.47cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.09cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01,  9.77cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 10.82cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.23cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.37cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 14.93cell/s]#015Executing:  77%|  | 23/30 [00:03<00:00, 14.91cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.37cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 13.55cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.46cell/s]#015Executing: 100%|| 30/30 [00:04<00:00,  7.38cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-WLMUZICGAH-ProfilerReport-1619192154-cd10d0bb/algo-1-1619192360\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.957 ip-10-0-148-20.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.957 ip-10-0-148-20.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.957 ip-10-0-148-20.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.957 ip-10-0-148-20.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.957 ip-10-0-148-20.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.957 ip-10-0-148-20.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.958 ip-10-0-148-20.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.958 ip-10-0-148-20.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.958 ip-10-0-148-20.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.958 ip-10-0-148-20.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.958 ip-10-0-148-20.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.958 ip-10-0-148-20.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.958 ip-10-0-148-20.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.958 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.959 ip-10-0-148-20.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.959 ip-10-0-148-20.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.993 ip-10-0-148-20.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.993 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.993 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.993 ip-10-0-148-20.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.994 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.994 ip-10-0-148-20.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.994 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.994 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.994 ip-10-0-148-20.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.994 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.995 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.995 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.995 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.998 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:16.998 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:21.144 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:17.905163', 'end_time': '2021-04-23T15:40:17.924536', 'duration': 0.019373, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:17.942682', 'end_time': '2021-04-23T15:40:18.747855', 'duration': 0.805173, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:17.967896Z', 'iopub.execute_input': '2021-04-23T15:40:17.968447Z', 'iopub.status.idle': '2021-04-23T15:40:18.747232Z', 'shell.execute_reply': '2021-04-23T15:40:18.747633Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 15:40:18.740 ip-10-0-148-20.ec2.internal:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192156\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:18.768670', 'end_time': '2021-04-23T15:40:18.979293', 'duration': 0.210623, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:18.791275Z', 'iopub.execute_input': '2021-04-23T15:40:18.791760Z', 'shell.execute_reply': '2021-04-23T15:40:18.978755Z', 'iopub.status.idle': '2021-04-23T15:40:18.979180Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n    \r\n",
      "  if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:18.998836', 'end_time': '2021-04-23T15:40:19.026502', 'duration': 0.027666, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.024235Z', 'iopub.execute_input': '2021-04-23T15:40:19.024725Z', 'iopub.status.idle': '2021-04-23T15:40:19.025996Z', 'shell.execute_reply': '2021-04-23T15:40:19.026373Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.045678', 'end_time': '2021-04-23T15:40:19.070466', 'duration': 0.024788, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.068207Z', 'iopub.execute_input': '2021-04-23T15:40:19.068681Z', 'shell.execute_reply': '2021-04-23T15:40:19.069959Z', 'iopub.status.idle': '2021-04-23T15:40:19.070363Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.089619', 'end_time': '2021-04-23T15:40:19.108825', 'duration': 0.019206, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.129088', 'end_time': '2021-04-23T15:40:19.154151', 'duration': 0.025063, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.151774Z', 'iopub.execute_input': '2021-04-23T15:40:19.152256Z', 'shell.execute_reply': '2021-04-23T15:40:19.153626Z', 'iopub.status.idle': '2021-04-23T15:40:19.154043Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.173868', 'end_time': '2021-04-23T15:40:19.206267', 'duration': 0.032399, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.201274Z', 'iopub.execute_input': '2021-04-23T15:40:19.202416Z', 'shell.execute_reply': '2021-04-23T15:40:19.205668Z', 'iopub.status.idle': '2021-04-23T15:40:19.206161Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.226947', 'end_time': '2021-04-23T15:40:19.270151', 'duration': 0.043204, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.256566Z', 'iopub.execute_input': '2021-04-23T15:40:19.260220Z', 'shell.execute_reply': '2021-04-23T15:40:19.269609Z', 'iopub.status.idle': '2021-04-23T15:40:19.270043Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"02f13b35-1143-417d-b0b1-a48d9a153e4d\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"3c8dfaf6-0d0d-45f0-bea5-14b906aff5e8\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringFormatter\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 15:38:58 and ran for 1 seconds. \\\\n Your training job started on 04/23/2021 at 15:38:58 and ran for 1 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"15:38:58 04/23/2021\",\"15:38:59 04/23/2021\",\"1 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1010\"},\"selection_policy\":{\"id\":\"1009\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"editor\":{\"id\":\"1011\"},\"field\":\"0\",\"formatter\":{\"id\":\"1012\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1013\"},\"field\":\"1\",\"formatter\":{\"id\":\"1014\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"StringEditor\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"3c8dfaf6-0d0d-45f0-bea5-14b906aff5e8\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"02f13b35-1143-417d-b0b1-a48d9a153e4d\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.290890', 'end_time': '2021-04-23T15:40:19.311149', 'duration': 0.020259, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.331534', 'end_time': '2021-04-23T15:40:19.357943', 'duration': 0.026409, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.355574Z', 'iopub.execute_input': '2021-04-23T15:40:19.356073Z', 'iopub.status.idle': '2021-04-23T15:40:19.357401Z', 'shell.execute_reply': '2021-04-23T15:40:19.357812Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-input'\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m ], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.378367', 'end_time': '2021-04-23T15:40:19.408331', 'duration': 0.029964, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.405912Z', 'iopub.execute_input': '2021-04-23T15:40:19.406445Z', 'iopub.status.idle': '2021-04-23T15:40:19.407809Z', 'shell.execute_reply': '2021-04-23T15:40:19.408200Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.428991', 'end_time': '2021-04-23T15:40:19.477274', 'duration': 0.048283, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.458254Z', 'iopub.execute_input': '2021-04-23T15:40:19.473154Z', 'iopub.status.idle': '2021-04-23T15:40:19.476754Z', 'shell.execute_reply': '2021-04-23T15:40:19.477142Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"4c256147-b81b-4ad9-b67f-8f24688a1f81\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"256c08e5-4927-454d-b99a-9f4d71304a56\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringEditor\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQNejcD0K1ytAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQOxRuB6FaytAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQArXo3A9iitAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQM3MzMzMzCtAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQNejcD0K1ytAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1067\"},\"selection_policy\":{\"id\":\"1066\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1070\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1071\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringFormatter\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1080\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1081\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringEditor\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{\"editor\":{\"id\":\"1068\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1069\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1082\"},\"field\":\"min\",\"formatter\":{\"id\":\"1083\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringEditor\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{\"editor\":{\"id\":\"1072\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1073\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1074\"},\"field\":\"max\",\"formatter\":{\"id\":\"1075\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1076\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1077\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1078\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1079\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringEditor\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"256c08e5-4927-454d-b99a-9f4d71304a56\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"4c256147-b81b-4ad9-b67f-8f24688a1f81\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.498780', 'end_time': '2021-04-23T15:40:19.539603', 'duration': 0.040823, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.527947Z', 'iopub.execute_input': '2021-04-23T15:40:19.533991Z', 'shell.execute_reply': '2021-04-23T15:40:19.539072Z', 'iopub.status.idle': '2021-04-23T15:40:19.539495Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.561815', 'end_time': '2021-04-23T15:40:19.593907', 'duration': 0.032092, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.591513Z', 'iopub.execute_input': '2021-04-23T15:40:19.592024Z', 'shell.execute_reply': '2021-04-23T15:40:19.593270Z', 'iopub.status.idle': '2021-04-23T15:40\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m :19.593796Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.616144', 'end_time': '2021-04-23T15:40:19.648140', 'duration': 0.031996, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.645894Z', 'iopub.execute_input': '2021-04-23T15:40:19.646405Z', 'iopub.status.idle': '2021-04-23T15:40:19.647637Z', 'shell.execute_reply': '2021-04-23T15:40:19.648012Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.670519', 'end_time': '2021-04-23T15:40:19.692773', 'duration': 0.022254, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.715019', 'end_time': '2021-04-23T15:40:19.743842', 'duration': 0.028823, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.741536Z', 'iopub.execute_input': '2021-04-23T15:40:19.742044Z', 'iopub.status.idle': '2021-04-23T15:40:19.743311Z', 'shell.execute_reply': '2021-04-23T15:40:19.743716Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.765983', 'end_time': '2021-04-23T15:40:19.794764', 'duration': 0.028781, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.792494Z', 'iopub.execute_input': '2021-04-23T15:40:19.792972Z', 'iopub.status.idle': '2021-04-23T15:40:19.794263Z', 'shell.execute_reply': '2021-04-23T15:40:19.794641Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.817266', 'end_time': '2021-04-23T15:40:19.855781', 'duration': 0.038515, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.844904Z', 'iopub.execute_input': '2021-04-23T15:40:19.845394Z', 'iopub.status.idle': '2021-04-23T15:40:19.855271Z', 'shell.execute_reply': '2021-04-23T15:40:19.855654Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the LowGPUUtilization rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>3</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>2</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>3</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n    s\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m ummary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.879204', 'end_time': '2021-04-23T15:40:19.910206', 'duration': 0.031002, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.907160Z', 'iopub.execute_input': '2021-04-23T15:40:19.907644Z', 'shell.execute_reply': '2021-04-23T15:40:19.909614Z', 'iopub.status.idle': '2021-04-23T15:40:19.910103Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.934417', 'end_time': '2021-04-23T15:40:19.963523', 'duration': 0.029106, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:19.961289Z', 'iopub.execute_input': '2021-04-23T15:40:19.961805Z', 'iopub.status.idle': '2021-04-23T15:40:19.963000Z', 'shell.execute_reply': '2021-04-23T15:40:19.963400Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:19.987551', 'end_time': '2021-04-23T15:40:20.041055', 'duration': 0.053504, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:20.029123Z', 'iopub.execute_input': '2021-04-23T15:40:20.032672Z', 'iopub.status.idle': '2021-04-23T15:40:20.040551Z', 'shell.execute_reply': '2021-04-23T15:40:20.040927Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8d13fad1-556d-4a3c-a167-498ef26f6bd7\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"0ba0129d-8cc7-4ca2-b5d4-7893e7861264\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"0ba0129d-8cc7-4ca2-b5d4-7893e7861264\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"8d13fad1-556d-4a3c-a167-498ef26f6bd7\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:20.066875', 'end_time': '2021-04-23T15:40:20.115879', 'duration': 0.049004, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:20.104804Z', 'iopub.execute_input': '2021-04-23T15:40:20.107413Z', 'iopub.status.idle': '2021-04-23T15:40:20.115361Z', 'shell.execute_reply': '2021-04-2\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m 3T15:40:20.115742Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d4978b86-6070-4473-9256-696390457176\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"e0871f1d-ab2d-43d3-a0a4-5aa75b0e6e3b\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"e0871f1d-ab2d-43d3-a0a4-5aa75b0e6e3b\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"d4978b86-6070-4473-9256-696390457176\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:20.143523', 'end_time': '2021-04-23T15:40:20.189358', 'duration': 0.045835, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:20.181082Z', 'iopub.execute_input': '2021-04-23T15:40:20.181640Z', 'shell.execute_reply': '2021-04-23T15:40:20.188596Z', 'iopub.status.idle': '2021-04-23T15:40:20.189201Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"4cd8b7bd-9a17-4af9-992a-291d553453c4\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"399e05bf-1393-40b2-9acc-d6870455c8d1\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"399e05bf-1393-40b2-9acc-d6870455c8d1\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"4cd8b7bd-9a17-4af9-992a-291d553453c4\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:20.218637', 'end_time': '2021-04-23T15:40:20.270559', 'duration': 0.051922, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:20.256303Z', 'iopub.execute_input': '2021-04-23T15:40:20.258424Z', 'shell.execute_reply': '2021-04-23T15:40:20.269962Z', 'iopub.status.idle': '2021-04-23T15:40:20.270449Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"745d1da3-6a1b-4230-9583-be7baa11f5d4\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"3bc9453e-dc2b-48f2-b35d-d2e1241962dc\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"wi\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m dth\":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"3bc9453e-dc2b-48f2-b35d-d2e1241962dc\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"745d1da3-6a1b-4230-9583-be7baa11f5d4\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"3d4ac6c1-74cd-467a-b6a7-71a28cfe2f08\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"ddac46ad-ffa8-44a8-bb73-f76551c9ed39\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"ddac46ad-ffa8-44a8-bb73-f76551c9ed39\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"3d4ac6c1-74cd-467a-b6a7-71a28cfe2f08\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:20.301609', 'end_time': '2021-04-23T15:40:20.353044', 'duration': 0.051435, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:20.344874Z', 'iopub.execute_input': '2021-04-23T15:40:20.345400Z', 'iopub.status.idle': '2021-04-23T15:40:20.352354Z', 'shell.execute_reply': '2021-04-23T15:40:20.352883Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"34855b6d-397e-4ffc-94f9-742e448e4aba\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"43accc27-f24e-4535-bf34-62aacc239f67\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 2 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"43accc27-f24e-4535-bf34-62aacc239f67\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"34855b6d-397e-4ffc-94f9-742e448e4aba\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:20.385000', 'end_time': '2021-04-23T15:40:20.443394', 'duration': 0.058394, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:20.429510Z', 'iopub.execute_input': '2021-04-23T15:40:20.435303Z', 'iopub.status.idle': '2021-04-23T15:40:20.442866Z', 'shell.execute_reply': '2021-04-23T15:40:20.443259Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"36ccc0d2-c5ca-41eb-91ee-20340e86a6bf\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"86b72b6d-0bd4-4d4c-a01a-f04a1116fb18\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 3 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"86b72b6d-0bd4-4d4c-a01a-f04a1116fb18\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"36ccc0d2-c5ca-41eb-91ee-20340e86a6bf\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (a\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m ttempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:20.476658', 'end_time': '2021-04-23T15:40:20.536702', 'duration': 0.060044, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:20.528266Z', 'iopub.execute_input': '2021-04-23T15:40:20.528812Z', 'iopub.status.idle': '2021-04-23T15:40:20.536148Z', 'shell.execute_reply': '2021-04-23T15:40:20.536536Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"28e600be-e72b-435f-8d40-4b0bbf5afeda\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"daaed051-fb64-4ffe-bdb5-1ff17beafae9\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 3 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"daaed051-fb64-4ffe-bdb5-1ff17beafae9\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"28e600be-e72b-435f-8d40-4b0bbf5afeda\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n         \r\n",
      "                                   height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:20.571960', 'end_time': '2021-04-23T15:40:20.629290', 'duration': 0.05733, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:20.618611Z', 'iopub.execute_input': '2021-04-23T15:40:20.621405Z', 'shell.execute_reply': '2021-04-23T15:40:20.628696Z', 'iopub.status.idle': '2021-04-23T15:40:20.629182Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"31afff22-9f15-44c5-a9d3-37b40da16ac3\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"11d3d099-218e-4a24-b6e4-7e80d36b02f3\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"11d3d099-218e-4a24-b6e4-7e80d36b02f3\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"31afff22-9f15-44c5-a9d3-37b40da16ac3\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T15:40:17.025233', 'end_time': '2021-04-23T15:40:21.070889', 'duration': 4.045656, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:21.145 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:21.145 ip-10-0-148-20.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.897 ip-10-0-244-44.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 3 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.937 ip-10-0-244-44.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.938 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619192340000000 to timestamp_end:1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.938 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619192340000000 to timestamp_end:1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.940 ip-10-0-244-44.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.941 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619192340000000 to timestamp_end:1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.941 ip-10-0-244-44.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.941 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619192340000000 to timestamp_end:1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.941 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619192340000000 to timestamp_end:1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.944 ip-10-0-244-44.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.944 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619192340000000 to timestamp_end:1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.944 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619192340000000 to timestamp_end:1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.945 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619192340000000 to timestamp_end:1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.946 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619192340000000 to timestamp_end:1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.950 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619192340000000 to timestamp_end:1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:20.950 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619192340000000 to timestamp_end:1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:20.758 ip-10-0-94-33.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192166\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:21.261 ip-10-0-94-33.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:21.570 ip-10-0-148-20.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m [2021-04-23 15:40:21.570 ip-10-0-148-20.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:26,  1.09cell/s]#015Executing:   7%|         | 2/30 [00:01<00:24,  1.16cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.74cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.48cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.10cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01,  9.81cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 10.87cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.31cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.51cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 15.11cell/s]#015Executing:  77%|  | 23/30 [00:03<00:00, 15.11cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.51cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 13.69cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.66cell/s]#015Executing: 100%|| 30/30 [00:04<00:00,  7.42cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EEIGKCWSSO-ProfilerReport-1619192156-521eb762/algo-1-1619192361\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:23.800 ip-10-0-68-126.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192164\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:24.308 ip-10-0-68-126.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:24.924 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:21.825121', 'end_time': '2021-04-23T15:40:21.845161', 'duration': 0.02004, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:21.863308', 'end_time': '2021-04-23T15:40:22.650968', 'duration': 0.78766, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:21.888653Z', 'iopub.execute_input': '2021-04-23T15:40:21.889165Z', 'iopub.status.idle': '2021-04-23T15:40:22.650348Z', 'shell.execute_reply': '2021-04-23T15:40:22.650746Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 15:40:22.643 ip-10-0-244-44.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192151\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:22.669821', 'end_time': '2021-04-23T15:40:22.877651', 'duration': 0.20783, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:22.691816Z', 'iopub.execute_input': '2021-04-23T15:40:22.692309Z', 'iopub.status.idle': '2021-04-23T15:40:22.877086Z', 'shell.execute_reply': '2021-04-23T15:40:22.877505Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      i\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m f (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:22.896989', 'end_time': '2021-04-23T15:40:22.924124', 'duration': 0.027135, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:22.921843Z', 'iopub.execute_input': '2021-04-23T15:40:22.922329Z', 'shell.execute_reply': '2021-04-23T15:40:22.923591Z', 'iopub.status.idle': '2021-04-23T15:40:22.924021Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:22.943003', 'end_time': '2021-04-23T15:40:22.967100', 'duration': 0.024097, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:22.964895Z', 'iopub.execute_input': '2021-04-23T15:40:22.965434Z', 'iopub.status.idle': '2021-04-23T15:40:22.966579Z', 'shell.execute_reply': '2021-04-23T15:40:22.966963Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:22.985979', 'end_time': '2021-04-23T15:40:23.005629', 'duration': 0.01965, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.024854', 'end_time': '2021-04-23T15:40:23.048990', 'duration': 0.024136, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.046703Z', 'iopub.execute_input': '2021-04-23T15:40:23.047179Z', 'shell.execute_reply': '2021-04-23T15:40:23.048478Z', 'iopub.status.idle': '2021-04-23T15:40:23.048891Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.067926', 'end_time': '2021-04-23T15:40:23.099440', 'duration': 0.031514, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.094608Z', 'iopub.execute_input': '2021-04-23T15:40:23.097252Z', 'iopub.status.idle': '2021-04-23T15:40:23.098913Z', 'shell.execute_reply': '2021-04-23T15:40:23.099317Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.118602', 'end_time': '2021-04-23T15:40:23.160300', 'duration': 0.041698, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.144777Z', 'iopub.execute_input': '2021-04-23T15:40:23.147546Z', 'iopub.status.idle': '2021-04-23T15:40:23.159794Z', 'shell.execute_reply': '2021-04-23T15:40:23.160180Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"26837474-c76c-4380-8e2c-3bb35e66b7a5\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"d839a6b1-67b4-47e3-9671-1225cc8e236a\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"StringFormatter\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"15:39:00 04/23/2021\",\"15:39:59 04/23/2021\",\"59 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1010\"},\"selection_policy\":{\"id\":\"1009\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"StringEditor\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1011\"},\"field\":\"0\",\"formatter\":{\"id\":\"1012\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1013\"},\"field\":\"1\",\"formatter\":{\"id\":\"1014\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 15:39:00 and ran for 59 seconds. \\\\n Your training job started on 04/23/2021 at 15:39:00 and ran for 59 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"d839a6b1-67b4-47e3-9671-1225cc8e236a\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"26837474-c76c-4380-8e2c-3bb35e66b7a5\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.180423', 'end_time': '2021-04-23T15:40:23.200441', 'duration': 0.020018, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.220968', 'end_time': '2021-04-23T15:40:23.246618', 'duration': 0.02565, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.244164Z', 'iopub.execute_input': '2021-04-23T15:40:23.244639Z', 'iopub.status.idle': '2021-04-23T15:40:23.246123Z', 'shell.execute_reply': '2021-04-23T15:40:23.246499Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-input'],\r\n",
      " 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.266754', 'end_time': '2021-04-23T15:40:23.295776', 'duration': 0.029022, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.293438Z', 'iopub.execute_input': '2021-04-23T15:40:23.293978Z', 'shell.execute_reply': '2021-04-23T15:40:23.295157Z', 'iopub.status.idle': '2021-04-23T15:40:23.295675Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.316415', 'end_time': '2021-04-23T15:40:23.363563', 'duration': 0.047148, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.344204Z', 'iopub.execute_input': '2021-04-23T15:40:23.350926Z', 'shell.execute_reply': '2021-04-23T15:40:23.363027Z', 'iopub.status.idle': '2021-04-23T15:40:23.363461Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"fa6a6823-c985-4419-b75b-d46e541e20bb\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"3303c52e-5280-4266-9ec4-f4dbd2de076c\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1080\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1081\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1076\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1077\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1074\"},\"field\":\"max\",\"formatter\":{\"id\":\"1075\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1072\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1073\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringFormatter\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"Selection\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{\"editor\":{\"id\":\"1068\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1069\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringFormatter\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQFyPwvUoHDJApHA9CtdDWEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAAAAAHsUrkfhOjBAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAADD9Shcj8IVQB+F61G4XjBACtejcD0K7z8=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAUrkfhesRYQLgehetRuDFASOF6FK4HUUA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQClcj8L16DFA9ihcj8L1V0A=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1067\"},\"selection_policy\":{\"id\":\"1066\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1082\"},\"field\":\"min\",\"formatter\":{\"id\":\"1083\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringFormatter\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1070\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1071\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1078\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1079\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringEditor\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"3303c52e-5280-4266-9ec4-f4dbd2de076c\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"fa6a6823-c985-4419-b75b-d46e541e20bb\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.384783', 'end_time': '2021-04-23T15:40:23.424433', 'duration': 0.03965, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.418904Z', 'iopub.execute_input': '2021-04-23T15:40:23.421501Z', 'shell.execute_reply': '2021-04-23T15:40:23.423914Z', 'iopub.status.idle': '2021-04-23T15:40:23.424333Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.446425', 'end_time': '2021-04-23T15:40:23.478262', 'duration': 0.031837, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.476024Z', 'iopub.execute_input': '2021-04-23T15:40:23.476560Z', 'iopub.status.idle': '2021-04-23T15:40:23.477723Z', 'shell.execute_reply': '2021-04-23T15:40:23\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m .478133Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.500312', 'end_time': '2021-04-23T15:40:23.532016', 'duration': 0.031704, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.529726Z', 'iopub.execute_input': '2021-04-23T15:40:23.530219Z', 'iopub.status.idle': '2021-04-23T15:40:23.531511Z', 'shell.execute_reply': '2021-04-23T15:40:23.531892Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.554085', 'end_time': '2021-04-23T15:40:23.575983', 'duration': 0.021898, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.597830', 'end_time': '2021-04-23T15:40:23.626367', 'duration': 0.028537, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.624100Z', 'iopub.execute_input': '2021-04-23T15:40:23.624598Z', 'iopub.status.idle': '2021-04-23T15:40:23.625867Z', 'shell.execute_reply': '2021-04-23T15:40:23.626241Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.648359', 'end_time': '2021-04-23T15:40:23.676841', 'duration': 0.028482, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.674600Z', 'iopub.execute_input': '2021-04-23T15:40:23.675092Z', 'shell.execute_reply': '2021-04-23T15:40:23.676333Z', 'iopub.status.idle': '2021-04-23T15:40:23.676738Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.698958', 'end_time': '2021-04-23T15:40:23.738046', 'duration': 0.039088, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.727091Z', 'iopub.execute_input': '2021-04-23T15:40:23.727625Z', 'iopub.status.idle': '2021-04-23T15:40:23.737535Z', 'shell.execute_reply': '2021-04-23T15:40:23.737918Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the BatchSize rule\\nwas the most frequently triggered. It processed 120 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>120</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>124</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>124</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n    summ\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m ary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.761237', 'end_time': '2021-04-23T15:40:23.791934', 'duration': 0.030697, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.788927Z', 'iopub.execute_input': '2021-04-23T15:40:23.789414Z', 'iopub.status.idle': '2021-04-23T15:40:23.791393Z', 'shell.execute_reply': '2021-04-23T15:40:23.791804Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.815827', 'end_time': '2021-04-23T15:40:23.844755', 'duration': 0.028928, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.842434Z', 'iopub.execute_input': '2021-04-23T15:40:23.842914Z', 'iopub.status.idle': '2021-04-23T15:40:23.844184Z', 'shell.execute_reply': '2021-04-23T15:40:23.844631Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.868993', 'end_time': '2021-04-23T15:40:23.923974', 'duration': 0.054981, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.911330Z', 'iopub.execute_input': '2021-04-23T15:40:23.915440Z', 'shell.execute_reply': '2021-04-23T15:40:23.923429Z', 'iopub.status.idle': '2021-04-23T15:40:23.923863Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"ee28f9ab-450d-4ba6-ba61-01cf701740bb\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"581803cb-5d3e-4208-8037-e110da8462ec\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"581803cb-5d3e-4208-8037-e110da8462ec\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"ee28f9ab-450d-4ba6-ba61-01cf701740bb\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:23.949429', 'end_time': '2021-04-23T15:40:23.998425', 'duration': 0.048996, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:23.989218Z', 'iopub.execute_input': '2021-04-23T15:40:23.989985Z', 'shell.execute_reply': '2021-04-23T15:40:23.997795Z', 'iopub.status.idle': '2021-04-23T1\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m 5:40:23.998310Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"2dafadfc-373a-4008-957f-89244e33c26c\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"939ca116-b85c-4d55-8f91-8afdf2a7c23c\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"939ca116-b85c-4d55-8f91-8afdf2a7c23c\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"2dafadfc-373a-4008-957f-89244e33c26c\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:24.025845', 'end_time': '2021-04-23T15:40:24.072884', 'duration': 0.047039, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:24.064672Z', 'iopub.execute_input': '2021-04-23T15:40:24.065279Z', 'iopub.status.idle': '2021-04-23T15:40:24.072340Z', 'shell.execute_reply': '2021-04-23T15:40:24.072735Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"4ba47f5d-38d5-47d8-b021-95147f05a212\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"109c5eda-a062-4c8b-b656-e06431177e4a\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"109c5eda-a062-4c8b-b656-e06431177e4a\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"4ba47f5d-38d5-47d8-b021-95147f05a212\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:24.101540', 'end_time': '2021-04-23T15:40:24.154056', 'duration': 0.052516, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:24.140723Z', 'iopub.execute_input': '2021-04-23T15:40:24.141441Z', 'iopub.status.idle': '2021-04-23T15:40:24.153528Z', 'shell.execute_reply': '2021-04-23T15:40:24.153914Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"14901e91-c8a5-467e-9527-dd7543821555\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"0caa298e-e927-4d3f-8da1-171d4d442af5\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m \":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"0caa298e-e927-4d3f-8da1-171d4d442af5\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"14901e91-c8a5-467e-9527-dd7543821555\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d6996140-33e7-45c7-bff5-00c51cb296eb\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"af4775de-27de-49ba-9d91-91da97882f4b\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"af4775de-27de-49ba-9d91-91da97882f4b\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"d6996140-33e7-45c7-bff5-00c51cb296eb\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:24.184298', 'end_time': '2021-04-23T15:40:24.235303', 'duration': 0.051005, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:24.227303Z', 'iopub.execute_input': '2021-04-23T15:40:24.227919Z', 'shell.execute_reply': '2021-04-23T15:40:24.234756Z', 'iopub.status.idle': '2021-04-23T15:40:24.235176Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8a0de4aa-a745-4f82-90c1-61cb99f98184\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"698a0373-6232-4dcf-b5b2-f04ee509de12\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 120 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"698a0373-6232-4dcf-b5b2-f04ee509de12\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"8a0de4aa-a745-4f82-90c1-61cb99f98184\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:24.267346', 'end_time': '2021-04-23T15:40:24.325304', 'duration': 0.057958, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:24.316647Z', 'iopub.execute_input': '2021-04-23T15:40:24.317386Z', 'iopub.status.idle': '2021-04-23T15:40:24.324782Z', 'shell.execute_reply': '2021-04-23T15:40:24.325176Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"e42b4f8d-dbf8-456e-acde-164b4975823c\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"f735bdfa-822c-486f-96b2-a76be63536a3\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 124 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"f735bdfa-822c-486f-96b2-a76be63536a3\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"e42b4f8d-dbf8-456e-acde-164b4975823c\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:24.358287', 'end_time': '2021-04-23T15:40:24.418821', 'duration': 0.060534, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:24.409979Z', 'iopub.execute_input': '2021-04-23T15:40:24.410673Z', 'shell.execute_reply': '2021-04-23T15:40:24.418204Z', 'iopub.status.idle': '2021-04-23T15:40:24.418714Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"e3871700-1999-41c4-ab72-33dc22a92d48\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"f80ffbf3-84df-4fba-b1c7-cec5f36cbbdd\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 124 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"f80ffbf3-84df-4fba-b1c7-cec5f36cbbdd\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"e3871700-1999-41c4-ab72-33dc22a92d48\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n      \r\n",
      "                                      height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:24.453454', 'end_time': '2021-04-23T15:40:24.510229', 'duration': 0.056775, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:24.501749Z', 'iopub.execute_input': '2021-04-23T15:40:24.502368Z', 'iopub.status.idle': '2021-04-23T15:40:24.509718Z', 'shell.execute_reply': '2021-04-23T15:40:24.510106Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d19dbaeb-0ea4-4f4e-b5eb-ded6df2452ea\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"686d2afd-e20c-4e5c-81ab-86af28574b68\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"686d2afd-e20c-4e5c-81ab-86af28574b68\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"d19dbaeb-0ea4-4f4e-b5eb-ded6df2452ea\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T15:40:20.977318', 'end_time': '2021-04-23T15:40:24.850825', 'duration': 3.873507, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:24.925 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:24.925 ip-10-0-244-44.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:25.342 ip-10-0-244-44.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619192460000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m [2021-04-23 15:40:25.342 ip-10-0-244-44.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:25,  1.13cell/s]#015Executing:   7%|         | 2/30 [00:01<00:23,  1.19cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.79cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.60cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.30cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01, 10.06cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 11.14cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.58cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.79cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 15.34cell/s]#015Executing:  77%|  | 23/30 [00:02<00:00, 15.24cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.58cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 13.76cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.72cell/s]#015Executing: 100%|| 30/30 [00:03<00:00,  7.75cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-SVKXQJNJXR-ProfilerReport-1619192151-ec8c2af1/algo-1-1619192355\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.272 ip-10-0-94-33.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.272 ip-10-0-94-33.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.272 ip-10-0-94-33.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.272 ip-10-0-94-33.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.273 ip-10-0-94-33.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.273 ip-10-0-94-33.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.273 ip-10-0-94-33.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.273 ip-10-0-94-33.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.273 ip-10-0-94-33.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.273 ip-10-0-94-33.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.273 ip-10-0-94-33.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.273 ip-10-0-94-33.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.273 ip-10-0-94-33.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.273 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.274 ip-10-0-94-33.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.274 ip-10-0-94-33.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.313 ip-10-0-94-33.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.313 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.313 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.315 ip-10-0-94-33.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.315 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.315 ip-10-0-94-33.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.315 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.316 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.317 ip-10-0-94-33.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.317 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.318 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.318 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.319 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.322 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:31.322 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.314 ip-10-0-68-126.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.315 ip-10-0-68-126.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.315 ip-10-0-68-126.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.315 ip-10-0-68-126.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.315 ip-10-0-68-126.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.315 ip-10-0-68-126.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.315 ip-10-0-68-126.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.315 ip-10-0-68-126.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.316 ip-10-0-68-126.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.316 ip-10-0-68-126.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.316 ip-10-0-68-126.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.316 ip-10-0-68-126.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.316 ip-10-0-68-126.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.316 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.316 ip-10-0-68-126.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.317 ip-10-0-68-126.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.358 ip-10-0-68-126.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.358 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.359 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.360 ip-10-0-68-126.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.361 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.361 ip-10-0-68-126.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.361 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.361 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.363 ip-10-0-68-126.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.363 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.363 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.363 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.364 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.368 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:34.368 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619192280000000 to timestamp_end:1619192340000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:35.666 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:32.369668', 'end_time': '2021-04-23T15:40:32.388683', 'duration': 0.019015, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:32.408216', 'end_time': '2021-04-23T15:40:33.253927', 'duration': 0.845711, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:32.433139Z', 'iopub.execute_input': '2021-04-23T15:40:32.433650Z', 'shell.execute_reply': '2021-04-23T15:40:33.253116Z', 'iopub.status.idle': '2021-04-23T15:40:33.253779Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 15:40:33.244 ip-10-0-94-33.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192166\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:33.272718', 'end_time': '2021-04-23T15:40:33.489266', 'duration': 0.216548, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:33.295533Z', 'iopub.execute_input': '2021-04-23T15:40:33.296075Z', 'iopub.status.idle': '2021-04-23T15:40:33.488727Z', 'shell.execute_reply': '2021-04-23T15:40:33.489122Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:33.508459', 'end_time': '2021-04-23T15:40:33.536478', 'duration': 0.028019, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:33.534220Z', 'iopub.execute_input': '2021-04-23T15:40:33.534740Z', 'iopub.status.idle': '2021-04-23T15:40:33.535963Z', 'shell.execute_reply': '2021-04-23T15:40:33.536339Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:33.555574', 'end_time': '2021-04-23T15:40:33.580596', 'duration': 0.025022, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:33.578344Z', 'iopub.execute_input': '2021-04-23T15:40:33.578842Z', 'iopub.status.idle': '2021-04-23T15:40:33.580066Z', 'shell.execute_reply': '2021-04-23T15:40:33.580445Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:33.599601', 'end_time': '2021-04-23T15:40:33.618936', 'duration': 0.019335, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:33.638200', 'end_time': '2021-04-23T15:40:33.663269', 'duration': 0.025069, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:33.661011Z', 'iopub.execute_input': '2021-04-23T15:40:33.661509Z', 'iopub.status.idle': '2021-04-23T15:40:33.662669Z', 'shell.execute_reply': '2021-04-23T15:40:33.663128Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:33.682416', 'end_time': '2021-04-23T15:40:33.715272', 'duration': 0.032856, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:33.710080Z', 'iopub.execute_input': '2021-04-23T15:40:33.711253Z', 'iopub.status.idle': '2021-04-23T15:40:33.714746Z', 'shell.execute_reply': '2021-04-23T15:40:33.715132Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:33.735145', 'end_time': '2021-04-23T15:40:33.779749', 'duration': 0.044604, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:33.765498Z', 'iopub.execute_input': '2021-04-23T15:40:33.769247Z', 'iopub.status.idle': '2021-04-23T15:40:33.779217Z', 'shell.execute_reply': '2021-04-23T15:40:33.779606Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"5f067d8f-6c06-402c-b1ff-43dd4b4419ea\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"faf3e8af-d4fe-41fc-b18f-f71935a59854\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"StringEditor\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 15:38:33 and ran for 26 seconds. \\\\n Your training job started on 04/23/2021 at 15:38:33 and ran for 26 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1009\"},\"field\":\"0\",\"formatter\":{\"id\":\"1010\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"15:38:33 04/23/2021\",\"15:38:59 04/23/2021\",\"26 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1013\"},\"selection_policy\":{\"id\":\"1014\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1011\"},\"field\":\"1\",\"formatter\":{\"id\":\"1012\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"Selection\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"faf3e8af-d4fe-41fc-b18f-f71935a59854\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"5f067d8f-6c06-402c-b1ff-43dd4b4419ea\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:33.800540', 'end_time': '2021-04-23T15:40:33.820920', 'duration': 0.02038, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:33.841164', 'end_time': '2021-04-23T15:40:33.867453', 'duration': 0.026289, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:33.864669Z', 'iopub.execute_input': '2021-04-23T15:40:33.865193Z', 'shell.execute_reply': '2021-04-23T15:40:33.866822Z', 'iopub.status.idle': '2021-04-23T15:40:33.867335Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-input'\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m ], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:33.887892', 'end_time': '2021-04-23T15:40:33.918498', 'duration': 0.030606, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:33.915902Z', 'iopub.execute_input': '2021-04-23T15:40:33.916494Z', 'iopub.status.idle': '2021-04-23T15:40:33.917960Z', 'shell.execute_reply': '2021-04-23T15:40:33.918354Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:33.939131', 'end_time': '2021-04-23T15:40:33.988138', 'duration': 0.049007, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:33.968310Z', 'iopub.execute_input': '2021-04-23T15:40:33.969067Z', 'iopub.status.idle': '2021-04-23T15:40:33.987606Z', 'shell.execute_reply': '2021-04-23T15:40:33.987995Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"dc8358bf-25f2-4f15-85cc-622af8dea6dd\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"5a0f5299-01de-49be-a236-d9c60e800083\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"Selection\"},{\"attributes\":{\"editor\":{\"id\":\"1070\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1071\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"editor\":{\"id\":\"1074\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1075\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQBSuR+F6lDJASOF6FK7nU0A=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAABxPQrXo7BNQD0K16NwPStAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAAAfhetRuL5YQB+F61G4HjFAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQEjhehSuRzJAw/UoXI/iTEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQPYoXI/CdTJA7FG4HoUrU0A=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1082\"},\"selection_policy\":{\"id\":\"1083\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringEditor\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{\"editor\":{\"id\":\"1080\"},\"field\":\"min\",\"formatter\":{\"id\":\"1081\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1066\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1067\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1068\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1069\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1078\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1079\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringEditor\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1072\"},\"field\":\"max\",\"formatter\":{\"id\":\"1073\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringEditor\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{\"editor\":{\"id\":\"1076\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1077\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringEditor\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"5a0f5299-01de-49be-a236-d9c60e800083\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"dc8358bf-25f2-4f15-85cc-622af8dea6dd\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.009585', 'end_time': '2021-04-23T15:40:34.050915', 'duration': 0.04133, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.044936Z', 'iopub.execute_input': '2021-04-23T15:40:34.045667Z', 'shell.execute_reply': '2021-04-23T15:40:34.050275Z', 'iopub.status.idle': '2021-04-23T15:40:34.050797Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.073083', 'end_time': '2021-04-23T15:40:34.105193', 'duration': 0.03211, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.102872Z', 'iopub.execute_input': '2021-04-23T15:40:34.103385Z', 'iopub.status.idle': '2021-04-23T15:40:34.104655Z', 'shell.execute_reply': '2021-04-23T15:40:3\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m 4.105043Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.127146', 'end_time': '2021-04-23T15:40:34.159262', 'duration': 0.032116, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.156890Z', 'iopub.execute_input': '2021-04-23T15:40:34.157400Z', 'shell.execute_reply': '2021-04-23T15:40:34.158658Z', 'iopub.status.idle': '2021-04-23T15:40:34.159152Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.181593', 'end_time': '2021-04-23T15:40:34.203557', 'duration': 0.021964, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.225500', 'end_time': '2021-04-23T15:40:34.254790', 'duration': 0.02929, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.251666Z', 'iopub.execute_input': '2021-04-23T15:40:34.252478Z', 'iopub.status.idle': '2021-04-23T15:40:34.254270Z', 'shell.execute_reply': '2021-04-23T15:40:34.254650Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.276948', 'end_time': '2021-04-23T15:40:34.305963', 'duration': 0.029015, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.303621Z', 'iopub.execute_input': '2021-04-23T15:40:34.304133Z', 'iopub.status.idle': '2021-04-23T15:40:34.305410Z', 'shell.execute_reply': '2021-04-23T15:40:34.305822Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.328463', 'end_time': '2021-04-23T15:40:34.368214', 'duration': 0.039751, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.356471Z', 'iopub.execute_input': '2021-04-23T15:40:34.357024Z', 'shell.execute_reply': '2021-04-23T15:40:34.367585Z', 'iopub.status.idle': '2021-04-23T15:40:34.368098Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the IOBottleneck rule\\nwas the most frequently triggered. It processed 58 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>58</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>58</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>53</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n    summa\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m ry[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.391996', 'end_time': '2021-04-23T15:40:34.423835', 'duration': 0.031839, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.420770Z', 'iopub.execute_input': '2021-04-23T15:40:34.421332Z', 'iopub.status.idle': '2021-04-23T15:40:34.423312Z', 'shell.execute_reply': '2021-04-23T15:40:34.423695Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.448004', 'end_time': '2021-04-23T15:40:34.477809', 'duration': 0.029805, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.475472Z', 'iopub.execute_input': '2021-04-23T15:40:34.475971Z', 'iopub.status.idle': '2021-04-23T15:40:34.477257Z', 'shell.execute_reply': '2021-04-23T15:40:34.477643Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.501826', 'end_time': '2021-04-23T15:40:34.556935', 'duration': 0.055109, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.544237Z', 'iopub.execute_input': '2021-04-23T15:40:34.548124Z', 'iopub.status.idle': '2021-04-23T15:40:34.556371Z', 'shell.execute_reply': '2021-04-23T15:40:34.556783Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"71d07bf4-64b3-488a-96f9-2edbf6795dee\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"3328bb59-4b12-4491-8040-6a897c91b3da\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"3328bb59-4b12-4491-8040-6a897c91b3da\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"71d07bf4-64b3-488a-96f9-2edbf6795dee\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.582613', 'end_time': '2021-04-23T15:40:34.634287', 'duration': 0.051674, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.622962Z', 'iopub.execute_input': '2021-04-23T15:40:34.623888Z', 'shell.execute_reply': '2021-04-23T15:40:34.633675Z', 'iopub.status.idle': '2021-04-23T15\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m :40:34.634164Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"4e6fc998-3d40-4541-be42-cdd3fbb9e724\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"eab7a831-28a0-4d02-bb85-d0abf371e3df\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"eab7a831-28a0-4d02-bb85-d0abf371e3df\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"4e6fc998-3d40-4541-be42-cdd3fbb9e724\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.661275', 'end_time': '2021-04-23T15:40:34.707937', 'duration': 0.046662, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.697719Z', 'iopub.execute_input': '2021-04-23T15:40:34.699965Z', 'iopub.status.idle': '2021-04-23T15:40:34.707405Z', 'shell.execute_reply': '2021-04-23T15:40:34.707794Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"59cf36d9-44c8-4af7-b1d7-d5556f838927\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"ff81a2dc-1462-4022-8649-0857e6e65b39\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"ff81a2dc-1462-4022-8649-0857e6e65b39\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"59cf36d9-44c8-4af7-b1d7-d5556f838927\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.736702', 'end_time': '2021-04-23T15:40:34.790428', 'duration': 0.053726, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.777301Z', 'iopub.execute_input': '2021-04-23T15:40:34.777940Z', 'iopub.status.idle': '2021-04-23T15:40:34.789883Z', 'shell.execute_reply': '2021-04-23T15:40:34.790283Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"cd7e68c2-56df-4f68-8dd5-ad0801c3fafe\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"65fe0e6b-b6fa-4904-848f-126cc7c52b26\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\"\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m :900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"65fe0e6b-b6fa-4904-848f-126cc7c52b26\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"cd7e68c2-56df-4f68-8dd5-ad0801c3fafe\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"36ca933a-2dd9-4558-883a-fd0a42842a30\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"d1106ccf-9ea1-4170-b9c7-2fb2f31bd2e3\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"d1106ccf-9ea1-4170-b9c7-2fb2f31bd2e3\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"36ca933a-2dd9-4558-883a-fd0a42842a30\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.821235', 'end_time': '2021-04-23T15:40:34.873370', 'duration': 0.052135, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.864862Z', 'iopub.execute_input': '2021-04-23T15:40:34.865484Z', 'iopub.status.idle': '2021-04-23T15:40:34.872796Z', 'shell.execute_reply': '2021-04-23T15:40:34.873212Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"9f374350-2524-4186-98f6-7d160b9e6cd7\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"cbf7c693-1f6c-408a-933b-6a824615063e\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 53 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"cbf7c693-1f6c-408a-933b-6a824615063e\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"9f374350-2524-4186-98f6-7d160b9e6cd7\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.905208', 'end_time': '2021-04-23T15:40:34.965135', 'duration': 0.059927, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:34.956205Z', 'iopub.execute_input': '2021-04-23T15:40:34.956896Z', 'shell.execute_reply': '2021-04-23T15:40:34.964592Z', 'iopub.status.idle': '2021-04-23T15:40:34.965019Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"0e44fb0b-76c0-4879-86c1-e9b4a2881468\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"9348872b-36e4-47e1-9ac4-46167214939d\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 58 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"9348872b-36e4-47e1-9ac4-46167214939d\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"0e44fb0b-76c0-4879-86c1-e9b4a2881468\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (att\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m empts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:34.998405', 'end_time': '2021-04-23T15:40:35.059107', 'duration': 0.060702, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:35.047333Z', 'iopub.execute_input': '2021-04-23T15:40:35.050837Z', 'shell.execute_reply': '2021-04-23T15:40:35.058569Z', 'iopub.status.idle': '2021-04-23T15:40:35.058992Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"43daa610-ef9a-4f8d-8546-445c6ed34880\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"b453a24f-e060-4bd0-abd7-43a483036f58\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 58 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"b453a24f-e060-4bd0-abd7-43a483036f58\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"43daa610-ef9a-4f8d-8546-445c6ed34880\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n          \r\n",
      "                                  height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:35.093791', 'end_time': '2021-04-23T15:40:35.151548', 'duration': 0.057757, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:35.143302Z', 'iopub.execute_input': '2021-04-23T15:40:35.143861Z', 'shell.execute_reply': '2021-04-23T15:40:35.151015Z', 'iopub.status.idle': '2021-04-23T15:40:35.151433Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"e990bdab-fb33-465b-85cb-16d6c720c7bd\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"d980f184-fc11-4562-920a-8316738a9e5a\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"d980f184-fc11-4562-920a-8316738a9e5a\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"e990bdab-fb33-465b-85cb-16d6c720c7bd\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T15:40:31.349020', 'end_time': '2021-04-23T15:40:35.593879', 'duration': 4.244859, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:35.667 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:35.667 ip-10-0-94-33.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:36.091 ip-10-0-94-33.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m [2021-04-23 15:40:36.091 ip-10-0-94-33.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DLLKOGQBPW-ProfilerReport-1619192166-94302480/algo-1-1619192374\u001b[0m #015Executing:   3%|         | 1/30 [00:01<00:30,  1.06s/cell]#015Executing:   7%|         | 2/30 [00:01<00:26,  1.06cell/s]#015Executing:  10%|         | 3/30 [00:02<00:16,  1.61cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.19cell/s]#015Executing:  30%|       | 9/30 [00:02<00:03,  6.71cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01,  9.36cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 10.45cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 11.93cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.17cell/s]#015Executing:  70%|   | 21/30 [00:03<00:00, 14.77cell/s]#015Executing:  77%|  | 23/30 [00:03<00:00, 14.80cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.23cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 13.46cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.48cell/s]#015Executing: 100%|| 30/30 [00:04<00:00,  7.07cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:38.546 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:35.269314', 'end_time': '2021-04-23T15:40:35.289705', 'duration': 0.020391, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:35.309218', 'end_time': '2021-04-23T15:40:36.141325', 'duration': 0.832107, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:35.334911Z', 'iopub.execute_input': '2021-04-23T15:40:35.335434Z', 'iopub.status.idle': '2021-04-23T15:40:36.140664Z', 'shell.execute_reply': '2021-04-23T15:40:36.141091Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 15:40:36.132 ip-10-0-68-126.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619192164\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:36.160496', 'end_time': '2021-04-23T15:40:36.376212', 'duration': 0.215716, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:36.183865Z', 'iopub.execute_input': '2021-04-23T15:40:36.184389Z', 'shell.execute_reply': '2021-04-23T15:40:36.375589Z', 'iopub.status.idle': '2021-04-23T15:40:36.376094Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n    \r\n",
      "  if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:36.395466', 'end_time': '2021-04-23T15:40:36.423279', 'duration': 0.027813, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:36.420972Z', 'iopub.execute_input': '2021-04-23T15:40:36.421478Z', 'iopub.status.idle': '2021-04-23T15:40:36.422738Z', 'shell.execute_reply': '2021-04-23T15:40:36.423144Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:36.442362', 'end_time': '2021-04-23T15:40:36.467462', 'duration': 0.0251, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:36.465129Z', 'iopub.execute_input': '2021-04-23T15:40:36.465623Z', 'iopub.status.idle': '2021-04-23T15:40:36.466913Z', 'shell.execute_reply': '2021-04-23T15:40:36.467325Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:36.486606', 'end_time': '2021-04-23T15:40:36.505492', 'duration': 0.018886, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:36.525032', 'end_time': '2021-04-23T15:40:36.549931', 'duration': 0.024899, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:36.547657Z', 'iopub.execute_input': '2021-04-23T15:40:36.548138Z', 'iopub.status.idle': '2021-04-23T15:40:36.549390Z', 'shell.execute_reply': '2021-04-23T15:40:36.549804Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:36.569083', 'end_time': '2021-04-23T15:40:36.601600', 'duration': 0.032517, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:36.597652Z', 'iopub.execute_input': '2021-04-23T15:40:36.599330Z', 'iopub.status.idle': '2021-04-23T15:40:36.601081Z', 'shell.execute_reply': '2021-04-23T15:40:36.601463Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:36.621181', 'end_time': '2021-04-23T15:40:36.665123', 'duration': 0.043942, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:36.648478Z', 'iopub.execute_input': '2021-04-23T15:40:36.651801Z', 'iopub.status.idle': '2021-04-23T15:40:36.664576Z', 'shell.execute_reply': '2021-04-23T15:40:36.664979Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"499cbda6-ccd6-48dc-8df9-291f7507d7d7\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"1f742111-8bfd-4ca6-b1cd-4b56f10623bf\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringFormatter\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 15:38:33 and ran for 26 seconds. \\\\n Your training job started on 04/23/2021 at 15:38:33 and ran for 26 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"editor\":{\"id\":\"1013\"},\"field\":\"1\",\"formatter\":{\"id\":\"1014\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1011\"},\"field\":\"0\",\"formatter\":{\"id\":\"1012\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"StringEditor\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"15:38:33 04/23/2021\",\"15:38:59 04/23/2021\",\"26 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1010\"},\"selection_policy\":{\"id\":\"1009\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"1f742111-8bfd-4ca6-b1cd-4b56f10623bf\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"499cbda6-ccd6-48dc-8df9-291f7507d7d7\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:36.686067', 'end_time': '2021-04-23T15:40:36.706458', 'duration': 0.020391, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:36.726760', 'end_time': '2021-04-23T15:40:36.752435', 'duration': 0.025675, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:36.749784Z', 'iopub.execute_input': '2021-04-23T15:40:36.750288Z', 'iopub.status.idle': '2021-04-23T15:40:36.751919Z', 'shell.execute_reply': '2021-04-23T15:40:36.752298Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-input\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m '], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:36.772997', 'end_time': '2021-04-23T15:40:36.802614', 'duration': 0.029617, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:36.800366Z', 'iopub.execute_input': '2021-04-23T15:40:36.800909Z', 'shell.execute_reply': '2021-04-23T15:40:36.802098Z', 'iopub.status.idle': '2021-04-23T15:40:36.802506Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:36.823204', 'end_time': '2021-04-23T15:40:36.871238', 'duration': 0.048034, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:36.852502Z', 'iopub.execute_input': '2021-04-23T15:40:36.853023Z', 'iopub.status.idle': '2021-04-23T15:40:36.870681Z', 'shell.execute_reply': '2021-04-23T15:40:36.871090Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8da321f4-585b-49af-bbe1-33dad8e3dd9e\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"49af62b9-0f20-490b-b051-acc931784b4f\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1068\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1069\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringEditor\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{\"editor\":{\"id\":\"1076\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1077\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQK5H4XoU7jFAXI/C9Sj8VkA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAA9CtejcH1YQFK4HoXr0SdAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQM3MzMzMTC9AAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQM3MzMzMzDFAUrgehevRN0A=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQGZmZmZm5jFArkfhehQuVkA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1067\"},\"selection_policy\":{\"id\":\"1066\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"editor\":{\"id\":\"1074\"},\"field\":\"max\",\"formatter\":{\"id\":\"1075\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"StringEditor\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1078\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1079\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"editor\":{\"id\":\"1082\"},\"field\":\"min\",\"formatter\":{\"id\":\"1083\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1072\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1073\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1080\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1081\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringEditor\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1070\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1071\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringFormatter\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"49af62b9-0f20-490b-b051-acc931784b4f\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"8da321f4-585b-49af-bbe1-33dad8e3dd9e\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:36.892495', 'end_time': '2021-04-23T15:40:36.933631', 'duration': 0.041136, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:36.927900Z', 'iopub.execute_input': '2021-04-23T15:40:36.928565Z', 'shell.execute_reply': '2021-04-23T15:40:36.933023Z', 'iopub.status.idle': '2021-04-23T15:40:36.933516Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:36.955893', 'end_time': '2021-04-23T15:40:36.987733', 'duration': 0.03184, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:36.985526Z', 'iopub.execute_input': '2021-04-23T15:40:36.986044Z', 'iopub.status.idle': '2021-04-23T15:40:36.987191Z', 'shell.execute_reply': '2021-04-23T15:40\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m :36.987598Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.009936', 'end_time': '2021-04-23T15:40:37.042117', 'duration': 0.032181, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:37.039852Z', 'iopub.execute_input': '2021-04-23T15:40:37.040384Z', 'shell.execute_reply': '2021-04-23T15:40:37.041539Z', 'iopub.status.idle': '2021-04-23T15:40:37.042005Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.064529', 'end_time': '2021-04-23T15:40:37.086670', 'duration': 0.022141, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.108706', 'end_time': '2021-04-23T15:40:37.137726', 'duration': 0.02902, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:37.135457Z', 'iopub.execute_input': '2021-04-23T15:40:37.135965Z', 'shell.execute_reply': '2021-04-23T15:40:37.137210Z', 'iopub.status.idle': '2021-04-23T15:40:37.137616Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.160066', 'end_time': '2021-04-23T15:40:37.188751', 'duration': 0.028685, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:37.186556Z', 'iopub.execute_input': '2021-04-23T15:40:37.187094Z', 'iopub.status.idle': '2021-04-23T15:40:37.188170Z', 'shell.execute_reply': '2021-04-23T15:40:37.188615Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.211569', 'end_time': '2021-04-23T15:40:37.251059', 'duration': 0.03949, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:37.239653Z', 'iopub.execute_input': '2021-04-23T15:40:37.240184Z', 'iopub.status.idle': '2021-04-23T15:40:37.250507Z', 'shell.execute_reply': '2021-04-23T15:40:37.250915Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the Dataloader rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>66</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>53</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>66</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n    summary\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.274981', 'end_time': '2021-04-23T15:40:37.306497', 'duration': 0.031516, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:37.303515Z', 'iopub.execute_input': '2021-04-23T15:40:37.304029Z', 'iopub.status.idle': '2021-04-23T15:40:37.305985Z', 'shell.execute_reply': '2021-04-23T15:40:37.306361Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.330718', 'end_time': '2021-04-23T15:40:37.360166', 'duration': 0.029448, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:37.357876Z', 'iopub.execute_input': '2021-04-23T15:40:37.358374Z', 'iopub.status.idle': '2021-04-23T15:40:37.359626Z', 'shell.execute_reply': '2021-04-23T15:40:37.360016Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.384708', 'end_time': '2021-04-23T15:40:37.439078', 'duration': 0.05437, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:37.427252Z', 'iopub.execute_input': '2021-04-23T15:40:37.430995Z', 'shell.execute_reply': '2021-04-23T15:40:37.438520Z', 'iopub.status.idle': '2021-04-23T15:40:37.438962Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d78a3d66-410a-4bc6-b657-02ea51391599\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"471c27a6-e9a7-483e-82ed-a9d17a3bb8a5\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"471c27a6-e9a7-483e-82ed-a9d17a3bb8a5\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"d78a3d66-410a-4bc6-b657-02ea51391599\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.464779', 'end_time': '2021-04-23T15:40:37.514212', 'duration': 0.049433, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:37.505121Z', 'iopub.execute_input': '2021-04-23T15:40:37.505698Z', 'shell.execute_reply': '2021-04-23T15:40:37.513594Z', 'iopub.status.idle': '2021-04-23T15:40\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m :37.514096Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"88ba6c56-1487-47b1-831c-c1cad68e8b48\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"80dc3254-bcbc-421b-9800-21857894e49a\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"80dc3254-bcbc-421b-9800-21857894e49a\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"88ba6c56-1487-47b1-831c-c1cad68e8b48\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.541442', 'end_time': '2021-04-23T15:40:37.587886', 'duration': 0.046444, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:37.579581Z', 'iopub.execute_input': '2021-04-23T15:40:37.580190Z', 'shell.execute_reply': '2021-04-23T15:40:37.587355Z', 'iopub.status.idle': '2021-04-23T15:40:37.587776Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"e866e0f5-53ac-4b2f-bb18-690d6ed94a58\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"b3072329-ea0a-4630-8ead-289235fe4d8d\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"b3072329-ea0a-4630-8ead-289235fe4d8d\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"e866e0f5-53ac-4b2f-bb18-690d6ed94a58\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.616561', 'end_time': '2021-04-23T15:40:37.669787', 'duration': 0.053226, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:37.654669Z', 'iopub.execute_input': '2021-04-23T15:40:37.656832Z', 'iopub.status.idle': '2021-04-23T15:40:37.669256Z', 'shell.execute_reply': '2021-04-23T15:40:37.669640Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d038ee39-08c7-4acc-abb2-b6eda37d8328\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"211e72b1-8346-4b92-acb4-1b105c8cab82\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":90\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m 0},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"211e72b1-8346-4b92-acb4-1b105c8cab82\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"d038ee39-08c7-4acc-abb2-b6eda37d8328\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"fef0b237-1bbe-4407-a8d6-d7470288997f\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"984f1ac0-35dc-4f2f-93fe-2ba2e30a3070\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"984f1ac0-35dc-4f2f-93fe-2ba2e30a3070\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"fef0b237-1bbe-4407-a8d6-d7470288997f\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.700956', 'end_time': '2021-04-23T15:40:37.753008', 'duration': 0.052052, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:37.744738Z', 'iopub.execute_input': '2021-04-23T15:40:37.745335Z', 'shell.execute_reply': '2021-04-23T15:40:37.752443Z', 'iopub.status.idle': '2021-04-23T15:40:37.752894Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"5dcb5de0-b503-45de-be3e-e1e76a023e27\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"88c31dc0-ef7f-4202-b91e-866b8c72d4ea\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 53 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"88c31dc0-ef7f-4202-b91e-866b8c72d4ea\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"5dcb5de0-b503-45de-be3e-e1e76a023e27\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.784949', 'end_time': '2021-04-23T15:40:37.843823', 'duration': 0.058874, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:37.824126Z', 'iopub.execute_input': '2021-04-23T15:40:37.835416Z', 'shell.execute_reply': '2021-04-23T15:40:37.843277Z', 'iopub.status.idle': '2021-04-23T15:40:37.843705Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"3ed90db5-88de-4900-89ee-94e736983c05\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"faca02bb-f5df-4728-be39-2ac1edc8ea4d\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 66 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"faca02bb-f5df-4728-be39-2ac1edc8ea4d\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"3ed90db5-88de-4900-89ee-94e736983c05\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attemp\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m ts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.876941', 'end_time': '2021-04-23T15:40:37.937854', 'duration': 0.060913, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:37.922810Z', 'iopub.execute_input': '2021-04-23T15:40:37.930063Z', 'iopub.status.idle': '2021-04-23T15:40:37.937329Z', 'shell.execute_reply': '2021-04-23T15:40:37.937714Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"6e72507d-3158-4b53-860c-9f62aa10782a\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"e23717d5-a7df-4396-a197-3acba686e460\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 66 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"e23717d5-a7df-4396-a197-3acba686e460\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"6e72507d-3158-4b53-860c-9f62aa10782a\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n             \r\n",
      "                               height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T15:40:37.972433', 'end_time': '2021-04-23T15:40:38.030551', 'duration': 0.058118, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T15:40:38.014252Z', 'iopub.execute_input': '2021-04-23T15:40:38.022019Z', 'iopub.status.idle': '2021-04-23T15:40:38.030010Z', 'shell.execute_reply': '2021-04-23T15:40:38.030405Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8795b6fc-7b8d-41ef-a929-932c0adaf709\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"a2da65fc-ded3-4667-a8ff-856e5592b08c\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"a2da65fc-ded3-4667-a8ff-856e5592b08c\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"8795b6fc-7b8d-41ef-a929-932c0adaf709\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T15:40:34.396170', 'end_time': '2021-04-23T15:40:38.472962', 'duration': 4.076792, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:38.547 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:38.547 ip-10-0-68-126.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:38.969 ip-10-0-68-126.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619192400000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m [2021-04-23 15:40:38.970 ip-10-0-68-126.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-DHMUPWSORF-ProfilerReport-1619192164-958c461b/algo-1-1619192382\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:26,  1.10cell/s]#015Executing:   7%|         | 2/30 [00:01<00:24,  1.14cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.71cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.43cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.03cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01,  9.74cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 10.81cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.26cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.45cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 15.01cell/s]#015Executing:  77%|  | 23/30 [00:03<00:00, 14.98cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.43cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 13.59cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.59cell/s]#015Executing: 100%|| 30/30 [00:04<00:00,  7.36cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:18.211 ip-10-2-238-48.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199802\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:18.697 ip-10-2-238-48.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:25.134 ip-10-2-77-202.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199810\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:25.618 ip-10-2-77-202.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:24.038 ip-10-0-143-30.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199805\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:24.540 ip-10-0-143-30.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.700 ip-10-2-238-48.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.700 ip-10-2-238-48.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.700 ip-10-2-238-48.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.701 ip-10-2-238-48.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.701 ip-10-2-238-48.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.701 ip-10-2-238-48.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.701 ip-10-2-238-48.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.701 ip-10-2-238-48.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.701 ip-10-2-238-48.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.701 ip-10-2-238-48.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.701 ip-10-2-238-48.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.701 ip-10-2-238-48.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.701 ip-10-2-238-48.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.701 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.702 ip-10-2-238-48.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.702 ip-10-2-238-48.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.740 ip-10-2-238-48.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.740 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.740 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.741 ip-10-2-238-48.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.741 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.741 ip-10-2-238-48.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.741 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.742 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.742 ip-10-2-238-48.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.742 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.742 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.743 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.743 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.746 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:28.746 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:32.825 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:29.619093', 'end_time': '2021-04-23T17:47:29.637845', 'duration': 0.018752, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:29.655452', 'end_time': '2021-04-23T17:47:30.461630', 'duration': 0.806178, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:29.679707Z', 'iopub.execute_input': '2021-04-23T17:47:29.680203Z', 'shell.execute_reply': '2021-04-23T17:47:30.460981Z', 'iopub.status.idle': '2021-04-23T17:47:30.461502Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 17:47:30.453 ip-10-2-238-48.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199802\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:30.481619', 'end_time': '2021-04-23T17:47:30.693224', 'duration': 0.211605, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:30.504260Z', 'iopub.execute_input': '2021-04-23T17:47:30.504818Z', 'iopub.status.idle': '2021-04-23T17:47:30.692704Z', 'shell.execute_reply': '2021-04-23T17:47:30.693084Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n    \r\n",
      "  if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:30.712089', 'end_time': '2021-04-23T17:47:30.740046', 'duration': 0.027957, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:30.737802Z', 'iopub.execute_input': '2021-04-23T17:47:30.738293Z', 'iopub.status.idle': '2021-04-23T17:47:30.739544Z', 'shell.execute_reply': '2021-04-23T17:47:30.739916Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:30.758844', 'end_time': '2021-04-23T17:47:30.783771', 'duration': 0.024927, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:30.781523Z', 'iopub.execute_input': '2021-04-23T17:47:30.781990Z', 'iopub.status.idle': '2021-04-23T17:47:30.783273Z', 'shell.execute_reply': '2021-04-23T17:47:30.783644Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:30.802304', 'end_time': '2021-04-23T17:47:30.820851', 'duration': 0.018547, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:30.839778', 'end_time': '2021-04-23T17:47:30.864156', 'duration': 0.024378, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:30.861873Z', 'iopub.execute_input': '2021-04-23T17:47:30.862355Z', 'iopub.status.idle': '2021-04-23T17:47:30.863622Z', 'shell.execute_reply': '2021-04-23T17:47:30.863994Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:30.882895', 'end_time': '2021-04-23T17:47:30.915003', 'duration': 0.032108, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:30.909943Z', 'iopub.execute_input': '2021-04-23T17:47:30.912698Z', 'iopub.status.idle': '2021-04-23T17:47:30.914453Z', 'shell.execute_reply': '2021-04-23T17:47:30.914863Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:30.933956', 'end_time': '2021-04-23T17:47:30.976591', 'duration': 0.042635, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:30.960513Z', 'iopub.execute_input': '2021-04-23T17:47:30.964089Z', 'shell.execute_reply': '2021-04-23T17:47:30.976076Z', 'iopub.status.idle': '2021-04-23T17:47:30.976482Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"5ca7d98f-850b-4ec3-87df-df5a66ed0ca2\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"0f491d39-4f0d-4ce5-a34c-e50ad8f911ab\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringEditor\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"17:45:51 04/23/2021\",\"17:45:59 04/23/2021\",\"8 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1010\"},\"selection_policy\":{\"id\":\"1009\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 17:45:51 and ran for 8 seconds. \\\\n Your training job started on 04/23/2021 at 17:45:51 and ran for 8 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1014\"},\"field\":\"1\",\"formatter\":{\"id\":\"1013\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1012\"},\"field\":\"0\",\"formatter\":{\"id\":\"1011\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringFormatter\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"0f491d39-4f0d-4ce5-a34c-e50ad8f911ab\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"5ca7d98f-850b-4ec3-87df-df5a66ed0ca2\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:30.996501', 'end_time': '2021-04-23T17:47:31.016190', 'duration': 0.019689, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.036269', 'end_time': '2021-04-23T17:47:31.061520', 'duration': 0.025251, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.058984Z', 'iopub.execute_input': '2021-04-23T17:47:31.059477Z', 'iopub.status.idle': '2021-04-23T17:47:31.060992Z', 'shell.execute_reply': '2021-04-23T17:47:31.061388Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-input'\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m ], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.081190', 'end_time': '2021-04-23T17:47:31.110226', 'duration': 0.029036, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.108003Z', 'iopub.execute_input': '2021-04-23T17:47:31.108501Z', 'iopub.status.idle': '2021-04-23T17:47:31.109730Z', 'shell.execute_reply': '2021-04-23T17:47:31.110099Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.130111', 'end_time': '2021-04-23T17:47:31.177175', 'duration': 0.047064, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.158808Z', 'iopub.execute_input': '2021-04-23T17:47:31.173446Z', 'iopub.status.idle': '2021-04-23T17:47:31.176667Z', 'shell.execute_reply': '2021-04-23T17:47:31.177042Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"3bc4f04d-af2f-46d6-b40b-cb859108578b\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"61ca871f-1277-4102-8b77-7e6c9e2640b2\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1073\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1072\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1075\"},\"field\":\"max\",\"formatter\":{\"id\":\"1074\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1069\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1068\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"Selection\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{\"editor\":{\"id\":\"1071\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1070\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1083\"},\"field\":\"min\",\"formatter\":{\"id\":\"1082\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringEditor\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQJqZmZmZmTBAw/UoXI/C8T8=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAAfhetRuH5YQM3MzMzMTC1AAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQMP1KFyPQi9AAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQBSuR+F6VDBApHA9Ctej8D8=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQM3MzMzMjDBAmpmZmZmZ8T8=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1067\"},\"selection_policy\":{\"id\":\"1066\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1077\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1076\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringEditor\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{\"editor\":{\"id\":\"1079\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1078\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringFormatter\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{\"editor\":{\"id\":\"1081\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1080\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringEditor\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"61ca871f-1277-4102-8b77-7e6c9e2640b2\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"3bc4f04d-af2f-46d6-b40b-cb859108578b\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.198009', 'end_time': '2021-04-23T17:47:31.238098', 'duration': 0.040089, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.232386Z', 'iopub.execute_input': '2021-04-23T17:47:31.233114Z', 'shell.execute_reply': '2021-04-23T17:47:31.237583Z', 'iopub.status.idle': '2021-04-23T17:47:31.237991Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.259624', 'end_time': '2021-04-23T17:47:31.290907', 'duration': 0.031283, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.288668Z', 'iopub.execute_input': '2021-04-23T17:47:31.289166Z', 'iopub.status.idle': '2021-04-23T17:47:31.290376Z', 'shell.execute_reply': '2021-04-23T17:47\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m :31.290746Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.312553', 'end_time': '2021-04-23T17:47:31.343957', 'duration': 0.031404, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.341715Z', 'iopub.execute_input': '2021-04-23T17:47:31.342211Z', 'iopub.status.idle': '2021-04-23T17:47:31.343442Z', 'shell.execute_reply': '2021-04-23T17:47:31.343823Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.365829', 'end_time': '2021-04-23T17:47:31.387390', 'duration': 0.021561, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.408893', 'end_time': '2021-04-23T17:47:31.437258', 'duration': 0.028365, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.435016Z', 'iopub.execute_input': '2021-04-23T17:47:31.435543Z', 'iopub.status.idle': '2021-04-23T17:47:31.436757Z', 'shell.execute_reply': '2021-04-23T17:47:31.437128Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.458954', 'end_time': '2021-04-23T17:47:31.486970', 'duration': 0.028016, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.484669Z', 'iopub.execute_input': '2021-04-23T17:47:31.485162Z', 'shell.execute_reply': '2021-04-23T17:47:31.486367Z', 'iopub.status.idle': '2021-04-23T17:47:31.486863Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.508740', 'end_time': '2021-04-23T17:47:31.547032', 'duration': 0.038292, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.536051Z', 'iopub.execute_input': '2021-04-23T17:47:31.536532Z', 'shell.execute_reply': '2021-04-23T17:47:31.546402Z', 'iopub.status.idle': '2021-04-23T17:47:31.546920Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the IOBottleneck rule\\nwas the most frequently triggered. It processed 23 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>23</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>17</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>23</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n    su\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m mmary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.569865', 'end_time': '2021-04-23T17:47:31.601393', 'duration': 0.031528, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.598491Z', 'iopub.execute_input': '2021-04-23T17:47:31.599049Z', 'iopub.status.idle': '2021-04-23T17:47:31.600893Z', 'shell.execute_reply': '2021-04-23T17:47:31.601264Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.624921', 'end_time': '2021-04-23T17:47:31.654418', 'duration': 0.029497, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.651725Z', 'iopub.execute_input': '2021-04-23T17:47:31.652303Z', 'iopub.status.idle': '2021-04-23T17:47:31.653871Z', 'shell.execute_reply': '2021-04-23T17:47:31.654284Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.677830', 'end_time': '2021-04-23T17:47:31.732628', 'duration': 0.054798, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.709051Z', 'iopub.execute_input': '2021-04-23T17:47:31.719880Z', 'iopub.status.idle': '2021-04-23T17:47:31.732113Z', 'shell.execute_reply': '2021-04-23T17:47:31.732491Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"51d2ae0c-84dc-47eb-8756-a970a9975aab\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"42903502-afc8-486c-822e-c6f955985def\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"42903502-afc8-486c-822e-c6f955985def\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"51d2ae0c-84dc-47eb-8756-a970a9975aab\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.757673', 'end_time': '2021-04-23T17:47:31.806926', 'duration': 0.049253, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.796429Z', 'iopub.execute_input': '2021-04-23T17:47:31.797282Z', 'shell.execute_reply': '2021-04-23T17:47:31.806378Z', 'iopub.status.idle': '2021-04-23\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m T17:47:31.806814Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"2e777799-6fb3-4f3b-927c-31ab95911d79\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"8efe041c-a834-4c8f-a8b5-359b92fadb90\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"8efe041c-a834-4c8f-a8b5-359b92fadb90\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"2e777799-6fb3-4f3b-927c-31ab95911d79\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.833207', 'end_time': '2021-04-23T17:47:31.879915', 'duration': 0.046708, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.868826Z', 'iopub.execute_input': '2021-04-23T17:47:31.871210Z', 'shell.execute_reply': '2021-04-23T17:47:31.879362Z', 'iopub.status.idle': '2021-04-23T17:47:31.879806Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"2b73e398-f510-4079-a301-6fc3101153d1\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"3195e717-cbb5-4df3-8f9b-bdddcf3adcfa\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"3195e717-cbb5-4df3-8f9b-bdddcf3adcfa\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"2b73e398-f510-4079-a301-6fc3101153d1\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.907833', 'end_time': '2021-04-23T17:47:31.961140', 'duration': 0.053307, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:31.946495Z', 'iopub.execute_input': '2021-04-23T17:47:31.947371Z', 'shell.execute_reply': '2021-04-23T17:47:31.960619Z', 'iopub.status.idle': '2021-04-23T17:47:31.961032Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"ab5bb6dd-00a4-4aa3-b1ea-8efb5d4be53f\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"af14e05b-8608-4029-8ad7-6a681fa71913\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"wid\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m th\":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"af14e05b-8608-4029-8ad7-6a681fa71913\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"ab5bb6dd-00a4-4aa3-b1ea-8efb5d4be53f\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"942261e6-622e-49ff-b54e-4ab7a58f0e96\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"4e3a2fd6-e993-484f-8549-732f6062c4e8\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"4e3a2fd6-e993-484f-8549-732f6062c4e8\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"942261e6-622e-49ff-b54e-4ab7a58f0e96\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:31.990737', 'end_time': '2021-04-23T17:47:32.042075', 'duration': 0.051338, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:32.032578Z', 'iopub.execute_input': '2021-04-23T17:47:32.033402Z', 'iopub.status.idle': '2021-04-23T17:47:32.041560Z', 'shell.execute_reply': '2021-04-23T17:47:32.041938Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"5f4ae6d9-3fe6-48af-a5d9-8776c209d296\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"9825342e-2581-48c3-af4a-ed39fc739711\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 17 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"9825342e-2581-48c3-af4a-ed39fc739711\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"5f4ae6d9-3fe6-48af-a5d9-8776c209d296\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:32.073117', 'end_time': '2021-04-23T17:47:32.131739', 'duration': 0.058622, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:32.122067Z', 'iopub.execute_input': '2021-04-23T17:47:32.122989Z', 'shell.execute_reply': '2021-04-23T17:47:32.131219Z', 'iopub.status.idle': '2021-04-23T17:47:32.131630Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"b7200cab-65cd-4070-a0ad-e579e32ffb03\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"167fcbd9-3645-4fb8-b527-41cd17b97880\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 23 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"167fcbd9-3645-4fb8-b527-41cd17b97880\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"b7200cab-65cd-4070-a0ad-e579e32ffb03\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:32.164065', 'end_time': '2021-04-23T17:47:32.222705', 'duration': 0.05864, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:32.214208Z', 'iopub.execute_input': '2021-04-23T17:47:32.214798Z', 'iopub.status.idle': '2021-04-23T17:47:32.222179Z', 'shell.execute_reply': '2021-04-23T17:47:32.222564Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8e8cec1e-603d-43b1-9af2-5cb636528943\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"8bb467a6-beb5-4e1b-bf0e-d6438ebea273\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 23 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"8bb467a6-beb5-4e1b-bf0e-d6438ebea273\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"8e8cec1e-603d-43b1-9af2-5cb636528943\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n        \r\n",
      "                                    height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:32.256795', 'end_time': '2021-04-23T17:47:32.312541', 'duration': 0.055746, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:32.304244Z', 'iopub.execute_input': '2021-04-23T17:47:32.304797Z', 'shell.execute_reply': '2021-04-23T17:47:32.312025Z', 'iopub.status.idle': '2021-04-23T17:47:32.312433Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"14ca35c1-e1b4-43ea-a57a-52ebdb1a046d\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"8e7489b2-e864-4b32-8059-416e04135c0c\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"8e7489b2-e864-4b32-8059-416e04135c0c\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"14ca35c1-e1b4-43ea-a57a-52ebdb1a046d\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T17:47:28.772565', 'end_time': '2021-04-23T17:47:32.754495', 'duration': 3.98193, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:32.826 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:32.826 ip-10-2-238-48.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:33.233 ip-10-2-238-48.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m [2021-04-23 17:47:33.233 ip-10-2-238-48.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-MKXLCKLZQL-ProfilerReport-1619199802-964c323b/algo-1-1619199995\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:25,  1.13cell/s]#015Executing:   7%|         | 2/30 [00:01<00:23,  1.18cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.76cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.55cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.23cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01, 10.01cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 11.10cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.58cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.84cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 15.41cell/s]#015Executing:  77%|  | 23/30 [00:02<00:00, 15.31cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.69cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 13.84cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.83cell/s]#015Executing: 100%|| 30/30 [00:03<00:00,  7.53cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.546 ip-10-0-143-30.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.546 ip-10-0-143-30.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.546 ip-10-0-143-30.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.546 ip-10-0-143-30.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.546 ip-10-0-143-30.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.546 ip-10-0-143-30.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.546 ip-10-0-143-30.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.547 ip-10-0-143-30.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.547 ip-10-0-143-30.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.547 ip-10-0-143-30.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.547 ip-10-0-143-30.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.547 ip-10-0-143-30.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.547 ip-10-0-143-30.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.547 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.547 ip-10-0-143-30.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.548 ip-10-0-143-30.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.588 ip-10-0-143-30.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.588 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.589 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.591 ip-10-0-143-30.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.591 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.591 ip-10-0-143-30.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.592 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.592 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.594 ip-10-0-143-30.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.594 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.595 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.595 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.596 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.600 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:34.600 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.625 ip-10-2-77-202.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.625 ip-10-2-77-202.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.626 ip-10-2-77-202.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.626 ip-10-2-77-202.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.626 ip-10-2-77-202.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.626 ip-10-2-77-202.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.626 ip-10-2-77-202.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.626 ip-10-2-77-202.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.626 ip-10-2-77-202.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.626 ip-10-2-77-202.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.626 ip-10-2-77-202.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.626 ip-10-2-77-202.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.626 ip-10-2-77-202.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.627 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.627 ip-10-2-77-202.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.628 ip-10-2-77-202.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.665 ip-10-2-77-202.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.665 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.666 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.668 ip-10-2-77-202.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.668 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.668 ip-10-2-77-202.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.668 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.669 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.670 ip-10-2-77-202.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.671 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.671 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.671 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.672 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.675 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:35.676 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:32.581 ip-10-2-93-108.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199813\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:33.078 ip-10-2-93-108.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:36.436 ip-10-0-235-57.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199820\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:36.920 ip-10-0-235-57.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:38.951 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:35.651025', 'end_time': '2021-04-23T17:47:35.670341', 'duration': 0.019316, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:35.688640', 'end_time': '2021-04-23T17:47:36.524737', 'duration': 0.836097, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:35.713845Z', 'iopub.execute_input': '2021-04-23T17:47:35.714358Z', 'iopub.status.idle': '2021-04-23T17:47:36.524098Z', 'shell.execute_reply': '2021-04-23T17:47:36.524505Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 17:47:36.516 ip-10-0-143-30.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199805\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:36.545321', 'end_time': '2021-04-23T17:47:36.761769', 'duration': 0.216448, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:36.568325Z', 'iopub.execute_input': '2021-04-23T17:47:36.568845Z', 'shell.execute_reply': '2021-04-23T17:47:36.761254Z', 'iopub.status.idle': '2021-04-23T17:47:36.761651Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n    \r\n",
      "  if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:36.781417', 'end_time': '2021-04-23T17:47:36.809654', 'duration': 0.028237, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:36.807432Z', 'iopub.execute_input': '2021-04-23T17:47:36.807960Z', 'shell.execute_reply': '2021-04-23T17:47:36.809126Z', 'iopub.status.idle': '2021-04-23T17:47:36.809540Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:36.829161', 'end_time': '2021-04-23T17:47:36.854388', 'duration': 0.025227, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:36.852154Z', 'iopub.execute_input': '2021-04-23T17:47:36.852645Z', 'shell.execute_reply': '2021-04-23T17:47:36.853837Z', 'iopub.status.idle': '2021-04-23T17:47:36.854269Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:36.873673', 'end_time': '2021-04-23T17:47:36.893145', 'duration': 0.019472, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:36.912831', 'end_time': '2021-04-23T17:47:36.938055', 'duration': 0.025224, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:36.935708Z', 'iopub.execute_input': '2021-04-23T17:47:36.936204Z', 'iopub.status.idle': '2021-04-23T17:47:36.937514Z', 'shell.execute_reply': '2021-04-23T17:47:36.937919Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:36.957414', 'end_time': '2021-04-23T17:47:36.990292', 'duration': 0.032878, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:36.986334Z', 'iopub.execute_input': '2021-04-23T17:47:36.988009Z', 'shell.execute_reply': '2021-04-23T17:47:36.989764Z', 'iopub.status.idle': '2021-04-23T17:47:36.990179Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.010089', 'end_time': '2021-04-23T17:47:37.054114', 'duration': 0.044025, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.040670Z', 'iopub.execute_input': '2021-04-23T17:47:37.041351Z', 'shell.execute_reply': '2021-04-23T17:47:37.053568Z', 'iopub.status.idle': '2021-04-23T17:47:37.054000Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"7e7a9cb3-e8e6-4c63-9692-b0e082205e01\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"157557c9-329b-4b45-92d5-c7a3efc96305\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1010\"},\"field\":\"0\",\"formatter\":{\"id\":\"1009\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 17:46:13 and ran for 46 seconds. \\\\n Your training job started on 04/23/2021 at 17:46:13 and ran for 46 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"Selection\"},{\"attributes\":{\"editor\":{\"id\":\"1012\"},\"field\":\"1\",\"formatter\":{\"id\":\"1011\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringEditor\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"StringEditor\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"17:46:13 04/23/2021\",\"17:46:59 04/23/2021\",\"46 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1014\"},\"selection_policy\":{\"id\":\"1013\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"157557c9-329b-4b45-92d5-c7a3efc96305\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"7e7a9cb3-e8e6-4c63-9692-b0e082205e01\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.074691', 'end_time': '2021-04-23T17:47:37.095212', 'duration': 0.020521, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.115728', 'end_time': '2021-04-23T17:47:37.141762', 'duration': 0.026034, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.139219Z', 'iopub.execute_input': '2021-04-23T17:47:37.139754Z', 'shell.execute_reply': '2021-04-23T17:47:37.141240Z', 'iopub.status.idle': '2021-04-23T17:47:37.141652Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-inp\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m ut'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.162417', 'end_time': '2021-04-23T17:47:37.192393', 'duration': 0.029976, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.190080Z', 'iopub.execute_input': '2021-04-23T17:47:37.190602Z', 'iopub.status.idle': '2021-04-23T17:47:37.191877Z', 'shell.execute_reply': '2021-04-23T17:47:37.192257Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.213445', 'end_time': '2021-04-23T17:47:37.262759', 'duration': 0.049314, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.243645Z', 'iopub.execute_input': '2021-04-23T17:47:37.244377Z', 'shell.execute_reply': '2021-04-23T17:47:37.262222Z', 'iopub.status.idle': '2021-04-23T17:47:37.262644Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"4dd40c9e-9455-4f48-81ee-4b7318937f1b\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"8b456701-ebcf-4a0a-bee6-682f3ffa8937\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1067\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1066\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQB+F61G4XjNAH4XrUbi+WEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAAK16NwPUpJQKRwPQrXoy1AAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAMBXQGZmZmZmZjFAUrgehetR8D8=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQHsUrkfh+jJAXI/C9Sj8V0A=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQI/C9ShcTzNAPQrXo3BNWEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1083\"},\"selection_policy\":{\"id\":\"1082\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"editor\":{\"id\":\"1079\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1078\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1073\"},\"field\":\"max\",\"formatter\":{\"id\":\"1072\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"Selection\"},{\"attributes\":{\"editor\":{\"id\":\"1069\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1068\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringFormatter\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1071\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1070\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1081\"},\"field\":\"min\",\"formatter\":{\"id\":\"1080\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringEditor\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringFormatter\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringEditor\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{\"editor\":{\"id\":\"1077\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1076\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1075\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1074\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringFormatter\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"8b456701-ebcf-4a0a-bee6-682f3ffa8937\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"4dd40c9e-9455-4f48-81ee-4b7318937f1b\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.284422', 'end_time': '2021-04-23T17:47:37.325993', 'duration': 0.041571, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.308291Z', 'iopub.execute_input': '2021-04-23T17:47:37.320341Z', 'iopub.status.idle': '2021-04-23T17:47:37.325464Z', 'shell.execute_reply': '2021-04-23T17:47:37.325849Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.348505', 'end_time': '2021-04-23T17:47:37.381156', 'duration': 0.032651, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.378919Z', 'iopub.execute_input': '2021-04-23T17:47:37.379493Z', 'iopub.status.idle': '2021-04-23T17:47:37.380615Z', 'shell.execute_reply': '2021-04-23T17\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m :47:37.381011Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.403715', 'end_time': '2021-04-23T17:47:37.436560', 'duration': 0.032845, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.434231Z', 'iopub.execute_input': '2021-04-23T17:47:37.434802Z', 'iopub.status.idle': '2021-04-23T17:47:37.436036Z', 'shell.execute_reply': '2021-04-23T17:47:37.436419Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.459229', 'end_time': '2021-04-23T17:47:37.481574', 'duration': 0.022345, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.504085', 'end_time': '2021-04-23T17:47:37.533696', 'duration': 0.029611, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.530657Z', 'iopub.execute_input': '2021-04-23T17:47:37.531364Z', 'iopub.status.idle': '2021-04-23T17:47:37.533178Z', 'shell.execute_reply': '2021-04-23T17:47:37.533558Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.556396', 'end_time': '2021-04-23T17:47:37.585621', 'duration': 0.029225, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.583354Z', 'iopub.execute_input': '2021-04-23T17:47:37.583894Z', 'iopub.status.idle': '2021-04-23T17:47:37.585111Z', 'shell.execute_reply': '2021-04-23T17:47:37.585486Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.608836', 'end_time': '2021-04-23T17:47:37.648638', 'duration': 0.039802, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.637086Z', 'iopub.execute_input': '2021-04-23T17:47:37.637588Z', 'iopub.status.idle': '2021-04-23T17:47:37.648107Z', 'shell.execute_reply': '2021-04-23T17:47:37.648494Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the StepOutlier rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>93</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>98</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>98</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n    s\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m ummary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.672414', 'end_time': '2021-04-23T17:47:37.704027', 'duration': 0.031613, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.700973Z', 'iopub.execute_input': '2021-04-23T17:47:37.701479Z', 'iopub.status.idle': '2021-04-23T17:47:37.703471Z', 'shell.execute_reply': '2021-04-23T17:47:37.703891Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.728355', 'end_time': '2021-04-23T17:47:37.757982', 'duration': 0.029627, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.755765Z', 'iopub.execute_input': '2021-04-23T17:47:37.756266Z', 'iopub.status.idle': '2021-04-23T17:47:37.757431Z', 'shell.execute_reply': '2021-04-23T17:47:37.757847Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.782296', 'end_time': '2021-04-23T17:47:37.837446', 'duration': 0.05515, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.824988Z', 'iopub.execute_input': '2021-04-23T17:47:37.829095Z', 'shell.execute_reply': '2021-04-23T17:47:37.836899Z', 'iopub.status.idle': '2021-04-23T17:47:37.837328Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"6ee8dc62-d4fc-466d-a3e4-c3713edb704a\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"d3403ea0-bb50-4d3c-84f9-f04029258c7e\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"d3403ea0-bb50-4d3c-84f9-f04029258c7e\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"6ee8dc62-d4fc-466d-a3e4-c3713edb704a\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.863556', 'end_time': '2021-04-23T17:47:37.913193', 'duration': 0.049637, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.904150Z', 'iopub.execute_input': '2021-04-23T17:47:37.904761Z', 'shell.execute_reply': '2021-04-23T17:47:37.912645Z', 'iopub.status.idle': '2021-04-23\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m T17:47:37.913075Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8009ab7b-4657-4dfa-98df-66e277b1b85e\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"57079b92-c3b0-48d0-8ddd-ae26b7ac1faa\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"57079b92-c3b0-48d0-8ddd-ae26b7ac1faa\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"8009ab7b-4657-4dfa-98df-66e277b1b85e\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.940743', 'end_time': '2021-04-23T17:47:37.987630', 'duration': 0.046887, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.977448Z', 'iopub.execute_input': '2021-04-23T17:47:37.979529Z', 'iopub.status.idle': '2021-04-23T17:47:37.987048Z', 'shell.execute_reply': '2021-04-23T17:47:37.987479Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"0bddc4db-0996-4573-820b-a25cef9ce6f0\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"c0b2c8d6-ef34-40a5-a7e5-34948c218d1e\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"c0b2c8d6-ef34-40a5-a7e5-34948c218d1e\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"0bddc4db-0996-4573-820b-a25cef9ce6f0\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.016743', 'end_time': '2021-04-23T17:47:38.071936', 'duration': 0.055193, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.058598Z', 'iopub.execute_input': '2021-04-23T17:47:38.059207Z', 'iopub.status.idle': '2021-04-23T17:47:38.071404Z', 'shell.execute_reply': '2021-04-23T17:47:38.071795Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"459f2c77-fe2a-4327-9444-70740496eae7\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"476fe925-5628-416e-bbb2-40b63d35c9f9\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"wid\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m th\":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"476fe925-5628-416e-bbb2-40b63d35c9f9\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"459f2c77-fe2a-4327-9444-70740496eae7\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"f3ed0c08-c04e-4dc7-83da-31e769b0dc38\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"82ebcd13-d4d9-4d9d-93f8-3f9aa8fbfc69\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"82ebcd13-d4d9-4d9d-93f8-3f9aa8fbfc69\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"f3ed0c08-c04e-4dc7-83da-31e769b0dc38\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.102732', 'end_time': '2021-04-23T17:47:38.154609', 'duration': 0.051877, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.144017Z', 'iopub.execute_input': '2021-04-23T17:47:38.146450Z', 'shell.execute_reply': '2021-04-23T17:47:38.154006Z', 'iopub.status.idle': '2021-04-23T17:47:38.154496Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"6e6b666e-391f-4cae-8a9f-8bd7a5b72f67\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"68be114a-cba0-4349-b50f-a7d0b74401e8\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 93 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"68be114a-cba0-4349-b50f-a7d0b74401e8\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"6e6b666e-391f-4cae-8a9f-8bd7a5b72f67\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.186938', 'end_time': '2021-04-23T17:47:38.247118', 'duration': 0.06018, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.235013Z', 'iopub.execute_input': '2021-04-23T17:47:38.238289Z', 'iopub.status.idle': '2021-04-23T17:47:38.246595Z', 'shell.execute_reply': '2021-04-23T17:47:38.246975Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8678d85d-e956-4987-b857-35ff09cb23ee\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"a6dc4d4f-af2d-46e8-98fe-c4756246bf40\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 98 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"a6dc4d4f-af2d-46e8-98fe-c4756246bf40\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"8678d85d-e956-4987-b857-35ff09cb23ee\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (a\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m ttempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.280776', 'end_time': '2021-04-23T17:47:38.341475', 'duration': 0.060699, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.329896Z', 'iopub.execute_input': '2021-04-23T17:47:38.333452Z', 'iopub.status.idle': '2021-04-23T17:47:38.340949Z', 'shell.execute_reply': '2021-04-23T17:47:38.341333Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"9b583a6d-0aa9-4f02-89e8-ad1e192603c9\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"244cf7c2-9229-40e6-ac40-0638cc507f0e\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 98 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"244cf7c2-9229-40e6-ac40-0638cc507f0e\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"9b583a6d-0aa9-4f02-89e8-ad1e192603c9\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n        \r\n",
      "                                    height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.376524', 'end_time': '2021-04-23T17:47:38.435321', 'duration': 0.058797, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.426665Z', 'iopub.execute_input': '2021-04-23T17:47:38.427238Z', 'shell.execute_reply': '2021-04-23T17:47:38.434788Z', 'iopub.status.idle': '2021-04-23T17:47:38.435208Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"e790e756-aad3-4c5d-a3da-3a1234c75e62\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"d97884db-150c-4ad7-999e-55296d342231\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"d97884db-150c-4ad7-999e-55296d342231\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"e790e756-aad3-4c5d-a3da-3a1234c75e62\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T17:47:34.627994', 'end_time': '2021-04-23T17:47:38.877514', 'duration': 4.24952, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:38.952 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:38.952 ip-10-0-143-30.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:39.698 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:36.545528', 'end_time': '2021-04-23T17:47:36.563647', 'duration': 0.018119, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:36.580773', 'end_time': '2021-04-23T17:47:37.382901', 'duration': 0.802128, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:36.604710Z', 'iopub.execute_input': '2021-04-23T17:47:36.605193Z', 'iopub.status.idle': '2021-04-23T17:47:37.382261Z', 'shell.execute_reply': '2021-04-23T17:47:37.382652Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 17:47:37.375 ip-10-2-77-202.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199810\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.402170', 'end_time': '2021-04-23T17:47:37.612235', 'duration': 0.210065, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.424259Z', 'iopub.execute_input': '2021-04-23T17:47:37.424781Z', 'iopub.status.idle': '2021-04-23T17:47:37.611687Z', 'shell.execute_reply': '2021-04-23T17:47:37.612091Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n    \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.630797', 'end_time': '2021-04-23T17:47:37.657481', 'duration': 0.026684, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.655334Z', 'iopub.execute_input': '2021-04-23T17:47:37.655821Z', 'iopub.status.idle': '2021-04-23T17:47:37.656962Z', 'shell.execute_reply': '2021-04-23T17:47:37.657340Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.675443', 'end_time': '2021-04-23T17:47:37.699049', 'duration': 0.023606, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.696899Z', 'iopub.execute_input': '2021-04-23T17:47:37.697361Z', 'shell.execute_reply': '2021-04-23T17:47:37.698526Z', 'iopub.status.idle': '2021-04-23T17:47:37.698944Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.717064', 'end_time': '2021-04-23T17:47:37.734906', 'duration': 0.017842, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.753112', 'end_time': '2021-04-23T17:47:37.776549', 'duration': 0.023437, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.774404Z', 'iopub.execute_input': '2021-04-23T17:47:37.774897Z', 'shell.execute_reply': '2021-04-23T17:47:37.776056Z', 'iopub.status.idle': '2021-04-23T17:47:37.776449Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.794718', 'end_time': '2021-04-23T17:47:37.825899', 'duration': 0.031181, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.821012Z', 'iopub.execute_input': '2021-04-23T17:47:37.822163Z', 'shell.execute_reply': '2021-04-23T17:47:37.825385Z', 'iopub.status.idle': '2021-04-23T17:47:37.825790Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.844334', 'end_time': '2021-04-23T17:47:37.885133', 'duration': 0.040799, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.875818Z', 'iopub.execute_input': '2021-04-23T17:47:37.876322Z', 'iopub.status.idle': '2021-04-23T17:47:37.884630Z', 'shell.execute_reply': '2021-04-23T17:47:37.885003Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"7b1385de-6046-4bb2-9ab1-67181a519116\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"854023c7-a942-4b89-bd02-248af29758aa\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"Selection\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{\"editor\":{\"id\":\"1009\"},\"field\":\"0\",\"formatter\":{\"id\":\"1010\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"StringEditor\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"editor\":{\"id\":\"1011\"},\"field\":\"1\",\"formatter\":{\"id\":\"1012\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 17:46:18 and ran for 41 seconds. \\\\n Your training job started on 04/23/2021 at 17:46:18 and ran for 41 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"17:46:18 04/23/2021\",\"17:46:59 04/23/2021\",\"41 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1013\"},\"selection_policy\":{\"id\":\"1014\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringFormatter\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"854023c7-a942-4b89-bd02-248af29758aa\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"7b1385de-6046-4bb2-9ab1-67181a519116\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.904468', 'end_time': '2021-04-23T17:47:37.923523', 'duration': 0.019055, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.942626', 'end_time': '2021-04-23T17:47:37.969164', 'duration': 0.026538, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:37.966542Z', 'iopub.execute_input': '2021-04-23T17:47:37.967080Z', 'iopub.status.idle': '2021-04-23T17:47:37.968654Z', 'shell.execute_reply': '2021-04-23T17:47:37.969035Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-inp\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m ut'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:37.988627', 'end_time': '2021-04-23T17:47:38.017214', 'duration': 0.028587, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.013517Z', 'iopub.execute_input': '2021-04-23T17:47:38.015069Z', 'iopub.status.idle': '2021-04-23T17:47:38.016715Z', 'shell.execute_reply': '2021-04-23T17:47:38.017084Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.036898', 'end_time': '2021-04-23T17:47:38.083435', 'duration': 0.046537, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.071222Z', 'iopub.execute_input': '2021-04-23T17:47:38.079847Z', 'iopub.status.idle': '2021-04-23T17:47:38.082927Z', 'shell.execute_reply': '2021-04-23T17:47:38.083305Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"132a2ee9-4719-4a6d-acf7-1c00abe12207\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"ced5121b-96a2-4d9c-8dda-47e814724817\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1074\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1075\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringEditor\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1072\"},\"field\":\"max\",\"formatter\":{\"id\":\"1073\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1076\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1077\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1080\"},\"field\":\"min\",\"formatter\":{\"id\":\"1081\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringEditor\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{\"editor\":{\"id\":\"1070\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1071\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"editor\":{\"id\":\"1068\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1069\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQArXo3A9yjJAPQrXo3B9WEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAABI4XoUrodKQAAAAAAAgCpAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAADhehSuR8FYQAAAAAAAADFAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQK5H4XoUrjJAUrgeheuRVUA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQIXrUbgexTJA16NwPQrnV0A=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1082\"},\"selection_policy\":{\"id\":\"1083\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1078\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1079\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringFormatter\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1066\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1067\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"ced5121b-96a2-4d9c-8dda-47e814724817\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"132a2ee9-4719-4a6d-acf7-1c00abe12207\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.103687', 'end_time': '2021-04-23T17:47:38.143765', 'duration': 0.040078, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.136397Z', 'iopub.execute_input': '2021-04-23T17:47:38.138306Z', 'shell.execute_reply': '2021-04-23T17:47:38.143245Z', 'iopub.status.idle': '2021-04-23T17:47:38.143656Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.165457', 'end_time': '2021-04-23T17:47:38.195685', 'duration': 0.030228, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.193497Z', 'iopub.execute_input': '2021-04-23T17:47:38.193979Z', 'shell.execute_reply': '2021-04-23T17:47:38.195162Z', 'iopub.status.idle': '2021-04-23T17\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m :47:38.195582Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.216573', 'end_time': '2021-04-23T17:47:38.246929', 'duration': 0.030356, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.244769Z', 'iopub.execute_input': '2021-04-23T17:47:38.245256Z', 'iopub.status.idle': '2021-04-23T17:47:38.246414Z', 'shell.execute_reply': '2021-04-23T17:47:38.246780Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.268022', 'end_time': '2021-04-23T17:47:38.288744', 'duration': 0.020722, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.309635', 'end_time': '2021-04-23T17:47:38.337286', 'duration': 0.027651, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.335110Z', 'iopub.execute_input': '2021-04-23T17:47:38.335621Z', 'iopub.status.idle': '2021-04-23T17:47:38.336752Z', 'shell.execute_reply': '2021-04-23T17:47:38.337154Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.358281', 'end_time': '2021-04-23T17:47:38.385386', 'duration': 0.027105, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.383190Z', 'iopub.execute_input': '2021-04-23T17:47:38.383678Z', 'shell.execute_reply': '2021-04-23T17:47:38.384887Z', 'iopub.status.idle': '2021-04-23T17:47:38.385283Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.406508', 'end_time': '2021-04-23T17:47:38.443829', 'duration': 0.037321, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.432921Z', 'iopub.execute_input': '2021-04-23T17:47:38.433410Z', 'iopub.status.idle': '2021-04-23T17:47:38.443287Z', 'shell.execute_reply': '2021-04-23T17:47:38.443691Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the StepOutlier rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>95</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>81</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>95</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n    s\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m ummary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.466236', 'end_time': '2021-04-23T17:47:38.496508', 'duration': 0.030272, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.493558Z', 'iopub.execute_input': '2021-04-23T17:47:38.494078Z', 'iopub.status.idle': '2021-04-23T17:47:38.496008Z', 'shell.execute_reply': '2021-04-23T17:47:38.496379Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.519514', 'end_time': '2021-04-23T17:47:38.547470', 'duration': 0.027956, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.545290Z', 'iopub.execute_input': '2021-04-23T17:47:38.545773Z', 'shell.execute_reply': '2021-04-23T17:47:38.546888Z', 'iopub.status.idle': '2021-04-23T17:47:38.547368Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.570149', 'end_time': '2021-04-23T17:47:38.622309', 'duration': 0.05216, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.613815Z', 'iopub.execute_input': '2021-04-23T17:47:38.614377Z', 'shell.execute_reply': '2021-04-23T17:47:38.621793Z', 'iopub.status.idle': '2021-04-23T17:47:38.622202Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"ee9f648d-fdc4-4743-9f19-cdd05773665d\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"edcf6b60-f182-4b31-ac6c-497f05591585\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"edcf6b60-f182-4b31-ac6c-497f05591585\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"ee9f648d-fdc4-4743-9f19-cdd05773665d\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.646392', 'end_time': '2021-04-23T17:47:38.692953', 'duration': 0.046561, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.681948Z', 'iopub.execute_input': '2021-04-23T17:47:38.684648Z', 'iopub.status.idle': '2021-04-23T17:47:38.692426Z', 'shell.execute_reply': '2021-04-23\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m T17:47:38.692816Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"28ecf3fb-d4fb-4a26-af8a-b705c783e37f\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"40143303-ca10-4af3-8654-a2f4a7a7a149\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"40143303-ca10-4af3-8654-a2f4a7a7a149\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"28ecf3fb-d4fb-4a26-af8a-b705c783e37f\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.718617', 'end_time': '2021-04-23T17:47:38.762183', 'duration': 0.043566, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.752814Z', 'iopub.execute_input': '2021-04-23T17:47:38.754941Z', 'iopub.status.idle': '2021-04-23T17:47:38.761676Z', 'shell.execute_reply': '2021-04-23T17:47:38.762050Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"6e128c94-8b95-4322-8436-0504a254fdc6\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"c83e8136-0c92-494a-9aab-9bee3c04f6d9\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"c83e8136-0c92-494a-9aab-9bee3c04f6d9\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"6e128c94-8b95-4322-8436-0504a254fdc6\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.789549', 'end_time': '2021-04-23T17:47:38.841036', 'duration': 0.051487, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.827734Z', 'iopub.execute_input': '2021-04-23T17:47:38.828271Z', 'iopub.status.idle': '2021-04-23T17:47:38.840517Z', 'shell.execute_reply': '2021-04-23T17:47:38.840899Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d578a810-fe3e-44f3-888c-4357c4ef37dc\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"adc830be-f4e8-44a7-bd0f-48d828162a0a\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"wid\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m th\":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"adc830be-f4e8-44a7-bd0f-48d828162a0a\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"d578a810-fe3e-44f3-888c-4357c4ef37dc\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"3a295eb4-2184-43a4-8f75-43385931d08f\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"c00a90e0-ae87-42e1-a6a7-961266da10da\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"c00a90e0-ae87-42e1-a6a7-961266da10da\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"3a295eb4-2184-43a4-8f75-43385931d08f\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.870062', 'end_time': '2021-04-23T17:47:38.920685', 'duration': 0.050623, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.906495Z', 'iopub.execute_input': '2021-04-23T17:47:38.912227Z', 'iopub.status.idle': '2021-04-23T17:47:38.920165Z', 'shell.execute_reply': '2021-04-23T17:47:38.920549Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"7f56b61c-1234-48e4-91c0-94df065d78df\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"080c93d0-a8c3-4cfb-b645-54078efa8237\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 81 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"080c93d0-a8c3-4cfb-b645-54078efa8237\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"7f56b61c-1234-48e4-91c0-94df065d78df\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:38.950752', 'end_time': '2021-04-23T17:47:39.009169', 'duration': 0.058417, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:38.996624Z', 'iopub.execute_input': '2021-04-23T17:47:39.000396Z', 'shell.execute_reply': '2021-04-23T17:47:39.008637Z', 'iopub.status.idle': '2021-04-23T17:47:39.009057Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"3e4b2476-e233-4fa3-821d-86b0bb646583\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"3ebd3a21-292c-4128-ad0e-b676e1c507e1\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 95 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"3ebd3a21-292c-4128-ad0e-b676e1c507e1\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"3e4b2476-e233-4fa3-821d-86b0bb646583\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:39.040638', 'end_time': '2021-04-23T17:47:39.100475', 'duration': 0.059837, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:39.079917Z', 'iopub.execute_input': '2021-04-23T17:47:39.091299Z', 'iopub.status.idle': '2021-04-23T17:47:39.099946Z', 'shell.execute_reply': '2021-04-23T17:47:39.100332Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"a7b7d826-70ce-4e60-a546-a62357d4eb00\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"f852a34e-a07d-455e-8a78-98e84f05b0ad\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 95 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"f852a34e-a07d-455e-8a78-98e84f05b0ad\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"a7b7d826-70ce-4e60-a546-a62357d4eb00\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n       \r\n",
      "                                     height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:39.133268', 'end_time': '2021-04-23T17:47:39.189971', 'duration': 0.056703, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:39.180428Z', 'iopub.execute_input': '2021-04-23T17:47:39.181296Z', 'iopub.status.idle': '2021-04-23T17:47:39.189450Z', 'shell.execute_reply': '2021-04-23T17:47:39.189833Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d1735248-bc99-4868-a578-ee7d3bf9957a\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"19dda6ca-f26e-4594-80fe-2f49f6ff92fd\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"19dda6ca-f26e-4594-80fe-2f49f6ff92fd\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"d1735248-bc99-4868-a578-ee7d3bf9957a\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T17:47:35.701518', 'end_time': '2021-04-23T17:47:39.630096', 'duration': 3.928578, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:39.699 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:39.699 ip-10-2-77-202.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:39.373 ip-10-0-143-30.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619200080000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m [2021-04-23 17:47:39.373 ip-10-0-143-30.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-BKQCPKGZYR-ProfilerReport-1619199805-69e87830/algo-1-1619200010\u001b[0m #015Executing:   3%|         | 1/30 [00:01<00:30,  1.06s/cell]#015Executing:   7%|         | 2/30 [00:01<00:26,  1.06cell/s]#015Executing:  10%|         | 3/30 [00:02<00:16,  1.62cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.20cell/s]#015Executing:  30%|       | 9/30 [00:02<00:03,  6.72cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01,  9.37cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 10.45cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 11.88cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.06cell/s]#015Executing:  70%|   | 21/30 [00:03<00:00, 14.68cell/s]#015Executing:  77%|  | 23/30 [00:03<00:00, 14.71cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.20cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 13.40cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.42cell/s]#015Executing: 100%|| 30/30 [00:04<00:00,  7.06cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:40.094 ip-10-2-77-202.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619200080000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m [2021-04-23 17:47:40.094 ip-10-2-77-202.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HUXOGYYLJB-ProfilerReport-1619199810-937175d3/algo-1-1619200005\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:25,  1.14cell/s]#015Executing:   7%|         | 2/30 [00:01<00:23,  1.18cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.77cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.60cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.33cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01, 10.14cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 11.24cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.79cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 15.15cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 15.75cell/s]#015Executing:  77%|  | 23/30 [00:02<00:00, 15.76cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 15.21cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 14.28cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 13.12cell/s]#015Executing: 100%|| 30/30 [00:03<00:00,  7.64cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:37.829 ip-10-0-190-90.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199808\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:38.312 ip-10-0-190-90.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:38.257 ip-10-2-238-88.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199815\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:38.738 ip-10-2-238-88.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.089 ip-10-2-93-108.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.089 ip-10-2-93-108.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.089 ip-10-2-93-108.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.089 ip-10-2-93-108.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.089 ip-10-2-93-108.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.089 ip-10-2-93-108.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.090 ip-10-2-93-108.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.090 ip-10-2-93-108.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.090 ip-10-2-93-108.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.090 ip-10-2-93-108.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.090 ip-10-2-93-108.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.090 ip-10-2-93-108.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.090 ip-10-2-93-108.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.090 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.090 ip-10-2-93-108.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.091 ip-10-2-93-108.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.130 ip-10-2-93-108.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.130 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.130 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.132 ip-10-2-93-108.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.132 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.132 ip-10-2-93-108.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.132 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.133 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.134 ip-10-2-93-108.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.134 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.134 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.135 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.136 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.139 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:43.139 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.924 ip-10-0-235-57.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.924 ip-10-0-235-57.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.924 ip-10-0-235-57.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.924 ip-10-0-235-57.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.924 ip-10-0-235-57.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.925 ip-10-0-235-57.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.925 ip-10-0-235-57.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.925 ip-10-0-235-57.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.925 ip-10-0-235-57.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.925 ip-10-0-235-57.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.925 ip-10-0-235-57.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.925 ip-10-0-235-57.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.925 ip-10-0-235-57.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.925 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.926 ip-10-0-235-57.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.926 ip-10-0-235-57.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.958 ip-10-0-235-57.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.958 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.959 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.961 ip-10-0-235-57.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.961 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.961 ip-10-0-235-57.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.961 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.962 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.963 ip-10-0-235-57.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.963 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.964 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.964 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.965 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.968 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:46.969 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:47.182 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:44.011906', 'end_time': '2021-04-23T17:47:44.030057', 'duration': 0.018151, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:44.048000', 'end_time': '2021-04-23T17:47:44.864330', 'duration': 0.81633, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:44.072167Z', 'iopub.execute_input': '2021-04-23T17:47:44.072665Z', 'iopub.status.idle': '2021-04-23T17:47:44.863715Z', 'shell.execute_reply': '2021-04-23T17:47:44.864107Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 17:47:44.856 ip-10-2-93-108.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199813\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:44.883733', 'end_time': '2021-04-23T17:47:45.093226', 'duration': 0.209493, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:44.905309Z', 'iopub.execute_input': '2021-04-23T17:47:44.905812Z', 'iopub.status.idle': '2021-04-23T17:47:45.092669Z', 'shell.execute_reply': '2021-04-23T17:47:45.093082Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n     \r\n",
      " if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.111650', 'end_time': '2021-04-23T17:47:45.138974', 'duration': 0.027324, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.136830Z', 'iopub.execute_input': '2021-04-23T17:47:45.137727Z', 'iopub.status.idle': '2021-04-23T17:47:45.138469Z', 'shell.execute_reply': '2021-04-23T17:47:45.138840Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.157257', 'end_time': '2021-04-23T17:47:45.181478', 'duration': 0.024221, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.179129Z', 'iopub.execute_input': '2021-04-23T17:47:45.179612Z', 'iopub.status.idle': '2021-04-23T17:47:45.180920Z', 'shell.execute_reply': '2021-04-23T17:47:45.181348Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.199653', 'end_time': '2021-04-23T17:47:45.217841', 'duration': 0.018188, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.236282', 'end_time': '2021-04-23T17:47:45.260186', 'duration': 0.023904, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.257844Z', 'iopub.execute_input': '2021-04-23T17:47:45.258342Z', 'iopub.status.idle': '2021-04-23T17:47:45.259683Z', 'shell.execute_reply': '2021-04-23T17:47:45.260052Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.278456', 'end_time': '2021-04-23T17:47:45.310005', 'duration': 0.031549, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.305957Z', 'iopub.execute_input': '2021-04-23T17:47:45.306482Z', 'shell.execute_reply': '2021-04-23T17:47:45.309421Z', 'iopub.status.idle': '2021-04-23T17:47:45.309896Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.328652', 'end_time': '2021-04-23T17:47:45.371006', 'duration': 0.042354, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.358204Z', 'iopub.execute_input': '2021-04-23T17:47:45.360903Z', 'iopub.status.idle': '2021-04-23T17:47:45.370482Z', 'shell.execute_reply': '2021-04-23T17:47:45.370864Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8a866622-7dd4-41e4-9383-9d22a90fff98\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"5f331cd9-10f8-4f5e-9959-bd388f9ffc1d\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1010\"},\"field\":\"0\",\"formatter\":{\"id\":\"1009\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"Selection\"},{\"attributes\":{\"editor\":{\"id\":\"1012\"},\"field\":\"1\",\"formatter\":{\"id\":\"1011\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"StringEditor\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"StringFormatter\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"17:46:26 04/23/2021\",\"17:46:59 04/23/2021\",\"33 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1014\"},\"selection_policy\":{\"id\":\"1013\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringEditor\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 17:46:26 and ran for 33 seconds. \\\\n Your training job started on 04/23/2021 at 17:46:26 and ran for 33 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringFormatter\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"5f331cd9-10f8-4f5e-9959-bd388f9ffc1d\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"8a866622-7dd4-41e4-9383-9d22a90fff98\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.390522', 'end_time': '2021-04-23T17:47:45.409873', 'duration': 0.019351, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.429328', 'end_time': '2021-04-23T17:47:45.454221', 'duration': 0.024893, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.451504Z', 'iopub.execute_input': '2021-04-23T17:47:45.451991Z', 'shell.execute_reply': '2021-04-23T17:47:45.453676Z', 'iopub.status.idle': '2021-04-23T17:47:45.454110Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-inpu\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m t'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.473522', 'end_time': '2021-04-23T17:47:45.502338', 'duration': 0.028816, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.499915Z', 'iopub.execute_input': '2021-04-23T17:47:45.500479Z', 'shell.execute_reply': '2021-04-23T17:47:45.501739Z', 'iopub.status.idle': '2021-04-23T17:47:45.502229Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.521961', 'end_time': '2021-04-23T17:47:45.568534', 'duration': 0.046573, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.550662Z', 'iopub.execute_input': '2021-04-23T17:47:45.551164Z', 'iopub.status.idle': '2021-04-23T17:47:45.567996Z', 'shell.execute_reply': '2021-04-23T17:47:45.568391Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8fc95ae2-fd31-4847-93c7-ee225b978eea\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"9d3e97fa-371b-4df0-9371-549367377024\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1079\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1078\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1069\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1068\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringEditor\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1071\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1070\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{\"editor\":{\"id\":\"1067\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1066\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringFormatter\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQHsUrkfhujJAAAAAAACAWEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAOBPQOxRuB6FayxAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAADD9Shcj8JYQOxRuB6F6zBAUrgehetRAEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQK5H4XoUrjJA4XoUrkcxWEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQLgehetRuDJArkfhehR+WEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1083\"},\"selection_policy\":{\"id\":\"1082\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1073\"},\"field\":\"max\",\"formatter\":{\"id\":\"1072\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1075\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1074\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1081\"},\"field\":\"min\",\"formatter\":{\"id\":\"1080\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringEditor\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{\"editor\":{\"id\":\"1077\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1076\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringFormatter\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"9d3e97fa-371b-4df0-9371-549367377024\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"8fc95ae2-fd31-4847-93c7-ee225b978eea\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.589016', 'end_time': '2021-04-23T17:47:45.628874', 'duration': 0.039858, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.616695Z', 'iopub.execute_input': '2021-04-23T17:47:45.621992Z', 'iopub.status.idle': '2021-04-23T17:47:45.628353Z', 'shell.execute_reply': '2021-04-23T17:47:45.628732Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.649961', 'end_time': '2021-04-23T17:47:45.681140', 'duration': 0.031179, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.678942Z', 'iopub.execute_input': '2021-04-23T17:47:45.679456Z', 'iopub.status.idle': '2021-04-23T17:47:45.680594Z', 'shell.execute_reply': '2021-04-23T17:\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m 47:45.680998Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.702338', 'end_time': '2021-04-23T17:47:45.733382', 'duration': 0.031044, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.731191Z', 'iopub.execute_input': '2021-04-23T17:47:45.731690Z', 'shell.execute_reply': '2021-04-23T17:47:45.732841Z', 'iopub.status.idle': '2021-04-23T17:47:45.733273Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.754642', 'end_time': '2021-04-23T17:47:45.776223', 'duration': 0.021581, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.797638', 'end_time': '2021-04-23T17:47:45.825821', 'duration': 0.028183, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.823564Z', 'iopub.execute_input': '2021-04-23T17:47:45.824061Z', 'iopub.status.idle': '2021-04-23T17:47:45.825315Z', 'shell.execute_reply': '2021-04-23T17:47:45.825689Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.846998', 'end_time': '2021-04-23T17:47:45.874689', 'duration': 0.027691, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.872469Z', 'iopub.execute_input': '2021-04-23T17:47:45.872944Z', 'shell.execute_reply': '2021-04-23T17:47:45.874184Z', 'iopub.status.idle': '2021-04-23T17:47:45.874586Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.896104', 'end_time': '2021-04-23T17:47:45.933816', 'duration': 0.037712, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.922810Z', 'iopub.execute_input': '2021-04-23T17:47:45.923297Z', 'shell.execute_reply': '2021-04-23T17:47:45.933225Z', 'iopub.status.idle': '2021-04-23T17:47:45.933708Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the LowGPUUtilization rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>78</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>78</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>66</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n\r\n",
      "    summary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:45.956002', 'end_time': '2021-04-23T17:47:45.986381', 'duration': 0.030379, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:45.983342Z', 'iopub.execute_input': '2021-04-23T17:47:45.983846Z', 'iopub.status.idle': '2021-04-23T17:47:45.985867Z', 'shell.execute_reply': '2021-04-23T17:47:45.986245Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:46.009831', 'end_time': '2021-04-23T17:47:46.038137', 'duration': 0.028306, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:46.035912Z', 'iopub.execute_input': '2021-04-23T17:47:46.036391Z', 'iopub.status.idle': '2021-04-23T17:47:46.037631Z', 'shell.execute_reply': '2021-04-23T17:47:46.038000Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:46.060843', 'end_time': '2021-04-23T17:47:46.113268', 'duration': 0.052425, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:46.096328Z', 'iopub.execute_input': '2021-04-23T17:47:46.105124Z', 'shell.execute_reply': '2021-04-23T17:47:46.112655Z', 'iopub.status.idle': '2021-04-23T17:47:46.113156Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"2ad0db16-1a3e-4d7d-a2ff-e1c55bb5a2c9\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"78ea17e3-8374-4fce-a691-102572ea6c31\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"78ea17e3-8374-4fce-a691-102572ea6c31\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"2ad0db16-1a3e-4d7d-a2ff-e1c55bb5a2c9\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:46.137526', 'end_time': '2021-04-23T17:47:46.184644', 'duration': 0.047118, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:46.176004Z', 'iopub.execute_input': '2021-04-23T17:47:46.176534Z', 'shell.execute_reply': '2021-04-23T17:47:46.184063Z', 'iopub.status.idle': '2021\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m -04-23T17:47:46.184536Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"760d49b8-0c84-4f1c-b181-3f79d182e180\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"b55c6884-565a-489a-a9de-e71ea332280d\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"b55c6884-565a-489a-a9de-e71ea332280d\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"760d49b8-0c84-4f1c-b181-3f79d182e180\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:46.210361', 'end_time': '2021-04-23T17:47:46.254438', 'duration': 0.044077, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:46.246543Z', 'iopub.execute_input': '2021-04-23T17:47:46.247086Z', 'iopub.status.idle': '2021-04-23T17:47:46.253931Z', 'shell.execute_reply': '2021-04-23T17:47:46.254304Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"41efa086-5171-49f8-85b7-8a6827239511\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"7074c741-9bf6-4952-ade6-902596a9ab0d\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"7074c741-9bf6-4952-ade6-902596a9ab0d\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"41efa086-5171-49f8-85b7-8a6827239511\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:46.281539', 'end_time': '2021-04-23T17:47:46.333390', 'duration': 0.051851, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:46.315960Z', 'iopub.execute_input': '2021-04-23T17:47:46.320346Z', 'shell.execute_reply': '2021-04-23T17:47:46.332822Z', 'iopub.status.idle': '2021-04-23T17:47:46.333273Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"88a0017f-86b1-443d-a61c-a629b6238561\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"2c590417-dbbe-4f05-a9cc-078ffb27124c\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m \",\"width\":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"2c590417-dbbe-4f05-a9cc-078ffb27124c\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"88a0017f-86b1-443d-a61c-a629b6238561\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"e2f5f608-9628-4b26-8cb0-aa00703bed4f\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"bd252e6d-5a3b-46f9-a0a7-e37fc2da7394\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"bd252e6d-5a3b-46f9-a0a7-e37fc2da7394\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"e2f5f608-9628-4b26-8cb0-aa00703bed4f\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:46.362408', 'end_time': '2021-04-23T17:47:46.411537', 'duration': 0.049129, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:46.403689Z', 'iopub.execute_input': '2021-04-23T17:47:46.404232Z', 'shell.execute_reply': '2021-04-23T17:47:46.410941Z', 'iopub.status.idle': '2021-04-23T17:47:46.411426Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"68d86ed5-fc15-411d-b32e-29ce4f825bec\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"7e9479f7-6d9a-4527-b3fd-65e960f39f92\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 66 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"7e9479f7-6d9a-4527-b3fd-65e960f39f92\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"68d86ed5-fc15-411d-b32e-29ce4f825bec\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:46.442248', 'end_time': '2021-04-23T17:47:46.498522', 'duration': 0.056274, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:46.479780Z', 'iopub.execute_input': '2021-04-23T17:47:46.484994Z', 'shell.execute_reply': '2021-04-23T17:47:46.497911Z', 'iopub.status.idle': '2021-04-23T17:47:46.498408Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"3a1578f6-c6be-49c1-a7e0-ffca43e59e08\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"e4e1d927-b83f-4160-91cc-0e7423a99cf1\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 78 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"e4e1d927-b83f-4160-91cc-0e7423a99cf1\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"3a1578f6-c6be-49c1-a7e0-ffca43e59e08\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n      \r\n",
      "  if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:46.529683', 'end_time': '2021-04-23T17:47:46.587227', 'duration': 0.057544, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:46.576139Z', 'iopub.execute_input': '2021-04-23T17:47:46.579622Z', 'shell.execute_reply': '2021-04-23T17:47:46.586704Z', 'iopub.status.idle': '2021-04-23T17:47:46.587118Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"e09245d6-ee98-4e28-b893-d20a2ae57380\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"53985714-64de-4662-9f05-91ae7bdf7215\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 78 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"53985714-64de-4662-9f05-91ae7bdf7215\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"e09245d6-ee98-4e28-b893-d20a2ae57380\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n \r\n",
      "                                           height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:46.619866', 'end_time': '2021-04-23T17:47:46.674069', 'duration': 0.054203, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:46.663799Z', 'iopub.execute_input': '2021-04-23T17:47:46.666465Z', 'shell.execute_reply': '2021-04-23T17:47:46.673482Z', 'iopub.status.idle': '2021-04-23T17:47:46.673959Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"3713bd1b-18d8-4de3-9a8b-273ae730ea79\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"f53b10f8-288d-4b04-833a-99d84aef9dc1\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"f53b10f8-288d-4b04-833a-99d84aef9dc1\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"3713bd1b-18d8-4de3-9a8b-273ae730ea79\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T17:47:43.172018', 'end_time': '2021-04-23T17:47:47.113613', 'duration': 3.941595, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:47.183 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:47.183 ip-10-2-93-108.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:47.583 ip-10-2-93-108.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619200080000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m [2021-04-23 17:47:47.583 ip-10-2-93-108.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-APPAQAEKCM-ProfilerReport-1619199813-e9517881/algo-1-1619200014\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:25,  1.14cell/s]#015Executing:   7%|         | 2/30 [00:01<00:23,  1.17cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.76cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.56cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.26cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01, 10.08cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 11.19cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.70cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.98cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 15.60cell/s]#015Executing:  77%|  | 23/30 [00:02<00:00, 15.62cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 15.09cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 14.22cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 13.21cell/s]#015Executing: 100%|| 30/30 [00:03<00:00,  7.61cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.322 ip-10-0-190-90.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.323 ip-10-0-190-90.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.323 ip-10-0-190-90.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.323 ip-10-0-190-90.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.323 ip-10-0-190-90.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.323 ip-10-0-190-90.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.323 ip-10-0-190-90.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.324 ip-10-0-190-90.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.324 ip-10-0-190-90.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.324 ip-10-0-190-90.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.324 ip-10-0-190-90.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.324 ip-10-0-190-90.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.324 ip-10-0-190-90.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.324 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.325 ip-10-0-190-90.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.325 ip-10-0-190-90.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.358 ip-10-0-190-90.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.358 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.359 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.359 ip-10-0-190-90.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.359 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.359 ip-10-0-190-90.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.360 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.360 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.360 ip-10-0-190-90.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.360 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.360 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.361 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.361 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.364 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:48.364 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619199900000000 to timestamp_end:1619199960000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.748 ip-10-2-238-88.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.748 ip-10-2-238-88.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.748 ip-10-2-238-88.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.748 ip-10-2-238-88.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.748 ip-10-2-238-88.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.748 ip-10-2-238-88.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.749 ip-10-2-238-88.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.749 ip-10-2-238-88.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.749 ip-10-2-238-88.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.749 ip-10-2-238-88.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.749 ip-10-2-238-88.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.749 ip-10-2-238-88.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.749 ip-10-2-238-88.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.749 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.750 ip-10-2-238-88.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.751 ip-10-2-238-88.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.785 ip-10-2-238-88.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.785 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.785 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.787 ip-10-2-238-88.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.787 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.787 ip-10-2-238-88.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.788 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.788 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.789 ip-10-2-238-88.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.789 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.790 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.790 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.791 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.793 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:48.794 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:50.933 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:47.837487', 'end_time': '2021-04-23T17:47:47.856327', 'duration': 0.01884, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:47.874223', 'end_time': '2021-04-23T17:47:48.669872', 'duration': 0.795649, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:47.898516Z', 'iopub.execute_input': '2021-04-23T17:47:47.899004Z', 'shell.execute_reply': '2021-04-23T17:47:48.669223Z', 'iopub.status.idle': '2021-04-23T17:47:48.669740Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 17:47:48.661 ip-10-0-235-57.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199820\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:48.689911', 'end_time': '2021-04-23T17:47:48.899382', 'duration': 0.209471, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:48.712074Z', 'iopub.execute_input': '2021-04-23T17:47:48.712570Z', 'iopub.status.idle': '2021-04-23T17:47:48.898807Z', 'shell.execute_reply': '2021-04-23T17:47:48.899231Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n     \r\n",
      " if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:48.918575', 'end_time': '2021-04-23T17:47:48.945774', 'duration': 0.027199, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:48.943318Z', 'iopub.execute_input': '2021-04-23T17:47:48.943817Z', 'shell.execute_reply': '2021-04-23T17:47:48.945180Z', 'iopub.status.idle': '2021-04-23T17:47:48.945671Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:48.964650', 'end_time': '2021-04-23T17:47:48.988693', 'duration': 0.024043, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:48.986241Z', 'iopub.execute_input': '2021-04-23T17:47:48.986715Z', 'shell.execute_reply': '2021-04-23T17:47:48.988186Z', 'iopub.status.idle': '2021-04-23T17:47:48.988594Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.007369', 'end_time': '2021-04-23T17:47:49.026049', 'duration': 0.01868, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.045140', 'end_time': '2021-04-23T17:47:49.069783', 'duration': 0.024643, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.067401Z', 'iopub.execute_input': '2021-04-23T17:47:49.067891Z', 'iopub.status.idle': '2021-04-23T17:47:49.069275Z', 'shell.execute_reply': '2021-04-23T17:47:49.069658Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.088593', 'end_time': '2021-04-23T17:47:49.120539', 'duration': 0.031946, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.116570Z', 'iopub.execute_input': '2021-04-23T17:47:49.118141Z', 'shell.execute_reply': '2021-04-23T17:47:49.120018Z', 'iopub.status.idle': '2021-04-23T17:47:49.120434Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.139533', 'end_time': '2021-04-23T17:47:49.182004', 'duration': 0.042471, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.169345Z', 'iopub.execute_input': '2021-04-23T17:47:49.172077Z', 'iopub.status.idle': '2021-04-23T17:47:49.181488Z', 'shell.execute_reply': '2021-04-23T17:47:49.181883Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"20d6f624-e52e-430b-879b-26b8c3c1970c\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"9ea35757-9ab7-4253-9998-6510e2c4d586\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"StringFormatter\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 17:46:18 and ran for 41 seconds. \\\\n Your training job started on 04/23/2021 at 17:46:18 and ran for 41 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"editor\":{\"id\":\"1012\"},\"field\":\"0\",\"formatter\":{\"id\":\"1011\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"StringEditor\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"17:46:18 04/23/2021\",\"17:46:59 04/23/2021\",\"41 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1010\"},\"selection_policy\":{\"id\":\"1009\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"editor\":{\"id\":\"1014\"},\"field\":\"1\",\"formatter\":{\"id\":\"1013\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringEditor\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"9ea35757-9ab7-4253-9998-6510e2c4d586\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"20d6f624-e52e-430b-879b-26b8c3c1970c\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.202007', 'end_time': '2021-04-23T17:47:49.221914', 'duration': 0.019907, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.243844', 'end_time': '2021-04-23T17:47:49.272634', 'duration': 0.02879, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.269846Z', 'iopub.execute_input': '2021-04-23T17:47:49.270359Z', 'shell.execute_reply': '2021-04-23T17:47:49.272042Z', 'iopub.status.idle': '2021-04-23T17:47:49.272534Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-input'\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m ], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.292515', 'end_time': '2021-04-23T17:47:49.321328', 'duration': 0.028813, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.318881Z', 'iopub.execute_input': '2021-04-23T17:47:49.319414Z', 'shell.execute_reply': '2021-04-23T17:47:49.320821Z', 'iopub.status.idle': '2021-04-23T17:47:49.321230Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.341416', 'end_time': '2021-04-23T17:47:49.388507', 'duration': 0.047091, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.370324Z', 'iopub.execute_input': '2021-04-23T17:47:49.384658Z', 'iopub.status.idle': '2021-04-23T17:47:49.388003Z', 'shell.execute_reply': '2021-04-23T17:47:49.388388Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"59433a83-d044-4b2c-bea7-878fa200795a\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"5d1b39b2-4b8b-4716-8291-30f146a1790d\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1071\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1070\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringFormatter\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQB+F61G4njJAAAAAAADAV0A=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAAK16NwPYpKQBSuR+F6lCtAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAABcj8L1KJxYQNejcD0K1zBASOF6FK5H8T8=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQIXrUbgehTJA4XoUrkfBVkA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQB+F61G4njJAUrgeheuRV0A=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1067\"},\"selection_policy\":{\"id\":\"1066\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringEditor\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1069\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1068\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringFormatter\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"editor\":{\"id\":\"1073\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1072\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{\"editor\":{\"id\":\"1075\"},\"field\":\"max\",\"formatter\":{\"id\":\"1074\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"Selection\"},{\"attributes\":{\"editor\":{\"id\":\"1083\"},\"field\":\"min\",\"formatter\":{\"id\":\"1082\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1081\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1080\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1077\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1076\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1079\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1078\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringFormatter\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"5d1b39b2-4b8b-4716-8291-30f146a1790d\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"59433a83-d044-4b2c-bea7-878fa200795a\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.409856', 'end_time': '2021-04-23T17:47:49.450153', 'duration': 0.040297, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.444381Z', 'iopub.execute_input': '2021-04-23T17:47:49.446971Z', 'iopub.status.idle': '2021-04-23T17:47:49.449637Z', 'shell.execute_reply': '2021-04-23T17:47:49.450032Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.471840', 'end_time': '2021-04-23T17:47:49.502658', 'duration': 0.030818, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.500229Z', 'iopub.execute_input': '2021-04-23T17:47:49.500729Z', 'iopub.status.idle': '2021-04-23T17:47:49.502153Z', 'shell.execute_reply': '2021-04-23T17:47\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m :49.502536Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.524738', 'end_time': '2021-04-23T17:47:49.555917', 'duration': 0.031179, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.553484Z', 'iopub.execute_input': '2021-04-23T17:47:49.553980Z', 'iopub.status.idle': '2021-04-23T17:47:49.555406Z', 'shell.execute_reply': '2021-04-23T17:47:49.555793Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.577811', 'end_time': '2021-04-23T17:47:49.599497', 'duration': 0.021686, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.621188', 'end_time': '2021-04-23T17:47:49.649475', 'duration': 0.028287, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.647076Z', 'iopub.execute_input': '2021-04-23T17:47:49.647585Z', 'shell.execute_reply': '2021-04-23T17:47:49.648964Z', 'iopub.status.idle': '2021-04-23T17:47:49.649374Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.671306', 'end_time': '2021-04-23T17:47:49.698841', 'duration': 0.027535, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.696442Z', 'iopub.execute_input': '2021-04-23T17:47:49.696925Z', 'iopub.status.idle': '2021-04-23T17:47:49.698340Z', 'shell.execute_reply': '2021-04-23T17:47:49.698724Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.721019', 'end_time': '2021-04-23T17:47:49.759300', 'duration': 0.038281, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.747855Z', 'iopub.execute_input': '2021-04-23T17:47:49.748341Z', 'shell.execute_reply': '2021-04-23T17:47:49.758750Z', 'iopub.status.idle': '2021-04-23T17:47:49.759196Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the LowGPUUtilization rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>87</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>87</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>83</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n  \r\n",
      "  summary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.782456', 'end_time': '2021-04-23T17:47:49.813124', 'duration': 0.030668, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.809882Z', 'iopub.execute_input': '2021-04-23T17:47:49.810438Z', 'iopub.status.idle': '2021-04-23T17:47:49.812598Z', 'shell.execute_reply': '2021-04-23T17:47:49.813007Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.836724', 'end_time': '2021-04-23T17:47:49.865063', 'duration': 0.028339, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.862627Z', 'iopub.execute_input': '2021-04-23T17:47:49.863136Z', 'iopub.status.idle': '2021-04-23T17:47:49.864568Z', 'shell.execute_reply': '2021-04-23T17:47:49.864949Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.888599', 'end_time': '2021-04-23T17:47:49.941775', 'duration': 0.053176, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.929439Z', 'iopub.execute_input': '2021-04-23T17:47:49.933329Z', 'iopub.status.idle': '2021-04-23T17:47:49.941273Z', 'shell.execute_reply': '2021-04-23T17:47:49.941658Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"fcae7c0a-9cd7-464f-89e2-80c69275973b\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"5e10a358-3d18-456d-945b-23746101bdce\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"5e10a358-3d18-456d-945b-23746101bdce\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"fcae7c0a-9cd7-464f-89e2-80c69275973b\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.967199', 'end_time': '2021-04-23T17:47:50.016259', 'duration': 0.04906, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.007208Z', 'iopub.execute_input': '2021-04-23T17:47:50.007780Z', 'iopub.status.idle': '2021-04-23T17:47:50.015736Z', 'shell.execute_reply': '2021-04\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m -23T17:47:50.016130Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"b2615c97-8a16-4183-89f9-c5b1f3b25aef\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"a17e558c-4423-4b75-834c-7463df7e171a\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"a17e558c-4423-4b75-834c-7463df7e171a\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"b2615c97-8a16-4183-89f9-c5b1f3b25aef\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.043736', 'end_time': '2021-04-23T17:47:50.088700', 'duration': 0.044964, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.080738Z', 'iopub.execute_input': '2021-04-23T17:47:50.081255Z', 'shell.execute_reply': '2021-04-23T17:47:50.088180Z', 'iopub.status.idle': '2021-04-23T17:47:50.088597Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"a168e69e-db2b-4e28-8536-b435d57ba632\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"8a77757b-835c-4b8d-8aec-223fd798236d\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"8a77757b-835c-4b8d-8aec-223fd798236d\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"a168e69e-db2b-4e28-8536-b435d57ba632\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.116916', 'end_time': '2021-04-23T17:47:50.167742', 'duration': 0.050826, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.155414Z', 'iopub.execute_input': '2021-04-23T17:47:50.155961Z', 'iopub.status.idle': '2021-04-23T17:47:50.167221Z', 'shell.execute_reply': '2021-04-23T17:47:50.167616Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"a77ffe21-160b-4069-881f-e2c40ff9a1b0\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"3b0fbd9a-7d19-49d9-bd37-f13fd7955f8e\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m width\":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"3b0fbd9a-7d19-49d9-bd37-f13fd7955f8e\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"a77ffe21-160b-4069-881f-e2c40ff9a1b0\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"03cc9142-c2a0-4137-993b-498201068add\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"1153c139-54ea-4b28-98e0-4821385003f9\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"1153c139-54ea-4b28-98e0-4821385003f9\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"03cc9142-c2a0-4137-993b-498201068add\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.197807', 'end_time': '2021-04-23T17:47:50.247700', 'duration': 0.049893, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.240011Z', 'iopub.execute_input': '2021-04-23T17:47:50.240584Z', 'iopub.status.idle': '2021-04-23T17:47:50.247174Z', 'shell.execute_reply': '2021-04-23T17:47:50.247576Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"5f23e826-8cfc-4f6e-b8aa-93f7a2f3e914\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"097ad348-715e-4931-8ca0-97d4af6fcf1f\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 83 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"097ad348-715e-4931-8ca0-97d4af6fcf1f\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"5f23e826-8cfc-4f6e-b8aa-93f7a2f3e914\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.279804', 'end_time': '2021-04-23T17:47:50.337465', 'duration': 0.057661, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.326436Z', 'iopub.execute_input': '2021-04-23T17:47:50.329754Z', 'shell.execute_reply': '2021-04-23T17:47:50.336871Z', 'iopub.status.idle': '2021-04-23T17:47:50.337361Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"15c27e66-6c39-49da-a524-c4bab8600f8e\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"2869e6cf-fc05-45c8-888a-6c6e8a0294da\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 87 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"2869e6cf-fc05-45c8-888a-6c6e8a0294da\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"15c27e66-6c39-49da-a524-c4bab8600f8e\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        i\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m f (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.370144', 'end_time': '2021-04-23T17:47:50.430234', 'duration': 0.06009, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.415898Z', 'iopub.execute_input': '2021-04-23T17:47:50.422202Z', 'iopub.status.idle': '2021-04-23T17:47:50.429683Z', 'shell.execute_reply': '2021-04-23T17:47:50.430104Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"dfbfc274-fa4b-456d-8604-caac06afcc7a\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"43bd88a8-872c-436a-b191-8250667f40ae\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 87 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"43bd88a8-872c-436a-b191-8250667f40ae\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"dfbfc274-fa4b-456d-8604-caac06afcc7a\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n     \r\n",
      "                                       height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.464125', 'end_time': '2021-04-23T17:47:50.521259', 'duration': 0.057134, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.510854Z', 'iopub.execute_input': '2021-04-23T17:47:50.513500Z', 'iopub.status.idle': '2021-04-23T17:47:50.520746Z', 'shell.execute_reply': '2021-04-23T17:47:50.521130Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"22678297-e517-4fb8-b128-0b3eef670bcb\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"2846b273-3768-4f6b-b4b6-a038c7ae8c69\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"2846b273-3768-4f6b-b4b6-a038c7ae8c69\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"22678297-e517-4fb8-b128-0b3eef670bcb\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T17:47:47.002891', 'end_time': '2021-04-23T17:47:50.861420', 'duration': 3.858529, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:50.934 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:50.934 ip-10-0-235-57.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:51.348 ip-10-0-235-57.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619200080000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m [2021-04-23 17:47:51.348 ip-10-0-235-57.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:25,  1.15cell/s]#015Executing:   7%|         | 2/30 [00:01<00:23,  1.19cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.79cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.60cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.29cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01, 10.00cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 11.08cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.57cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.82cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 15.41cell/s]#015Executing:  77%|  | 23/30 [00:02<00:00, 15.39cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.75cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 13.95cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.88cell/s]#015Executing: 100%|| 30/30 [00:03<00:00,  7.78cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-ZFZAYEOGFY-ProfilerReport-1619199820-fcec9a82/algo-1-1619200012\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:52.301 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.225091', 'end_time': '2021-04-23T17:47:49.243948', 'duration': 0.018857, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.261629', 'end_time': '2021-04-23T17:47:50.049539', 'duration': 0.78791, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.285708Z', 'iopub.execute_input': '2021-04-23T17:47:49.286196Z', 'shell.execute_reply': '2021-04-23T17:47:50.048902Z', 'iopub.status.idle': '2021-04-23T17:47:50.049410Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 17:47:50.041 ip-10-0-190-90.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199808\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.069440', 'end_time': '2021-04-23T17:47:50.276517', 'duration': 0.207077, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.091436Z', 'iopub.execute_input': '2021-04-23T17:47:50.091924Z', 'iopub.status.idle': '2021-04-23T17:47:50.275982Z', 'shell.execute_reply': '2021-04-23T17:47:50.276373Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n     \r\n",
      " if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.295799', 'end_time': '2021-04-23T17:47:50.322711', 'duration': 0.026912, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.320391Z', 'iopub.execute_input': '2021-04-23T17:47:50.320909Z', 'shell.execute_reply': '2021-04-23T17:47:50.322124Z', 'iopub.status.idle': '2021-04-23T17:47:50.322609Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.341447', 'end_time': '2021-04-23T17:47:50.365661', 'duration': 0.024214, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.363422Z', 'iopub.execute_input': '2021-04-23T17:47:50.363890Z', 'shell.execute_reply': '2021-04-23T17:47:50.365062Z', 'iopub.status.idle': '2021-04-23T17:47:50.365564Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.384530', 'end_time': '2021-04-23T17:47:50.403220', 'duration': 0.01869, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.422171', 'end_time': '2021-04-23T17:47:50.447013', 'duration': 0.024842, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.444536Z', 'iopub.execute_input': '2021-04-23T17:47:50.445171Z', 'shell.execute_reply': '2021-04-23T17:47:50.446504Z', 'iopub.status.idle': '2021-04-23T17:47:50.446909Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.466104', 'end_time': '2021-04-23T17:47:50.498211', 'duration': 0.032107, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.492726Z', 'iopub.execute_input': '2021-04-23T17:47:50.494326Z', 'iopub.status.idle': '2021-04-23T17:47:50.497713Z', 'shell.execute_reply': '2021-04-23T17:47:50.498098Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.517084', 'end_time': '2021-04-23T17:47:50.559940', 'duration': 0.042856, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.543360Z', 'iopub.execute_input': '2021-04-23T17:47:50.549902Z', 'iopub.status.idle': '2021-04-23T17:47:50.559441Z', 'shell.execute_reply': '2021-04-23T17:47:50.559824Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"b5dd11f3-5166-4687-9128-1bcbb908f89f\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"454a4ceb-f570-44ff-aede-bb2e241dadd8\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringFormatter\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 17:45:56 and ran for 3 seconds. \\\\n Your training job started on 04/23/2021 at 17:45:56 and ran for 3 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1014\"},\"field\":\"1\",\"formatter\":{\"id\":\"1013\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"17:45:56 04/23/2021\",\"17:45:59 04/23/2021\",\"3 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1010\"},\"selection_policy\":{\"id\":\"1009\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"StringEditor\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{\"editor\":{\"id\":\"1012\"},\"field\":\"0\",\"formatter\":{\"id\":\"1011\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"454a4ceb-f570-44ff-aede-bb2e241dadd8\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"b5dd11f3-5166-4687-9128-1bcbb908f89f\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.579929', 'end_time': '2021-04-23T17:47:50.599760', 'duration': 0.019831, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.619579', 'end_time': '2021-04-23T17:47:50.644816', 'duration': 0.025237, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.642156Z', 'iopub.execute_input': '2021-04-23T17:47:50.642677Z', 'iopub.status.idle': '2021-04-23T17:47:50.644314Z', 'shell.execute_reply': '2021-04-23T17:47:50.644700Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-input'],\r\n",
      " 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.664797', 'end_time': '2021-04-23T17:47:50.693520', 'duration': 0.028723, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.690992Z', 'iopub.execute_input': '2021-04-23T17:47:50.691628Z', 'iopub.status.idle': '2021-04-23T17:47:50.693022Z', 'shell.execute_reply': '2021-04-23T17:47:50.693407Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.713636', 'end_time': '2021-04-23T17:47:50.760995', 'duration': 0.047359, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.741648Z', 'iopub.execute_input': '2021-04-23T17:47:50.756791Z', 'iopub.status.idle': '2021-04-23T17:47:50.760497Z', 'shell.execute_reply': '2021-04-23T17:47:50.760881Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"3071830a-d13a-4078-8c7b-b20faea1a4cb\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"91f2a497-0cb9-4269-8a47-713354c1d10d\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1077\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1076\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"StringFormatter\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1081\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1080\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"StringEditor\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1071\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1070\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1075\"},\"field\":\"max\",\"formatter\":{\"id\":\"1074\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringEditor\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQArXo3A9Ci9AKVyPwvWoPUA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAMBYQJqZmZmZGS1AAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAADhehSuR+FYQIXrUbgeBS5AAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQOF6FK5H4S5APQrXo3B9NkA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQAAAAAAAAC9AexSuR+E6PEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1067\"},\"selection_policy\":{\"id\":\"1066\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1073\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1072\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1069\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1068\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1083\"},\"field\":\"min\",\"formatter\":{\"id\":\"1082\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1079\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1078\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"91f2a497-0cb9-4269-8a47-713354c1d10d\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"3071830a-d13a-4078-8c7b-b20faea1a4cb\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.781909', 'end_time': '2021-04-23T17:47:50.822360', 'duration': 0.040451, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.810260Z', 'iopub.execute_input': '2021-04-23T17:47:50.816686Z', 'iopub.status.idle': '2021-04-23T17:47:50.821861Z', 'shell.execute_reply': '2021-04-23T17:47:50.822244Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.843989', 'end_time': '2021-04-23T17:47:50.874759', 'duration': 0.03077, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.872292Z', 'iopub.execute_input': '2021-04-23T17:47:50.872910Z', 'iopub.status.idle': '2021-04-23T17:47:50.874257Z', 'shell.execute_reply': '2021-04-23T17:47:50\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m .874645Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.896832', 'end_time': '2021-04-23T17:47:50.927878', 'duration': 0.031046, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.925366Z', 'iopub.execute_input': '2021-04-23T17:47:50.925988Z', 'iopub.status.idle': '2021-04-23T17:47:50.927382Z', 'shell.execute_reply': '2021-04-23T17:47:50.927765Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.949711', 'end_time': '2021-04-23T17:47:50.971331', 'duration': 0.02162, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.993043', 'end_time': '2021-04-23T17:47:51.021629', 'duration': 0.028586, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.018390Z', 'iopub.execute_input': '2021-04-23T17:47:51.019330Z', 'iopub.status.idle': '2021-04-23T17:47:51.021130Z', 'shell.execute_reply': '2021-04-23T17:47:51.021515Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.043466', 'end_time': '2021-04-23T17:47:51.071307', 'duration': 0.027841, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.068784Z', 'iopub.execute_input': '2021-04-23T17:47:51.069410Z', 'iopub.status.idle': '2021-04-23T17:47:51.070781Z', 'shell.execute_reply': '2021-04-23T17:47:51.071191Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.093338', 'end_time': '2021-04-23T17:47:51.132232', 'duration': 0.038894, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.120330Z', 'iopub.execute_input': '2021-04-23T17:47:51.121160Z', 'iopub.status.idle': '2021-04-23T17:47:51.131730Z', 'shell.execute_reply': '2021-04-23T17:47:51.132119Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the LoadBalancing rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>8</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>6</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>8</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n    summary[r\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m ule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.155205', 'end_time': '2021-04-23T17:47:51.185787', 'duration': 0.030582, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.182461Z', 'iopub.execute_input': '2021-04-23T17:47:51.183167Z', 'iopub.status.idle': '2021-04-23T17:47:51.185292Z', 'shell.execute_reply': '2021-04-23T17:47:51.185673Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.209347', 'end_time': '2021-04-23T17:47:51.237816', 'duration': 0.028469, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.235361Z', 'iopub.execute_input': '2021-04-23T17:47:51.235973Z', 'shell.execute_reply': '2021-04-23T17:47:51.237308Z', 'iopub.status.idle': '2021-04-23T17:47:51.237720Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.261245', 'end_time': '2021-04-23T17:47:51.315164', 'duration': 0.053919, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.297049Z', 'iopub.execute_input': '2021-04-23T17:47:51.305849Z', 'iopub.status.idle': '2021-04-23T17:47:51.314651Z', 'shell.execute_reply': '2021-04-23T17:47:51.315041Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"6aa3d2c8-64ee-4de6-b3f8-fe93940e45c8\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"7697898b-73f6-439c-a9e3-d4e4a7a7736d\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"7697898b-73f6-439c-a9e3-d4e4a7a7736d\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"6aa3d2c8-64ee-4de6-b3f8-fe93940e45c8\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.340120', 'end_time': '2021-04-23T17:47:51.388879', 'duration': 0.048759, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.378472Z', 'iopub.execute_input': '2021-04-23T17:47:51.379317Z', 'iopub.status.idle': '2021-04-23T17:47:51.388379Z', 'shell.execute_reply': '2021-04-23T17:47:\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m 51.388763Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"1c2d2776-6a05-465c-b723-973f70c3e9b7\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"d4d1d013-3896-4650-b4e8-43ac1b7161bc\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"d4d1d013-3896-4650-b4e8-43ac1b7161bc\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"1c2d2776-6a05-465c-b723-973f70c3e9b7\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.415380', 'end_time': '2021-04-23T17:47:51.460439', 'duration': 0.045059, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.452003Z', 'iopub.execute_input': '2021-04-23T17:47:51.452711Z', 'shell.execute_reply': '2021-04-23T17:47:51.459825Z', 'iopub.status.idle': '2021-04-23T17:47:51.460337Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"95f33221-86c0-4c93-bda6-09d14a672838\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"bc900ce2-e214-4358-8659-be5127a078b5\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"bc900ce2-e214-4358-8659-be5127a078b5\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"95f33221-86c0-4c93-bda6-09d14a672838\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.488989', 'end_time': '2021-04-23T17:47:51.539858', 'duration': 0.050869, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.527154Z', 'iopub.execute_input': '2021-04-23T17:47:51.527669Z', 'iopub.status.idle': '2021-04-23T17:47:51.539386Z', 'shell.execute_reply': '2021-04-23T17:47:51.539739Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"e0cc8e19-8374-4413-b058-13cdd9401fa1\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"63c3b148-c166-4d95-9942-8a6f3dc6b272\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m },\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"63c3b148-c166-4d95-9942-8a6f3dc6b272\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"e0cc8e19-8374-4413-b058-13cdd9401fa1\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"3ea8e67d-0ae7-4adc-a754-87e68f077647\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"2d58a534-1064-4ae5-87a3-df263073ee83\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"2d58a534-1064-4ae5-87a3-df263073ee83\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"3ea8e67d-0ae7-4adc-a754-87e68f077647\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.569607', 'end_time': '2021-04-23T17:47:51.619118', 'duration': 0.049511, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.611191Z', 'iopub.execute_input': '2021-04-23T17:47:51.611702Z', 'shell.execute_reply': '2021-04-23T17:47:51.618514Z', 'iopub.status.idle': '2021-04-23T17:47:51.619018Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"5d273ead-38f4-4cf7-9627-a1734285892b\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"9191d538-78f7-4772-9291-10168641505c\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 6 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"9191d538-78f7-4772-9291-10168641505c\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"5d273ead-38f4-4cf7-9627-a1734285892b\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.650697', 'end_time': '2021-04-23T17:47:51.707604', 'duration': 0.056907, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.699483Z', 'iopub.execute_input': '2021-04-23T17:47:51.700016Z', 'shell.execute_reply': '2021-04-23T17:47:51.707030Z', 'iopub.status.idle': '2021-04-23T17:47:51.707507Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"25317bc5-03b9-4604-9b82-42642fb8d2c4\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"50d49a21-5ec2-465b-b226-04c56fc6129c\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 8 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"50d49a21-5ec2-465b-b226-04c56fc6129c\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"25317bc5-03b9-4604-9b82-42642fb8d2c4\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.739874', 'end_time': '2021-04-23T17:47:51.798191', 'duration': 0.058317, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.790186Z', 'iopub.execute_input': '2021-04-23T17:47:51.790717Z', 'shell.execute_reply': '2021-04-23T17:47:51.797623Z', 'iopub.status.idle': '2021-04-23T17:47:51.798096Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"ab907c2b-1949-471f-b443-7ac775e15050\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"cc57bd8e-8da5-4c42-bc67-65c6e0e7b2e8\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 8 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"cc57bd8e-8da5-4c42-bc67-65c6e0e7b2e8\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"ab907c2b-1949-471f-b443-7ac775e15050\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                 \r\n",
      "                           height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.831942', 'end_time': '2021-04-23T17:47:51.887198', 'duration': 0.055256, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.879091Z', 'iopub.execute_input': '2021-04-23T17:47:51.879613Z', 'iopub.status.idle': '2021-04-23T17:47:51.886682Z', 'shell.execute_reply': '2021-04-23T17:47:51.887081Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d9ffb362-a866-48e1-b7a9-e7b658df828b\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"c3962cd4-f3d2-4ef0-ae1b-ebedf00c91e3\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"c3962cd4-f3d2-4ef0-ae1b-ebedf00c91e3\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"d9ffb362-a866-48e1-b7a9-e7b658df828b\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T17:47:48.390755', 'end_time': '2021-04-23T17:47:52.227916', 'duration': 3.837161, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:52.301 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:52.301 ip-10-0-190-90.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:52.713 ip-10-0-190-90.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m [2021-04-23 17:47:52.713 ip-10-0-190-90.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:25,  1.15cell/s]#015Executing:   7%|         | 2/30 [00:01<00:23,  1.20cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.80cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.63cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.32cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01, 10.11cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 11.18cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.66cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.89cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 15.45cell/s]#015Executing:  77%|  | 23/30 [00:02<00:00, 15.41cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.80cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 14.01cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.99cell/s]#015Executing: 100%|| 30/30 [00:03<00:00,  7.82cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-HMOKGGKPNE-ProfilerReport-1619199808-a0c55ebe/algo-1-1619200019\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:52.922 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.850714', 'end_time': '2021-04-23T17:47:49.869465', 'duration': 0.018751, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:49.887137', 'end_time': '2021-04-23T17:47:50.676656', 'duration': 0.789519, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:49.911461Z', 'iopub.execute_input': '2021-04-23T17:47:49.911951Z', 'shell.execute_reply': '2021-04-23T17:47:50.676009Z', 'iopub.status.idle': '2021-04-23T17:47:50.676529Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 17:47:50.668 ip-10-2-238-88.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199815\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.696505', 'end_time': '2021-04-23T17:47:50.901665', 'duration': 0.20516, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.718145Z', 'iopub.execute_input': '2021-04-23T17:47:50.718626Z', 'iopub.status.idle': '2021-04-23T17:47:50.901138Z', 'shell.execute_reply': '2021-04-23T17:47:50.901524Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n     \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.920521', 'end_time': '2021-04-23T17:47:50.947214', 'duration': 0.026693, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.944806Z', 'iopub.execute_input': '2021-04-23T17:47:50.945365Z', 'shell.execute_reply': '2021-04-23T17:47:50.946721Z', 'iopub.status.idle': '2021-04-23T17:47:50.947115Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:50.965798', 'end_time': '2021-04-23T17:47:50.989943', 'duration': 0.024145, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:50.987615Z', 'iopub.execute_input': '2021-04-23T17:47:50.988086Z', 'iopub.status.idle': '2021-04-23T17:47:50.989456Z', 'shell.execute_reply': '2021-04-23T17:47:50.989822Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.008668', 'end_time': '2021-04-23T17:47:51.027294', 'duration': 0.018626, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.046305', 'end_time': '2021-04-23T17:47:51.070850', 'duration': 0.024545, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.068505Z', 'iopub.execute_input': '2021-04-23T17:47:51.069001Z', 'iopub.status.idle': '2021-04-23T17:47:51.070341Z', 'shell.execute_reply': '2021-04-23T17:47:51.070722Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.089686', 'end_time': '2021-04-23T17:47:51.122226', 'duration': 0.03254, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.118256Z', 'iopub.execute_input': '2021-04-23T17:47:51.119864Z', 'shell.execute_reply': '2021-04-23T17:47:51.121722Z', 'iopub.status.idle': '2021-04-23T17:47:51.122120Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.141252', 'end_time': '2021-04-23T17:47:51.183926', 'duration': 0.042674, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.173958Z', 'iopub.execute_input': '2021-04-23T17:47:51.174460Z', 'iopub.status.idle': '2021-04-23T17:47:51.183425Z', 'shell.execute_reply': '2021-04-23T17:47:51.183796Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"2eb9ea7c-7ba3-42ea-ba8a-cdebfa80fdda\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"8200aeca-1b93-477d-a4b2-2a30c860a71c\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1009\"},\"field\":\"0\",\"formatter\":{\"id\":\"1010\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringFormatter\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"17:46:34 04/23/2021\",\"17:46:59 04/23/2021\",\"25 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1013\"},\"selection_policy\":{\"id\":\"1014\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"editor\":{\"id\":\"1011\"},\"field\":\"1\",\"formatter\":{\"id\":\"1012\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringEditor\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 17:46:34 and ran for 25 seconds. \\\\n Your training job started on 04/23/2021 at 17:46:34 and ran for 25 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"Selection\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"8200aeca-1b93-477d-a4b2-2a30c860a71c\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"2eb9ea7c-7ba3-42ea-ba8a-cdebfa80fdda\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.204401', 'end_time': '2021-04-23T17:47:51.224278', 'duration': 0.019877, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.244265', 'end_time': '2021-04-23T17:47:51.269729', 'duration': 0.025464, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.267052Z', 'iopub.execute_input': '2021-04-23T17:47:51.267518Z', 'shell.execute_reply': '2021-04-23T17:47:51.269225Z', 'iopub.status.idle': '2021-04-23T17:47:51.269627Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-input\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m '], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.289588', 'end_time': '2021-04-23T17:47:51.318794', 'duration': 0.029206, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.316419Z', 'iopub.execute_input': '2021-04-23T17:47:51.316913Z', 'iopub.status.idle': '2021-04-23T17:47:51.318304Z', 'shell.execute_reply': '2021-04-23T17:47:51.318671Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.338987', 'end_time': '2021-04-23T17:47:51.386649', 'duration': 0.047662, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.367886Z', 'iopub.execute_input': '2021-04-23T17:47:51.368614Z', 'shell.execute_reply': '2021-04-23T17:47:51.386140Z', 'iopub.status.idle': '2021-04-23T17:47:51.386543Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"9c0908df-84f9-43e7-8c8d-20b03320c6bf\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"6051f27e-8e1e-44f0-8d0e-18300980b799\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1068\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1069\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1072\"},\"field\":\"max\",\"formatter\":{\"id\":\"1073\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1066\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1067\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQEjhehSuxzJA4XoUrkdBWEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAABmZmZmZrZVQMP1KFyPQixAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAADhehSuR8FYQArXo3A9SjFAAAAAAAAA8D8=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQK5H4XoUrjJArkfhehTuUUA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQD0K16NwvTJAcT0K16MgWEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1082\"},\"selection_policy\":{\"id\":\"1083\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1076\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1077\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"Selection\"},{\"attributes\":{\"editor\":{\"id\":\"1074\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1075\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1078\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1079\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringEditor\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{\"editor\":{\"id\":\"1070\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1071\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1080\"},\"field\":\"min\",\"formatter\":{\"id\":\"1081\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"StringFormatter\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"6051f27e-8e1e-44f0-8d0e-18300980b799\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"9c0908df-84f9-43e7-8c8d-20b03320c6bf\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.407478', 'end_time': '2021-04-23T17:47:51.447604', 'duration': 0.040126, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.441727Z', 'iopub.execute_input': '2021-04-23T17:47:51.444419Z', 'shell.execute_reply': '2021-04-23T17:47:51.447095Z', 'iopub.status.idle': '2021-04-23T17:47:51.447503Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.469310', 'end_time': '2021-04-23T17:47:51.500319', 'duration': 0.031009, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.497987Z', 'iopub.execute_input': '2021-04-23T17:47:51.498481Z', 'shell.execute_reply': '2021-04-23T17:47:51.499739Z', 'iopub.status.idle': '2021-04-23T17:4\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m 7:51.500219Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.521777', 'end_time': '2021-04-23T17:47:51.552633', 'duration': 0.030856, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.550362Z', 'iopub.execute_input': '2021-04-23T17:47:51.550854Z', 'shell.execute_reply': '2021-04-23T17:47:51.552060Z', 'iopub.status.idle': '2021-04-23T17:47:51.552534Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.574268', 'end_time': '2021-04-23T17:47:51.595661', 'duration': 0.021393, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.617163', 'end_time': '2021-04-23T17:47:51.645535', 'duration': 0.028372, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.643147Z', 'iopub.execute_input': '2021-04-23T17:47:51.643646Z', 'shell.execute_reply': '2021-04-23T17:47:51.644990Z', 'iopub.status.idle': '2021-04-23T17:47:51.645431Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.667032', 'end_time': '2021-04-23T17:47:51.694875', 'duration': 0.027843, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.692545Z', 'iopub.execute_input': '2021-04-23T17:47:51.693032Z', 'iopub.status.idle': '2021-04-23T17:47:51.694367Z', 'shell.execute_reply': '2021-04-23T17:47:51.694745Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.716871', 'end_time': '2021-04-23T17:47:51.754761', 'duration': 0.03789, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.743711Z', 'iopub.execute_input': '2021-04-23T17:47:51.744186Z', 'iopub.status.idle': '2021-04-23T17:47:51.754255Z', 'shell.execute_reply': '2021-04-23T17:47:51.754636Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the LowGPUUtilization rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>56</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>56</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>50</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\n  \r\n",
      "  summary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.777609', 'end_time': '2021-04-23T17:47:51.808672', 'duration': 0.031063, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.805504Z', 'iopub.execute_input': '2021-04-23T17:47:51.806259Z', 'iopub.status.idle': '2021-04-23T17:47:51.808148Z', 'shell.execute_reply': '2021-04-23T17:47:51.808547Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.832152', 'end_time': '2021-04-23T17:47:51.860875', 'duration': 0.028723, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.858636Z', 'iopub.execute_input': '2021-04-23T17:47:51.859121Z', 'shell.execute_reply': '2021-04-23T17:47:51.860373Z', 'iopub.status.idle': '2021-04-23T17:47:51.860774Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.884315', 'end_time': '2021-04-23T17:47:51.937597', 'duration': 0.053282, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.909949Z', 'iopub.execute_input': '2021-04-23T17:47:51.928721Z', 'shell.execute_reply': '2021-04-23T17:47:51.936981Z', 'iopub.status.idle': '2021-04-23T17:47:51.937492Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"6982b904-ce19-4c3e-a887-d5c6fb02ef24\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"16b35e11-a777-49b0-b212-de769f534cd0\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"16b35e11-a777-49b0-b212-de769f534cd0\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"6982b904-ce19-4c3e-a887-d5c6fb02ef24\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:51.962582', 'end_time': '2021-04-23T17:47:52.011065', 'duration': 0.048483, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:51.994846Z', 'iopub.execute_input': '2021-04-23T17:47:52.001840Z', 'iopub.status.idle': '2021-04-23T17:47:52.010557Z', 'shell.execute_reply': '2021-0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m 4-23T17:47:52.010932Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"901ed0d2-88fc-4bf9-8c9b-53c1bd62aadc\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"aef4e0a1-36a5-4ed7-972c-029a354c0c8e\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"aef4e0a1-36a5-4ed7-972c-029a354c0c8e\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"901ed0d2-88fc-4bf9-8c9b-53c1bd62aadc\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:52.037680', 'end_time': '2021-04-23T17:47:52.082534', 'duration': 0.044854, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:52.072409Z', 'iopub.execute_input': '2021-04-23T17:47:52.074668Z', 'shell.execute_reply': '2021-04-23T17:47:52.082027Z', 'iopub.status.idle': '2021-04-23T17:47:52.082431Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"2ea36e05-9384-4b66-835a-dfe1996c3c8a\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"485c4a79-97b4-4f1e-98bf-eabe53f400bc\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"485c4a79-97b4-4f1e-98bf-eabe53f400bc\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"2ea36e05-9384-4b66-835a-dfe1996c3c8a\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:52.110490', 'end_time': '2021-04-23T17:47:52.161640', 'duration': 0.05115, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:52.147516Z', 'iopub.execute_input': '2021-04-23T17:47:52.149600Z', 'iopub.status.idle': '2021-04-23T17:47:52.161133Z', 'shell.execute_reply': '2021-04-23T17:47:52.161512Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8f16f0ba-b4b6-4bf1-a223-0b4749e728e2\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"0f0c0067-b7a8-4c82-9a85-1386862adadb\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m width\":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"0f0c0067-b7a8-4c82-9a85-1386862adadb\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"8f16f0ba-b4b6-4bf1-a223-0b4749e728e2\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d267d9ef-dee2-46a3-850b-9b3da05ce796\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"148caac5-74b9-4ffb-9dca-6bc239a9c95b\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"148caac5-74b9-4ffb-9dca-6bc239a9c95b\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"d267d9ef-dee2-46a3-850b-9b3da05ce796\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:52.191261', 'end_time': '2021-04-23T17:47:52.241261', 'duration': 0.05, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:52.233322Z', 'iopub.execute_input': '2021-04-23T17:47:52.233840Z', 'shell.execute_reply': '2021-04-23T17:47:52.240737Z', 'iopub.status.idle': '2021-04-23T17:47:52.241156Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"ee365641-4648-42e5-83cc-afc04f569b17\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"a387ec2a-7901-4285-b1fa-daf50fd5a92a\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 50 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"a387ec2a-7901-4285-b1fa-daf50fd5a92a\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"ee365641-4648-42e5-83cc-afc04f569b17\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:52.272470', 'end_time': '2021-04-23T17:47:52.329324', 'duration': 0.056854, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:52.305696Z', 'iopub.execute_input': '2021-04-23T17:47:52.321302Z', 'shell.execute_reply': '2021-04-23T17:47:52.328770Z', 'iopub.status.idle': '2021-04-23T17:47:52.329205Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"1ed631ae-76ed-4e20-9295-723d5dd311b6\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"d427e28c-68da-4d59-9e1e-8e6a38835697\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 56 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"d427e28c-68da-4d59-9e1e-8e6a38835697\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"1ed631ae-76ed-4e20-9295-723d5dd311b6\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (a\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m ttempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:52.361544', 'end_time': '2021-04-23T17:47:52.419857', 'duration': 0.058313, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:52.408825Z', 'iopub.execute_input': '2021-04-23T17:47:52.412181Z', 'iopub.status.idle': '2021-04-23T17:47:52.419358Z', 'shell.execute_reply': '2021-04-23T17:47:52.419731Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"8c382829-73e2-41c0-9767-cdb28e8cc09c\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"f3260b68-97af-4073-8252-b8c72d37da0d\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 56 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"f3260b68-97af-4073-8252-b8c72d37da0d\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"8c382829-73e2-41c0-9767-cdb28e8cc09c\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n        \r\n",
      "                                    height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:47:52.453919', 'end_time': '2021-04-23T17:47:52.509807', 'duration': 0.055888, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:47:52.499209Z', 'iopub.execute_input': '2021-04-23T17:47:52.501938Z', 'iopub.status.idle': '2021-04-23T17:47:52.509300Z', 'shell.execute_reply': '2021-04-23T17:47:52.509676Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"5da3ed47-1ace-4f1d-8bef-0c016846463a\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"183eb614-988d-4779-8ccc-b532e20e3d55\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"183eb614-988d-4779-8ccc-b532e20e3d55\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"5da3ed47-1ace-4f1d-8bef-0c016846463a\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T17:47:48.820460', 'end_time': '2021-04-23T17:47:52.850574', 'duration': 4.030114, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:52.922 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:52.922 ip-10-2-238-88.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:53.331 ip-10-2-238-88.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619200080000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m [2021-04-23 17:47:53.331 ip-10-2-238-88.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m #015Executing:   3%|         | 1/30 [00:01<00:30,  1.07s/cell]#015Executing:   7%|         | 2/30 [00:01<00:25,  1.09cell/s]#015Executing:  10%|         | 3/30 [00:02<00:16,  1.67cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.34cell/s]#015Executing:  30%|       | 9/30 [00:02<00:03,  6.94cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01,  9.67cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 10.79cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.32cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.62cell/s]#015Executing:  70%|   | 21/30 [00:03<00:00, 15.26cell/s]#015Executing:  77%|  | 23/30 [00:03<00:00, 15.28cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.74cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 13.96cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.96cell/s]#015Executing: 100%|| 30/30 [00:04<00:00,  7.44cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-EVQTDKPOVS-ProfilerReport-1619199815-10ccf4be/algo-1-1619200016\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:00.681 ip-10-0-133-241.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199818\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:01.176 ip-10-0-133-241.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.178 ip-10-0-133-241.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.179 ip-10-0-133-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.179 ip-10-0-133-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.179 ip-10-0-133-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.179 ip-10-0-133-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.179 ip-10-0-133-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.179 ip-10-0-133-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.179 ip-10-0-133-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.179 ip-10-0-133-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.180 ip-10-0-133-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.180 ip-10-0-133-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.180 ip-10-0-133-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.180 ip-10-0-133-241.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.180 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.180 ip-10-0-133-241.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.181 ip-10-0-133-241.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.218 ip-10-0-133-241.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.218 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.219 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.221 ip-10-0-133-241.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.221 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.221 ip-10-0-133-241.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.221 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.222 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.223 ip-10-0-133-241.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.224 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.224 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.224 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.225 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.228 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:11.229 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619199960000000 to timestamp_end:1619200020000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:15.305 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:12.117374', 'end_time': '2021-04-23T17:48:12.136471', 'duration': 0.019097, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:12.153732', 'end_time': '2021-04-23T17:48:12.971308', 'duration': 0.817576, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:12.177930Z', 'iopub.execute_input': '2021-04-23T17:48:12.178434Z', 'shell.execute_reply': '2021-04-23T17:48:12.970625Z', 'iopub.status.idle': '2021-04-23T17:48:12.971179Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 17:48:12.963 ip-10-0-133-241.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619199818\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:12.990628', 'end_time': '2021-04-23T17:48:13.202470', 'duration': 0.211842, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.012636Z', 'iopub.execute_input': '2021-04-23T17:48:13.013135Z', 'shell.execute_reply': '2021-04-23T17:48:13.201944Z', 'iopub.status.idle': '2021-04-23T17:48:13.202353Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n  \r\n",
      "    if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.221101', 'end_time': '2021-04-23T17:48:13.248780', 'duration': 0.027679, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.246446Z', 'iopub.execute_input': '2021-04-23T17:48:13.246963Z', 'iopub.status.idle': '2021-04-23T17:48:13.248253Z', 'shell.execute_reply': '2021-04-23T17:48:13.248648Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.267170', 'end_time': '2021-04-23T17:48:13.291550', 'duration': 0.02438, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.289270Z', 'iopub.execute_input': '2021-04-23T17:48:13.289772Z', 'iopub.status.idle': '2021-04-23T17:48:13.291033Z', 'shell.execute_reply': '2021-04-23T17:48:13.291416Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.309682', 'end_time': '2021-04-23T17:48:13.327830', 'duration': 0.018148, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.346355', 'end_time': '2021-04-23T17:48:13.370741', 'duration': 0.024386, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.368319Z', 'iopub.execute_input': '2021-04-23T17:48:13.368859Z', 'iopub.status.idle': '2021-04-23T17:48:13.370190Z', 'shell.execute_reply': '2021-04-23T17:48:13.370584Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.389139', 'end_time': '2021-04-23T17:48:13.421231', 'duration': 0.032092, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.416397Z', 'iopub.execute_input': '2021-04-23T17:48:13.419089Z', 'shell.execute_reply': '2021-04-23T17:48:13.420708Z', 'iopub.status.idle': '2021-04-23T17:48:13.421121Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.439772', 'end_time': '2021-04-23T17:48:13.482364', 'duration': 0.042592, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.466662Z', 'iopub.execute_input': '2021-04-23T17:48:13.469481Z', 'shell.execute_reply': '2021-04-23T17:48:13.481836Z', 'iopub.status.idle': '2021-04-23T17:48:13.482250Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"e77db37f-0147-4470-8673-a244452fe752\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"f3ad5b9d-20d0-4d48-a259-fc82f999a5ff\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringFormatter\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{\"editor\":{\"id\":\"1009\"},\"field\":\"0\",\"formatter\":{\"id\":\"1010\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"17:46:20 04/23/2021\",\"17:46:59 04/23/2021\",\"39 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1013\"},\"selection_policy\":{\"id\":\"1014\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"StringEditor\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 17:46:20 and ran for 39 seconds. \\\\n Your training job started on 04/23/2021 at 17:46:20 and ran for 39 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"editor\":{\"id\":\"1011\"},\"field\":\"1\",\"formatter\":{\"id\":\"1012\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"Selection\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"f3ad5b9d-20d0-4d48-a259-fc82f999a5ff\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"e77db37f-0147-4470-8673-a244452fe752\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.501894', 'end_time': '2021-04-23T17:48:13.522058', 'duration': 0.020164, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.541451', 'end_time': '2021-04-23T17:48:13.566230', 'duration': 0.024779, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.563727Z', 'iopub.execute_input': '2021-04-23T17:48:13.564230Z', 'shell.execute_reply': '2021-04-23T17:48:13.565729Z', 'iopub.status.idle': '2021-04-23T17:48:13.566128Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-in\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m put'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.585665', 'end_time': '2021-04-23T17:48:13.614259', 'duration': 0.028594, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.612030Z', 'iopub.execute_input': '2021-04-23T17:48:13.612601Z', 'iopub.status.idle': '2021-04-23T17:48:13.613754Z', 'shell.execute_reply': '2021-04-23T17:48:13.614129Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.633935', 'end_time': '2021-04-23T17:48:13.679990', 'duration': 0.046055, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.661214Z', 'iopub.execute_input': '2021-04-23T17:48:13.676519Z', 'iopub.status.idle': '2021-04-23T17:48:13.679447Z', 'shell.execute_reply': '2021-04-23T17:48:13.679851Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"64d09887-1448-4598-97f3-2bac9758fb23\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"fba0957f-8dff-4a33-a516-86e537846260\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringEditor\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"editor\":{\"id\":\"1068\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1069\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1070\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1071\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1080\"},\"field\":\"min\",\"formatter\":{\"id\":\"1081\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringFormatter\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringEditor\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQD0K16NwPTJAH4XrUbh+WEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAAAK16NwPUpIQK5H4XoULixAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAAAfhetRuL5YQClcj8L16DBAexSuR+F6EEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQBSuR+F6FDJApHA9CtcDWEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQDMzMzMzMzJAcT0K16NQWEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1082\"},\"selection_policy\":{\"id\":\"1083\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1072\"},\"field\":\"max\",\"formatter\":{\"id\":\"1073\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1076\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1077\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1066\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1067\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{\"editor\":{\"id\":\"1074\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1075\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1078\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1079\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"fba0957f-8dff-4a33-a516-86e537846260\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"64d09887-1448-4598-97f3-2bac9758fb23\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.700998', 'end_time': '2021-04-23T17:48:13.740850', 'duration': 0.039852, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.735136Z', 'iopub.execute_input': '2021-04-23T17:48:13.735679Z', 'shell.execute_reply': '2021-04-23T17:48:13.740298Z', 'iopub.status.idle': '2021-04-23T17:48:13.740737Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.761998', 'end_time': '2021-04-23T17:48:13.793112', 'duration': 0.031114, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.790742Z', 'iopub.execute_input': '2021-04-23T17:48:13.791284Z', 'shell.execute_reply': '2021-04-23T17:48:13.792575Z', 'iopub.status.idle': '2021-04-23T1\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m 7:48:13.793000Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.814151', 'end_time': '2021-04-23T17:48:13.845304', 'duration': 0.031153, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.843012Z', 'iopub.execute_input': '2021-04-23T17:48:13.843540Z', 'iopub.status.idle': '2021-04-23T17:48:13.844790Z', 'shell.execute_reply': '2021-04-23T17:48:13.845172Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.866676', 'end_time': '2021-04-23T17:48:13.888283', 'duration': 0.021607, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.909247', 'end_time': '2021-04-23T17:48:13.937740', 'duration': 0.028493, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.935069Z', 'iopub.execute_input': '2021-04-23T17:48:13.935610Z', 'shell.execute_reply': '2021-04-23T17:48:13.937049Z', 'iopub.status.idle': '2021-04-23T17:48:13.937588Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:13.959464', 'end_time': '2021-04-23T17:48:13.987374', 'duration': 0.02791, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:13.984946Z', 'iopub.execute_input': '2021-04-23T17:48:13.985455Z', 'iopub.status.idle': '2021-04-23T17:48:13.986855Z', 'shell.execute_reply': '2021-04-23T17:48:13.987237Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:14.009085', 'end_time': '2021-04-23T17:48:14.047919', 'duration': 0.038834, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:14.036064Z', 'iopub.execute_input': '2021-04-23T17:48:14.036660Z', 'shell.execute_reply': '2021-04-23T17:48:14.047383Z', 'iopub.status.idle': '2021-04-23T17:48:14.047808Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the GPUMemoryIncrease rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>87</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>87</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>79</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\'] \\\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m n    summary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:14.070326', 'end_time': '2021-04-23T17:48:14.101022', 'duration': 0.030696, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:14.097822Z', 'iopub.execute_input': '2021-04-23T17:48:14.098338Z', 'iopub.status.idle': '2021-04-23T17:48:14.100489Z', 'shell.execute_reply': '2021-04-23T17:48:14.100886Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:14.124112', 'end_time': '2021-04-23T17:48:14.152639', 'duration': 0.028527, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:14.150251Z', 'iopub.execute_input': '2021-04-23T17:48:14.150752Z', 'iopub.status.idle': '2021-04-23T17:48:14.152077Z', 'shell.execute_reply': '2021-04-23T17:48:14.152504Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:14.175671', 'end_time': '2021-04-23T17:48:14.229298', 'duration': 0.053627, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:14.220556Z', 'iopub.execute_input': '2021-04-23T17:48:14.221174Z', 'iopub.status.idle': '2021-04-23T17:48:14.228785Z', 'shell.execute_reply': '2021-04-23T17:48:14.229160Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"f74576f8-d6b5-4389-8014-f9cd4f9ee4fd\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"372c3419-5c6c-4345-a192-7f0c3f65a4fe\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"372c3419-5c6c-4345-a192-7f0c3f65a4fe\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"f74576f8-d6b5-4389-8014-f9cd4f9ee4fd\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:14.253648', 'end_time': '2021-04-23T17:48:14.301636', 'duration': 0.047988, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:14.292558Z', 'iopub.execute_input': '2021-04-23T17:48:14.293148Z', 'iopub.status.idle': '2021-04-23T17:48:14.301122Z', 'shell.execute_reply': '202\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m 1-04-23T17:48:14.301499Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"48e9d7c0-f815-42a9-8527-f6aea727e653\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"12716c81-250c-43a9-9744-12a348e7f91a\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"12716c81-250c-43a9-9744-12a348e7f91a\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"48e9d7c0-f815-42a9-8527-f6aea727e653\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:14.327465', 'end_time': '2021-04-23T17:48:14.372322', 'duration': 0.044857, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:14.364139Z', 'iopub.execute_input': '2021-04-23T17:48:14.364724Z', 'iopub.status.idle': '2021-04-23T17:48:14.371776Z', 'shell.execute_reply': '2021-04-23T17:48:14.372151Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"a657cf5e-492c-4e9e-9bc2-99d7e917c314\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"09dbacc9-2761-42f0-ab9d-e0a2995c8512\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"09dbacc9-2761-42f0-ab9d-e0a2995c8512\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"a657cf5e-492c-4e9e-9bc2-99d7e917c314\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:14.399548', 'end_time': '2021-04-23T17:48:14.450475', 'duration': 0.050927, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:14.437721Z', 'iopub.execute_input': '2021-04-23T17:48:14.438253Z', 'iopub.status.idle': '2021-04-23T17:48:14.449959Z', 'shell.execute_reply': '2021-04-23T17:48:14.450335Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"dccbb25e-4425-454a-a2fd-4579f6cc00f1\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"fc82f515-b0b9-4ba4-b44d-64b9ff40c049\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 times\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m .\",\"width\":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"fc82f515-b0b9-4ba4-b44d-64b9ff40c049\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"dccbb25e-4425-454a-a2fd-4579f6cc00f1\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"e4d0e4cb-f59e-483c-83d0-f7665db1ef8f\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"223ffd7e-6daa-48be-b132-e8e39a64c7d8\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"223ffd7e-6daa-48be-b132-e8e39a64c7d8\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"e4d0e4cb-f59e-483c-83d0-f7665db1ef8f\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:14.479361', 'end_time': '2021-04-23T17:48:14.529137', 'duration': 0.049776, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:14.521036Z', 'iopub.execute_input': '2021-04-23T17:48:14.521587Z', 'iopub.status.idle': '2021-04-23T17:48:14.528618Z', 'shell.execute_reply': '2021-04-23T17:48:14.528998Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"a4599b3f-f127-4c49-8e13-8e00cc6c3476\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"5b5d3388-d05b-4e87-a50c-b2438ff09fc5\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 79 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"5b5d3388-d05b-4e87-a50c-b2438ff09fc5\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"a4599b3f-f127-4c49-8e13-8e00cc6c3476\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:14.559390', 'end_time': '2021-04-23T17:48:14.616690', 'duration': 0.0573, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:14.608257Z', 'iopub.execute_input': '2021-04-23T17:48:14.608874Z', 'iopub.status.idle': '2021-04-23T17:48:14.616113Z', 'shell.execute_reply': '2021-04-23T17:48:14.616547Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"b41a103d-e0f6-4096-af2c-b4fab0982a3a\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"c873d5d0-8a39-4239-acec-8c2c7261dc46\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 87 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"c873d5d0-8a39-4239-acec-8c2c7261dc46\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"b41a103d-e0f6-4096-af2c-b4fab0982a3a\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n       \r\n",
      " if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:14.648155', 'end_time': '2021-04-23T17:48:14.706728', 'duration': 0.058573, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:14.698030Z', 'iopub.execute_input': '2021-04-23T17:48:14.698636Z', 'iopub.status.idle': '2021-04-23T17:48:14.706213Z', 'shell.execute_reply': '2021-04-23T17:48:14.706590Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"5cdbd12d-dac7-4b60-9881-a2a2815b4f77\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"a9c868f5-5a73-43a1-8105-640e740c7a15\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 87 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"a9c868f5-5a73-43a1-8105-640e740c7a15\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"5cdbd12d-dac7-4b60-9881-a2a2815b4f77\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n  \r\n",
      "                                          height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:48:14.739599', 'end_time': '2021-04-23T17:48:14.795233', 'duration': 0.055634, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:48:14.786897Z', 'iopub.execute_input': '2021-04-23T17:48:14.787486Z', 'iopub.status.idle': '2021-04-23T17:48:14.794713Z', 'shell.execute_reply': '2021-04-23T17:48:14.795094Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"29cacf63-fe95-442b-b075-7a9f5de98188\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"51c1e6ba-5089-46d7-9e47-12d78146c46d\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"51c1e6ba-5089-46d7-9e47-12d78146c46d\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"29cacf63-fe95-442b-b075-7a9f5de98188\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T17:48:11.255877', 'end_time': '2021-04-23T17:48:15.236137', 'duration': 3.98026, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:15.306 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:15.306 ip-10-0-133-241.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:15.704 ip-10-0-133-241.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619200080000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m [2021-04-23 17:48:15.704 ip-10-0-133-241.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36mjob-XPZFEXSNFC-ProfilerReport-1619199818-540cd369/algo-1-1619200037\u001b[0m #015Executing:   3%|         | 1/30 [00:00<00:26,  1.11cell/s]#015Executing:   7%|         | 2/30 [00:01<00:24,  1.16cell/s]#015Executing:  10%|         | 3/30 [00:01<00:15,  1.74cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.51cell/s]#015Executing:  30%|       | 9/30 [00:02<00:02,  7.19cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01,  9.99cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 11.11cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 12.62cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.89cell/s]#015Executing:  70%|   | 21/30 [00:02<00:00, 15.48cell/s]#015Executing:  77%|  | 23/30 [00:02<00:00, 15.47cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.93cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 14.14cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 13.10cell/s]#015Executing: 100%|| 30/30 [00:03<00:00,  7.54cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:14.678 ip-10-2-219-252.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619200479\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:15.173 ip-10-2-219-252.ec2.internal:1 INFO profiler_trial.py:67] Waiting for profiler data.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.183 ip-10-2-219-252.ec2.internal:1 INFO profiler_trial.py:37] Output files of ProfilerTrial will be saved to /opt/ml/processing/output/rule\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m No environment variable found with name \"scan_interval_us\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m No environment variable found with name \"nb_path\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m No environment variable found with name \"custom_rule_parameters\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.184 ip-10-2-219-252.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.184 ip-10-2-219-252.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.184 ip-10-2-219-252.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.184 ip-10-2-219-252.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.184 ip-10-2-219-252.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.184 ip-10-2-219-252.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.184 ip-10-2-219-252.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.184 ip-10-2-219-252.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.184 ip-10-2-219-252.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.184 ip-10-2-219-252.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.184 ip-10-2-219-252.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.185 ip-10-2-219-252.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.185 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:89] Output files of ProfilerReport Rule will be saved to /opt/ml/processing/output/rule/profiler-output/profiler-reports\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.185 ip-10-2-219-252.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule ProfilerReport at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.186 ip-10-2-219-252.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 2 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.244 ip-10-2-219-252.ec2.internal:1 INFO metrics_reader_base.py:134] Getting 0 event files\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.244 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:BatchSize for timestamp_start:1619200620000000 to timestamp_end:1619200680000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.245 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:CPUBottleneck for timestamp_start:1619200620000000 to timestamp_end:1619200680000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.249 ip-10-2-219-252.ec2.internal:1 INFO cpu_bottleneck.py:164] Found 0 CPU bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.249 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:Dataloader for timestamp_start:1619200620000000 to timestamp_end:1619200680000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.250 ip-10-2-219-252.ec2.internal:1 INFO dataloader.py:185] No dataloading metrics found.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.250 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:GPUMemoryIncrease for timestamp_start:1619200620000000 to timestamp_end:1619200680000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.251 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:IOBottleneck for timestamp_start:1619200620000000 to timestamp_end:1619200680000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.254 ip-10-2-219-252.ec2.internal:1 INFO io_bottleneck.py:163] Found 0 IO bottlenecks\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.254 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LoadBalancing for timestamp_start:1619200620000000 to timestamp_end:1619200680000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.254 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:LowGPUUtilization for timestamp_start:1619200620000000 to timestamp_end:1619200680000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.255 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:MaxInitializationTime for timestamp_start:1619200620000000 to timestamp_end:1619200680000000\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.257 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallSystemUsage for timestamp_start:1619200620000000 to timestamp_end:1619200680000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.262 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:OverallFrameworkMetrics for timestamp_start:1619200620000000 to timestamp_end:1619200680000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:25.262 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:161] Invoking rule:StepOutlier for timestamp_start:1619200620000000 to timestamp_end:1619200680000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m #015Executing:   0%|          | 0/30 [00:00<?, ?cell/s]/usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m /usr/local/lib/python3.7/site-packages/papermill/iorw.py:126: UserWarning: The specified input file (/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp) does not end in one of ['.ipynb', '.json']\r\n",
      "  \"The specified input file ({}) does not end in one of {}\".format(path, extensions)\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:29.590 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:104] notebook execute return code:{'cells': [{'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:26.307666', 'end_time': '2021-04-23T17:59:26.326809', 'duration': 0.019143, 'status': 'completed'}}, 'source': '# SageMaker Debugger Profiling Report\\n\\nSageMaker Debugger auto generated this report. You can generate similar reports on all supported training jobs. The report provides summary of training job, system resource usage statistics, framework metrics, rules summary, and detailed analysis from each rule. The graphs and tables are interactive. \\n\\n**Legal disclaimer:** This report and any recommendations are provided for informational purposes only and are not definitive. You are responsible for making your own independent assessment of the information.\\n'}, {'cell_type': 'code', 'execution_count': 1, 'metadata': {'tags': ['hide-output', 'hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:26.345669', 'end_time': '2021-04-23T17:59:27.178568', 'duration': 0.832899, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:26.370851Z', 'iopub.execute_input': '2021-04-23T17:59:26.371359Z', 'iopub.status.idle': '2021-04-23T17:59:27.177913Z', 'shell.execute_reply': '2021-04-23T17:59:27.178332Z'}}, 'outputs': [{'output_type': 'stream', 'name': 'stdout', 'text': '[2021-04-23 17:59:27.169 ip-10-2-219-252.ec2.internal:26 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: /opt/ml/processing/input/profiler/signals/ProfilerReport-1619200479\\n'}], 'source': 'import json\\nimport pandas as pd\\nimport glob\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport datetime\\nfrom smdebug.profiler.utils import us_since_epoch_to_human_readable_time, ns_since_epoch_to_human_readable_time\\n'}, {'cell_type': 'code', 'execution_count': 2, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:27.197592', 'end_time': '2021-04-23T17:59:27.414217', 'duration': 0.216625, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:27.220490Z', 'iopub.execute_input': '2021-04-23T17:59:27.221404Z', 'iopub.status.idle': '2021-04-23T17:59:27.413668Z', 'shell.execute_reply': '2021-04-23T17:59:27.414067Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'application/javascript': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  var JS_MIME_TYPE = \\'application/javascript\\';\\n  var HTML_MIME_TYPE = \\'text/html\\';\\n  var EXEC_MIME_TYPE = \\'application/vnd.bokehjs_exec.v0+json\\';\\n  var CLASS_NAME = \\'output_bokeh rendered_html\\';\\n\\n  /**\\n   * Render data to the DOM node\\n   */\\n  function render(props, node) {\\n    var script = document.createElement(\"script\");\\n    node.appendChild(script);\\n  }\\n\\n  /**\\n   * Handle when an output is cleared or removed\\n   */\\n  function handleClearOutput(event, handle) {\\n    var cell = handle.cell;\\n\\n    var id = cell.output_area._bokeh_element_id;\\n    var server_id = cell.output_area._bokeh_server_id;\\n    // Clean up Bokeh references\\n    if (id != null && id in Bokeh.index) {\\n      Bokeh.index[id].model.document.clear();\\n      delete Bokeh.index[id];\\n    }\\n\\n    if (server_id !== undefined) {\\n      // Clean up Bokeh references\\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server[\\'\" + server_id + \"\\'].get_sessions()[0].document.roots[0]._id)\";\\n      cell.notebook.kernel.execute(cmd, {\\n        iopub: {\\n          output: function(msg) {\\n            var id = msg.content.text.trim();\\n            if (id in Bokeh.index) {\\n              Bokeh.index[id].model.document.clear();\\n              delete Bokeh.index[id];\\n            }\\n          }\\n        }\\n      });\\n      // Destroy server and session\\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server(\\'\" + server_id + \"\\')\";\\n      cell.notebook.kernel.execute(cmd);\\n    }\\n  }\\n\\n  /**\\n   * Handle when a new output is added\\n   */\\n  function handleAddOutput(event, handle) {\\n    var output_area = handle.output_area;\\n    var output = handle.output;\\n\\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\\n      return\\n    }\\n\\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(\\' \\')[0]);\\n\\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\\n      // store reference to embed id on output_area\\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\\n    }\\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\\n      var bk_div = document.createElement(\"div\");\\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\\n      var script_attrs = bk_div.children[0].attributes;\\n      for (var i = 0; i < script_attrs.length; i++) {\\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\\n      }\\n      // store reference to server id on output_area\\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\\n    }\\n  }\\n\\n  function register_renderer(events, OutputArea) {\\n\\n    function append_mime(data, metadata, element) {\\n      // create a DOM node to render to\\n      var toinsert = this.create_output_subarea(\\n        metadata,\\n        CLASS_NAME,\\n        EXEC_MIME_TYPE\\n      );\\n      this.keyboard_manager.register_events(toinsert);\\n      // Render to node\\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\\n      render(props, toinsert[toinsert.length - 1]);\\n      element.append(toinsert);\\n      return toinsert\\n    }\\n\\n    /* Handle when an output is cleared or removed */\\n    events.on(\\'clear_output.CodeCell\\', handleClearOutput);\\n    events.on(\\'delete.Cell\\', handleClearOutput);\\n\\n    /* Handle when a new output is added */\\n    events.on(\\'output_added.OutputArea\\', handleAddOutput);\\n\\n    /**\\n     * Register the mime type and append_mime function with output_area\\n     */\\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\\n      /* Is output safe? */\\n      safe: true,\\n      /* Index of renderer in `output_area.display_order` */\\n      index: 0\\n    });\\n  }\\n\\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\\n  if (root.Jupyter !== undefined) {\\n    var events = require(\\'base/js/events\\');\\n    var OutputArea = require(\\'notebook/js/outputarea\\').OutputArea;\\n\\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\\n      register_renderer(events, OutputArea);\\n    }\\n  }\\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n      if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));', 'application/vnd.bokehjs_load.v0+json': '\\n(function(root) {\\n  function now() {\\n    return new Date();\\n  }\\n\\n  var force = true;\\n\\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\\n    root._bokeh_onload_callbacks = [];\\n    root._bokeh_is_loading = undefined;\\n  }\\n\\n  \\n\\n  \\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\\n    root._bokeh_timeout = Date.now() + 5000;\\n    root._bokeh_failed_load = false;\\n  }\\n\\n  var NB_LOAD_WARNING = {\\'data\\': {\\'text/html\\':\\n     \"<div style=\\'background-color: #fdd\\'>\\\\n\"+\\n     \"<p>\\\\n\"+\\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\\\n\"+\\n     \"may be due to a slow or bad network connection. Possible fixes:\\\\n\"+\\n     \"</p>\\\\n\"+\\n     \"<ul>\\\\n\"+\\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\\\n\"+\\n     \"<li>use INLINE resources instead, as so:</li>\\\\n\"+\\n     \"</ul>\\\\n\"+\\n     \"<code>\\\\n\"+\\n     \"from bokeh.resources import INLINE\\\\n\"+\\n     \"output_notebook(resources=INLINE)\\\\n\"+\\n     \"</code>\\\\n\"+\\n     \"</div>\"}};\\n\\n  function display_loaded() {\\n    var el = document.getElementById(null);\\n    if (el != null) {\\n      el.textContent = \"BokehJS is loading...\";\\n    }\\n    if (root.Bokeh !== undefined) {\\n      if (el != null) {\\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\\n      }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(display_loaded, 100)\\n    }\\n  }\\n\\n\\n  function run_callbacks() {\\n    try {\\n      root._bokeh_onload_callbacks.forEach(function(callback) {\\n        if (callback != null)\\n          callback();\\n      });\\n    } finally {\\n      delete root._bokeh_onload_callbacks\\n    }\\n    console.debug(\"Bokeh: all callbacks have finished\");\\n  }\\n\\n  function load_libs(css_urls, js_urls, callback) {\\n    if (css_urls == null) css_urls = [];\\n    if (js_urls == null) js_urls = [];\\n\\n    root._bokeh_onload_callbacks.push(callback);\\n    if (root._bokeh_is_loading > 0) {\\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\\n      return null;\\n    }\\n    if (js_urls == null || js_urls.length === 0) {\\n      run_callbacks();\\n      return null;\\n    }\\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\\n\\n    function on_load() {\\n      root._bokeh_is_loading--;\\n      if (root._bokeh_is_loading === 0) {\\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\\n        run_callbacks()\\n      }\\n    }\\n\\n    function on_error() {\\n      console.error(\"failed to load \" + url);\\n    }\\n\\n    for (var i = 0; i < css_urls.length; i++) {\\n      var url = css_urls[i];\\n      const element = document.createElement(\"link\");\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.rel = \"stylesheet\";\\n      element.type = \"text/css\";\\n      element.href = url;\\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\\n      document.body.appendChild(element);\\n    }\\n\\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\": \"T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\": \"98GDGJ0kOMCUMUePhksaQ/GYgB3+NH9h996V88sh3aOiUNX3N+fLXAtry6xctSZ6\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\": \"89bArO+nlbP3sgakeHjCo1JYxYR5wufVgA3IbUvDY+K7w4zyxJqssu7wVnfeKCq8\"};\\n\\n    for (var i = 0; i < js_urls.length; i++) {\\n      var url = js_urls[i];\\n      var element = document.createElement(\\'script\\');\\n      element.onload = on_load;\\n      element.onerror = on_error;\\n      element.async = false;\\n      element.src = url;\\n  \r\n",
      "    if (url in hashes) {\\n        element.crossOrigin = \"anonymous\";\\n        element.integrity = \"sha384-\" + hashes[url];\\n      }\\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\\n      document.head.appendChild(element);\\n    }\\n  };\\n\\n  function inject_raw_css(css) {\\n    const element = document.createElement(\"style\");\\n    element.appendChild(document.createTextNode(css));\\n    document.body.appendChild(element);\\n  }\\n\\n  \\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\"];\\n  var css_urls = [];\\n  \\n\\n  var inline_js = [\\n    function(Bokeh) {\\n      Bokeh.set_log_level(\"info\");\\n    },\\n    function(Bokeh) {\\n    \\n    \\n    }\\n  ];\\n\\n  function run_inline_js() {\\n    \\n    if (root.Bokeh !== undefined || force === true) {\\n      \\n    for (var i = 0; i < inline_js.length; i++) {\\n      inline_js[i].call(root, root.Bokeh);\\n    }\\n    } else if (Date.now() < root._bokeh_timeout) {\\n      setTimeout(run_inline_js, 100);\\n    } else if (!root._bokeh_failed_load) {\\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\\n      root._bokeh_failed_load = true;\\n    } else if (force !== true) {\\n      var cell = $(document.getElementById(null)).parents(\\'.cell\\').data().cell;\\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\\n    }\\n\\n  }\\n\\n  if (root._bokeh_is_loading === 0) {\\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\\n    run_inline_js();\\n  } else {\\n    load_libs(css_urls, js_urls, function() {\\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\\n      run_inline_js();\\n    });\\n  }\\n}(window));'}}], 'source': \"import bokeh\\nfrom bokeh.io import output_notebook, show\\nfrom bokeh.layouts import column, row\\nfrom bokeh.plotting import figure\\nfrom bokeh.models.widgets import DataTable, DateFormatter, TableColumn\\nfrom bokeh.models import ColumnDataSource, PreText\\nfrom math import pi\\nfrom bokeh.transform import cumsum\\nimport warnings\\nfrom bokeh.models.widgets import Paragraph\\nfrom bokeh.models import Legend\\nfrom bokeh.util.warnings import BokehDeprecationWarning, BokehUserWarning\\nwarnings.simplefilter('ignore', BokehDeprecationWarning)\\nwarnings.simplefilter('ignore', BokehUserWarning)\\n\\noutput_notebook(hide_banner=True)\"}, {'cell_type': 'code', 'execution_count': 3, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:27.433550', 'end_time': '2021-04-23T17:59:27.461900', 'duration': 0.02835, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:27.459371Z', 'iopub.execute_input': '2021-04-23T17:59:27.460136Z', 'iopub.status.idle': '2021-04-23T17:59:27.461358Z', 'shell.execute_reply': '2021-04-23T17:59:27.461757Z'}}, 'outputs': [], 'source': 'def create_piechart(data_dict, title=None, height=400, width=400, x1=0, x2=0.1, radius=0.4, toolbar_location=\\'right\\'):\\n   \\n    plot = figure(plot_height=height, \\n                  plot_width=width,\\n                  toolbar_location=toolbar_location,\\n                  tools=\"hover,wheel_zoom,reset,pan\", \\n                  tooltips=\"@phase:@value\", \\n                  title=title,\\n                  x_range=(-radius-x1, radius+x2))\\n\\n    data = pd.Series(data_dict).reset_index(name=\\'value\\').rename(columns={\\'index\\':\\'phase\\'})\\n    data[\\'angle\\'] = data[\\'value\\']/data[\\'value\\'].sum() * 2*pi\\n    data[\\'color\\'] = bokeh.palettes.viridis(len(data_dict))\\n\\n    plot.wedge(x=0, y=0., radius=radius,\\n        start_angle=cumsum(\\'angle\\', include_zero=True), \\n        end_angle=cumsum(\\'angle\\'),\\n        line_color=\"white\", \\n        source=data, \\n        fill_color=\\'color\\', \\n        legend=\\'phase\\'\\n              )\\n    plot.legend.label_text_font_size = \"8pt\"\\n    plot.legend.location = \\'center_right\\'\\n    plot.axis.axis_label=None\\n    plot.axis.visible=False\\n    plot.grid.grid_line_color = None\\n    plot.outline_line_color = \"white\"\\n    \\n    return plot'}, {'cell_type': 'code', 'execution_count': 4, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:27.480891', 'end_time': '2021-04-23T17:59:27.506232', 'duration': 0.025341, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:27.503725Z', 'iopub.execute_input': '2021-04-23T17:59:27.504492Z', 'shell.execute_reply': '2021-04-23T17:59:27.505679Z', 'iopub.status.idle': '2021-04-23T17:59:27.506118Z'}}, 'outputs': [], 'source': 'from IPython.display import display, HTML, Markdown, Image\\ndef pretty_print(df):\\n    raw_html = df.to_html().replace(\"\\\\\\\\n\",\"<br>\").replace(\\'<tr>\\',\\'<tr style=\"text-align: left;\">\\')\\n    return display(HTML(raw_html))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:27.525270', 'end_time': '2021-04-23T17:59:27.544242', 'duration': 0.018972, 'status': 'completed'}}, 'source': '## Training job summary'}, {'cell_type': 'code', 'execution_count': 5, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:27.563867', 'end_time': '2021-04-23T17:59:27.589130', 'duration': 0.025263, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:27.586598Z', 'iopub.execute_input': '2021-04-23T17:59:27.587306Z', 'shell.execute_reply': '2021-04-23T17:59:27.588592Z', 'iopub.status.idle': '2021-04-23T17:59:27.589019Z'}}, 'outputs': [], 'source': \"def load_report(rule_name):\\n    try:\\n        report = json.load(open('/opt/ml/processing/output/rule/profiler-output/profiler-reports/'+rule_name+'.json'))\\n        return report\\n    except FileNotFoundError:\\n        print (rule_name + ' not triggered')\"}, {'cell_type': 'code', 'execution_count': 6, 'metadata': {'tags': ['hide-input', 'hide-output'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:27.608243', 'end_time': '2021-04-23T17:59:27.641755', 'duration': 0.033512, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:27.636117Z', 'iopub.execute_input': '2021-04-23T17:59:27.637635Z', 'iopub.status.idle': '2021-04-23T17:59:27.641190Z', 'shell.execute_reply': '2021-04-23T17:59:27.641611Z'}}, 'outputs': [], 'source': '\\njob_statistics = {}\\nreport = load_report(\\'MaxInitializationTime\\')\\nif report:\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        first_step = report[\\'Details\\'][\"step_num\"][\"first\"]\\n        last_step = report[\\'Details\\'][\"step_num\"][\"last\"]\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"Start time\"] = f\"{hour} {day}\"\\n    tmp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_end\\'] * 1000000)\\n    date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n    day = date.date().strftime(\"%m/%d/%Y\")\\n    hour = date.time().strftime(\"%H:%M:%S\")\\n    job_statistics[\"End time\"] = f\"{hour} {day}\"\\n    job_duration_in_seconds = int(report[\\'Details\\'][\\'job_end\\'] - report[\\'Details\\'][\\'job_start\\']) \\n    job_statistics[\"Job duration\"] = f\"{job_duration_in_seconds} seconds\"\\n    if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n        tmp = us_since_epoch_to_human_readable_time(first_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop start\"] = f\"{hour} {day}\"\\n        tmp = us_since_epoch_to_human_readable_time(last_step)\\n        date = datetime.datetime.strptime(tmp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        job_statistics[\"Training loop end\"] = f\"{hour} {day}\"\\n        training_loop_duration_in_seconds = int((last_step - first_step) / 1000000)\\n        job_statistics[\"Training loop duration\"] = f\"{training_loop_duration_in_seconds} seconds\"\\n        initialization_in_seconds = int(first_step/1000000 - report[\\'Details\\'][\\'job_start\\'])\\n        job_statistics[\"Initialization time\"] = f\"{initialization_in_seconds} seconds\"\\n        finalization_in_seconds = int(np.abs(report[\\'Details\\'][\\'job_end\\'] - last_step/1000000))\\n        job_statistics[\"Finalization time\"] = f\"{finalization_in_seconds} seconds\"\\n        initialization_perc = int(initialization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Initialization\"] = f\"{initialization_perc} %\"\\n        training_loop_perc = int(training_loop_duration_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Training loop\"] = f\"{training_loop_perc} %\"\\n        finalization_perc = int(finalization_in_seconds / job_duration_in_seconds * 100)\\n        job_statistics[\"Finalization\"] = f\"{finalization_perc} %\"'}, {'cell_type': 'code', 'execution_count': 7, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:27.662005', 'end_time': '2021-04-23T17:59:27.706742', 'duration': 0.044737, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:27.695815Z', 'iopub.execute_input': '2021-04-23T17:59:27.696735Z', 'iopub.status.idle': '2021-04-23T17:59:27.706199Z', 'shell.execute_reply': '2021-04-23T17:59:27.706597Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"4aaf7ec2-29f1-4e2c-bc85-d0d6a52741df\" data-root-id=\"1008\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1008'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"33976ccb-bf1e-4e81-8b3e-49ccefc100cf\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1006\"},{\"id\":\"1007\"}]},\"id\":\"1008\",\"type\":\"Column\"},{\"attributes\":{\"columns\":[{\"id\":\"1002\"},{\"id\":\"1003\"}],\"height\":380,\"source\":{\"id\":\"1001\"},\"view\":{\"id\":\"1005\"},\"width\":450},\"id\":\"1004\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"Selection\"},{\"attributes\":{\"data\":{\"0\":[\"Start time\",\"End time\",\"Job duration\"],\"1\":[\"17:57:06 04/23/2021\",\"17:57:59 04/23/2021\",\"53 seconds\"],\"index\":[0,1,2]},\"selected\":{\"id\":\"1009\"},\"selection_policy\":{\"id\":\"1010\"}},\"id\":\"1001\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1012\"},\"field\":\"0\",\"formatter\":{\"id\":\"1011\"},\"title\":\"\"},\"id\":\"1002\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"StringFormatter\"},{\"attributes\":{\"text\":\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took. \\\\n Your training job started on 04/23/2021 at 17:57:06 and ran for 53 seconds. \\\\n Your training job started on 04/23/2021 at 17:57:06 and ran for 53 seconds.. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\",\"width\":800},\"id\":\"1006\",\"type\":\"Paragraph\"},{\"attributes\":{\"children\":[{\"id\":\"1004\"}]},\"id\":\"1007\",\"type\":\"Row\"},{\"attributes\":{\"editor\":{\"id\":\"1014\"},\"field\":\"1\",\"formatter\":{\"id\":\"1013\"},\"title\":\"Job Statistics\"},\"id\":\"1003\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1012\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"StringEditor\"},{\"attributes\":{\"source\":{\"id\":\"1001\"}},\"id\":\"1005\",\"type\":\"CDSView\"}],\"root_ids\":[\"1008\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"33976ccb-bf1e-4e81-8b3e-49ccefc100cf\",\"root_ids\":[\"1008\"],\"roots\":{\"1008\":\"4aaf7ec2-29f1-4e2c-bc85-d0d6a52741df\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if report:\\n    text =  \"\"\"The following table gives a summary about the training job. The table includes information about when the training job started and ended, how much time initialization, training loop and finalization took.\"\"\"\\n    if len(job_statistics) > 0:\\n        df = pd.DataFrame.from_dict(job_statistics, orient=\\'index\\')\\n        start_time = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'job_start\\'] * 1000000)\\n        date = datetime.datetime.strptime(start_time, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n        day = date.date().strftime(\"%m/%d/%Y\")\\n        hour = date.time().strftime(\"%H:%M:%S\")\\n        duration = job_duration_in_seconds\\n        text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n\\n        #pretty_print(df)\\n        if \"first\" in report[\\'Details\\'][\"step_num\"] and \"last\" in report[\\'Details\\'][\"step_num\"]:\\n            if finalization_perc  < 0:\\n                job_statistics[\"Finalization%\"]  = 0\\n            if training_loop_perc < 0:\\n                job_statistics[\"Training loop\"] = 0\\n            if initialization_perc < 0:\\n                job_statistics[\"Initialization\"] = 0\\n        else:\\n            text = f\"\"\"{text} \\\\n Your training job started on {day} at {hour} and ran for {duration} seconds.\"\"\"\\n            \\n    if len(job_statistics) > 0:\\n        df2 = df.reset_index()\\n        df2.columns = [\"0\", \"1\"]\\n        source = ColumnDataSource(data=df2)\\n        columns = [TableColumn(field=\\'0\\', title=\"\"),\\n                   TableColumn(field=\\'1\\', title=\"Job Statistics\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=380)\\n\\n    plot = None\\n\\n    if \"Initialization\" in job_statistics:\\n        piechart_data = {}\\n        piechart_data[\"Initialization\"] = initialization_perc  \\n        piechart_data[\"Training loop\"]  = training_loop_perc\\n        piechart_data[\"Finalization\"]  = finalization_perc \\n\\n        plot = create_piechart(piechart_data, \\n                               height=350,\\n                               width=500,\\n                               x1=0.15,\\n                               x2=0.15,\\n                               radius=0.15, \\n                               toolbar_location=None)\\n\\n    if plot != None:\\n        paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width = 800)\\n        show(column(paragraph, row(table, plot)))\\n    else:\\n        paragraph = Paragraph(text=f\"\"\"{text}. No step information was profiled from your training job. The time spent on initialization and finalization cannot be computed.\"\"\" , width = 800)\\n        show(column(paragraph, row(table)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:27.727207', 'end_time': '2021-04-23T17:59:27.747262', 'duration': 0.020055, 'status': 'completed'}}, 'source': '## System usage statistics'}, {'cell_type': 'code', 'execution_count': 8, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:27.767431', 'end_time': '2021-04-23T17:59:27.793674', 'duration': 0.026243, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:27.790905Z', 'iopub.execute_input': '2021-04-23T17:59:27.791441Z', 'iopub.status.idle': '2021-04-23T17:59:27.793114Z', 'shell.execute_reply': '2021-04-23T17:59:27.793535Z'}}, 'outputs': [], 'source': \"report = load_report('OverallSystemUsage')\"}, {'cell_type': 'code', 'execution_count': 9, 'metadata': {'tags': ['hide-in\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m put'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:27.813950', 'end_time': '2021-04-23T17:59:27.844189', 'duration': 0.030239, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:27.841632Z', 'iopub.execute_input': '2021-04-23T17:59:27.842428Z', 'iopub.status.idle': '2021-04-23T17:59:27.843655Z', 'shell.execute_reply': '2021-04-23T17:59:27.844049Z'}}, 'outputs': [], 'source': 'text1 = \\'\\'\\nif report:\\n    if \"GPU\" in report[\"Details\"]:\\n        for node_id in report[\"Details\"][\"GPU\"]:\\n            gpu_p95 = report[\"Details\"][\"GPU\"][node_id][\"p95\"]\\n            gpu_p50 = report[\"Details\"][\"GPU\"][node_id][\"p50\"]\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            cpu_p50 = report[\"Details\"][\"CPU\"][node_id][\"p50\"]\\n            \\n            if gpu_p95 < 70 and cpu_p95 < 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                The 95th percentile of the total CPU utilization is only {int(cpu_p95)}%. Node {node_id} is underutilized. \\n                You may want to consider switching to a smaller instance type.\"\"\"\\n            elif gpu_p95 < 70 and cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total GPU utilization on node {node_id} is only {int(gpu_p95)}%. \\n                However, the 95th percentile of the total CPU utilization is {int(cpu_p95)}%. GPUs on node {node_id} are underutilized \\n                likely because of CPU bottlenecks\"\"\"\\n            elif gpu_p50 > 70:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                GPUs on node {node_id} are well utilized\"\"\"\\n            else:\\n                text1 = f\"\"\"{text1}The median total GPU utilization on node {node_id} is {int(gpu_p50)}%. \\n                The median total CPU utilization is {int(cpu_p50)}%.\"\"\"\\n    else:\\n        for node_id in report[\"Details\"][\"CPU\"]:\\n            cpu_p95 = report[\"Details\"][\"CPU\"][node_id][\"p95\"]\\n            if cpu_p95 > 70:\\n                text1 = f\"\"\"{text1}The 95th percentile of the total CPU utilization on node {node_id} is {int**(cpu_p95)}%. GPUs on node {node_id} are well utilized\"\"\"\\n    text1 = Paragraph(text=f\"\"\"{text1}\"\"\", width=1100)\\n    text2 = Paragraph(text=f\"\"\"The following table shows statistics of resource utilization per worker (node), \\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\"\"\", width=900)\\n'}, {'cell_type': 'code', 'execution_count': 10, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:27.864741', 'end_time': '2021-04-23T17:59:27.913755', 'duration': 0.049014, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:27.894176Z', 'iopub.execute_input': '2021-04-23T17:59:27.909580Z', 'iopub.status.idle': '2021-04-23T17:59:27.913184Z', 'shell.execute_reply': '2021-04-23T17:59:27.913606Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"48e821b3-50c0-4b41-ace0-1e4d2d0a7910\" data-root-id=\"1059\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1059'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"59305723-4e09-4a8b-9561-bd8bb18f6033\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1045\"},{\"id\":\"1046\"},{\"id\":\"1058\"}]},\"id\":\"1059\",\"type\":\"Column\"},{\"attributes\":{\"editor\":{\"id\":\"1069\"},\"field\":\"Node\",\"formatter\":{\"id\":\"1068\"},\"title\":\"node\"},\"id\":\"1048\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1077\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1081\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1080\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1069\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1078\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1081\"},\"field\":\"p50\",\"formatter\":{\"id\":\"1080\"},\"title\":\"p50\"},\"id\":\"1054\",\"type\":\"TableColumn\"},{\"attributes\":{\"columns\":[{\"id\":\"1048\"},{\"id\":\"1049\"},{\"id\":\"1050\"},{\"id\":\"1051\"},{\"id\":\"1052\"},{\"id\":\"1053\"},{\"id\":\"1054\"},{\"id\":\"1055\"}],\"height\":120,\"source\":{\"id\":\"1047\"},\"view\":{\"id\":\"1057\"},\"width\":800},\"id\":\"1056\",\"type\":\"DataTable\"},{\"attributes\":{},\"id\":\"1076\",\"type\":\"StringFormatter\"},{\"attributes\":{\"children\":[{\"id\":\"1056\"}]},\"id\":\"1058\",\"type\":\"Row\"},{\"attributes\":{\"editor\":{\"id\":\"1071\"},\"field\":\"metric\",\"formatter\":{\"id\":\"1070\"},\"title\":\"metric\"},\"id\":\"1049\",\"type\":\"TableColumn\"},{\"attributes\":{\"editor\":{\"id\":\"1079\"},\"field\":\"p95\",\"formatter\":{\"id\":\"1078\"},\"title\":\"p95\"},\"id\":\"1053\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1075\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1079\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1067\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"1070\",\"type\":\"StringFormatter\"},{\"attributes\":{\"width\":1100},\"id\":\"1045\",\"type\":\"Paragraph\"},{\"attributes\":{\"editor\":{\"id\":\"1083\"},\"field\":\"min\",\"formatter\":{\"id\":\"1082\"},\"title\":\"min\"},\"id\":\"1055\",\"type\":\"TableColumn\"},{\"attributes\":{\"text\":\"The following table shows statistics of resource utilization per worker (node), \\\\n    such as the total CPU and GPU utilization, and the memory utilization on CPU and GPU. \\\\n    The table also includes the total I/O wait time and the total amount of data sent or received in bytes.\\\\n    The table shows min and max values as well as p99, p90 and p50 percentiles.\",\"width\":900},\"id\":\"1046\",\"type\":\"Paragraph\"},{\"attributes\":{},\"id\":\"1068\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1075\"},\"field\":\"max\",\"formatter\":{\"id\":\"1074\"},\"title\":\"max\"},\"id\":\"1051\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1071\",\"type\":\"StringEditor\"},{\"attributes\":{\"editor\":{\"id\":\"1077\"},\"field\":\"p99\",\"formatter\":{\"id\":\"1076\"},\"title\":\"p99\"},\"id\":\"1052\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1072\",\"type\":\"StringFormatter\"},{\"attributes\":{\"editor\":{\"id\":\"1073\"},\"field\":\"unit\",\"formatter\":{\"id\":\"1072\"},\"title\":\"unit\"},\"id\":\"1050\",\"type\":\"TableColumn\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"StringEditor\"},{\"attributes\":{},\"id\":\"1082\",\"type\":\"StringFormatter\"},{\"attributes\":{\"source\":{\"id\":\"1047\"}},\"id\":\"1057\",\"type\":\"CDSView\"},{\"attributes\":{\"data\":{\"Node\":[\"algo-1\",\"algo-1\",\"algo-1\",\"algo-1\"],\"index\":[0,1,2,3],\"level_0\":[0,1,2,3],\"max\":{\"__ndarray__\":\"AAAAAAAAZUAAAAAAAABZQEjhehSuRxRAmpmZmZkpVUA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"metric\":[\"Network\",\"CPU\",\"CPU memory\",\"I/O\"],\"min\":{\"__ndarray__\":\"AAAAAAAAAACuR+F6FK7nP4XrUbgehRFAAAAAAAAAAAA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p50\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAEBJQOF6FK5H4RJAexSuR+F6+D8=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p95\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQD0K16NwPRRAFK5H4Xq0TUA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"p99\":{\"__ndarray__\":\"AAAAAAAAAAAAAAAAAABZQEjhehSuRxRAH4XrUbi+UkA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[4]},\"unit\":[\"bytes\",\"percentage\",\"percentage\",\"percentage\"]},\"selected\":{\"id\":\"1066\"},\"selection_policy\":{\"id\":\"1067\"}},\"id\":\"1047\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1074\",\"type\":\"StringFormatter\"},{\"attributes\":{},\"id\":\"1066\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1083\",\"type\":\"StringEditor\"}],\"root_ids\":[\"1059\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"59305723-4e09-4a8b-9561-bd8bb18f6033\",\"root_ids\":[\"1059\"],\"roots\":{\"1059\":\"48e821b3-50c0-4b41-ace0-1e4d2d0a7910\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nunits = {\"CPU\": \"percentage\", \"CPU memory\": \"percentage\", \"GPU\": \"percentage\", \"Network\": \"bytes\", \"GPU memory\": \"percentage\", \"I/O\": \"percentage\"}\\nif report:\\n    for metric in report[\\'Details\\']:\\n        for node_id in report[\\'Details\\'][metric]:\\n            values = report[\\'Details\\'][metric][node_id]\\n            rows.append([node_id, metric, units[metric], values[\\'max\\'], values[\\'p99\\'], values[\\'p95\\'], values[\\'p50\\'], values[\\'min\\']])\\n\\n    df = pd.DataFrame(rows) \\n    df.columns = [\\'Node\\', \\'metric\\', \\'unit\\', \\'max\\', \\'p99\\', \\'p95\\', \\'p50\\', \\'min\\']\\n    df2 = df.reset_index()\\n    source = ColumnDataSource(data=df2)\\n    columns = [TableColumn(field=\\'Node\\', title=\"node\"),\\n               TableColumn(field=\\'metric\\', title=\"metric\"),\\n               TableColumn(field=\\'unit\\', title=\"unit\"),\\n               TableColumn(field=\\'max\\', title=\"max\"),\\n               TableColumn(field=\\'p99\\', title=\"p99\"),\\n               TableColumn(field=\\'p95\\', title=\"p95\"),\\n               TableColumn(field=\\'p50\\', title=\"p50\"),\\n               TableColumn(field=\\'min\\', title=\"min\"),]\\n    table = DataTable(source=source, columns=columns, width=800, height=df2.shape[0]*30)\\n\\n    show(column( text1, text2, row(table)))'}, {'cell_type': 'code', 'execution_count': 11, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:27.935144', 'end_time': '2021-04-23T17:59:27.977299', 'duration': 0.042155, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:27.970986Z', 'iopub.execute_input': '2021-04-23T17:59:27.974023Z', 'shell.execute_reply': '2021-04-23T17:59:27.976751Z', 'iopub.status.idle': '2021-04-23T17:59:27.977183Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Framework metrics summary'}}], 'source': 'report = load_report(\\'OverallFrameworkMetrics\\')\\nif report:\\n    if \\'Details\\' in report:\\n\\n        display(Markdown(f\"\"\"## Framework metrics summary\"\"\"))\\n        plots = []\\n        text = \\'\\'\\n        if \\'phase\\' in report[\\'Details\\']:\\n            text = f\"\"\"The following two pie charts show the time spent on the TRAIN phase, the EVAL phase, \\n            and others. The \\'others\\' includes the time spent between steps (after one step has finished and before\\n            the next step has started). Ideally, most of the training time should be spent on the \\n            TRAIN and EVAL phases. If TRAIN/EVAL were not specified in the training script, steps will be recorded as \\n            GLOBAL.\"\"\"\\n\\n            if \\'others\\' in report[\\'Details\\'][\\'phase\\']:\\n                others = float(report[\\'Details\\'][\\'phase\\'][\\'others\\'])\\n\\n                if others > 25:\\n                    text = f\"\"\"{text} Your training job spent quite a significant amount of time ({round(others,2)}%) in phase \"others\".\\n                    You should check what is happening in between the steps.\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase and others\")\\n                plots.append(plot)\\n\\n        if \\'forward_backward\\' in report[\\'Details\\']:\\n\\n            event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n            perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n            text = f\"\"\"{text} The pie chart on the right shows a more detailed breakdown. \\n            It shows that {int(perc)}% of the time was spent in event \"{event}\".\"\"\"\\n\\n            if perc > 70:\\n                text = f\"\"\"There is quite a significant difference between the time spent on forward and backward\\n                pass.\"\"\"\\n            else:\\n                text = f\"\"\"{text} It shows that {int(perc)}% of the training time\\n                was spent on \"{event}\".\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"The ratio between forward and backward pass\") \\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text=\\'\\'\\n        if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n            key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n            ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n            text = f\"\"\"The following piechart shows a breakdown of the CPU/GPU operators. \\n                It shows that {int(ratio)}% of training time was spent on executing the \"{key}\" operator.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on CPU/GPU operators\")\\n            plots.append(plot)\\n\\n\\n        if \\'general\\' in report[\\'Details\\']:\\n            event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n            perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"General framework operations\")\\n            plots.append(plot)\\n\\n        if len(plots) > 0:\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plots)))\\n\\n        plots = []\\n        text = \\'\\'\\n        if \\'horovod\\' in report[\\'Details\\']:\\n            display(Markdown(f\"\"\"#### Overview: Horovod metrics\"\"\"))\\n            event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n            perc = report[\\'Details\\'][\\'horovod\\'][event]\\n            text = f\"\"\"{text} The following pie chart shows a detailed breakdown of the Horovod metrics profiled\\n            from your training job. The most expensive function was \"{event}\" with {int(perc)}%.\"\"\"\\n\\n            plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                                title=\"Horovod metrics \")\\n\\n            paragraph = Paragraph(text=text, width=1100)\\n            show(column(paragraph, row(plot)))\\n'}, {'cell_type': 'code', 'execution_count': 12, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:27.999444', 'end_time': '2021-04-23T17:59:28.031700', 'duration': 0.032256, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.029153Z', 'iopub.execute_input': '2021-04-23T17:59:28.029962Z', 'iopub.status.idle': '2021-04-23T17:59:28.031170Z', 'shell.execute_reply': '2021-04-23T1\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m 7:59:28.031559Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'CPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: CPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'CPU\\'], key=report[\\'Details\\'][\\'CPU\\'].get)\\n        perc = report[\\'Details\\'][\\'CPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'CPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'CPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'CPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"CPU operator\"),]\\n\\n        table = DataTable(source=source, columns=columns, width=550, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that ran on the CPUs.\\n        The most expensive operator on the CPUs was \"{event}\" with {int(perc)} %.\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'CPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))\\n'}, {'cell_type': 'code', 'execution_count': 13, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.053762', 'end_time': '2021-04-23T17:59:28.086333', 'duration': 0.032571, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.083708Z', 'iopub.execute_input': '2021-04-23T17:59:28.084448Z', 'iopub.status.idle': '2021-04-23T17:59:28.085780Z', 'shell.execute_reply': '2021-04-23T17:59:28.086191Z'}}, 'outputs': [], 'source': 'pd.set_option(\\'display.float_format\\', lambda x: \\'%.2f\\' % x)\\nrows = [] \\nvalues = []\\nif report:\\n    if \\'GPU_total\\' in report[\\'Details\\']:\\n        display(Markdown(f\"\"\"#### Overview: GPU operators\"\"\"))\\n        event = max(report[\\'Details\\'][\\'GPU\\'], key=report[\\'Details\\'][\\'GPU\\'].get)\\n        perc = report[\\'Details\\'][\\'GPU\\'][event]\\n\\n        for function in report[\\'Details\\'][\\'GPU\\']:\\n            percentage = round(report[\\'Details\\'][\\'GPU\\'][function],2)\\n            time = report[\\'Details\\'][\\'GPU_total\\'][function]               \\n            rows.append([percentage, time, function])\\n\\n        df = pd.DataFrame(rows) \\n        df.columns = [\\'percentage\\', \\'time\\', \\'operator\\']\\n\\n        df = df.sort_values(by=[\\'percentage\\'], ascending=False)\\n        source = ColumnDataSource(data=df)\\n        columns = [TableColumn(field=\\'percentage\\', title=\"Percentage\"),\\n                   TableColumn(field=\\'time\\', title=\"Cumulative time in microseconds\"),\\n                  TableColumn(field=\\'operator\\', title=\"GPU operator\"),]\\n        table = DataTable(source=source, columns=columns, width=450, height=350)\\n\\n        text = Paragraph(text=f\"\"\"The following table shows a list of operators that your training job ran on GPU.\\n        The most expensive operator on GPU was \"{event}\" with {int(perc)} %\"\"\")\\n\\n        plot = create_piechart(report[\\'Details\\'][\\'GPU\\'],\\n                                height=350,\\n                                width=600,\\n                                x1=0.2,\\n                                x2=0.6,\\n                                radius=0.3, \\n                               )\\n\\n        show(column(text, row(table, plot)))'}, {'cell_type': 'markdown', 'metadata': {'tags': [], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.108534', 'end_time': '2021-04-23T17:59:28.130610', 'duration': 0.022076, 'status': 'completed'}}, 'source': '## Rules summary'}, {'cell_type': 'code', 'execution_count': 14, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.152695', 'end_time': '2021-04-23T17:59:28.182329', 'duration': 0.029634, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.179677Z', 'iopub.execute_input': '2021-04-23T17:59:28.180392Z', 'iopub.status.idle': '2021-04-23T17:59:28.181786Z', 'shell.execute_reply': '2021-04-23T17:59:28.182188Z'}}, 'outputs': [], 'source': \"description = {}\\ndescription['CPUBottleneck'] = 'Checks if the CPU utilization is high and the GPU utilization is low. \\\\\\nIt might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive \\\\\\nfrom the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue \\\\\\nif the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['IOBottleneck'] =  'Checks if the data I/O wait time is high and the GPU utilization is low. \\\\\\nIt might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. \\\\\\nThe rule evaluates the I/O and GPU utilization rates and triggers the issue \\\\\\nif the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.'\\ndescription['Dataloader'] = 'Checks how many data loaders are running in parallel and whether the total number is equal the number \\\\\\nof available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. \\\\\\nIf too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.'\\ndescription['GPUMemoryIncrease'] = 'Measures the average GPU memory footprint and triggers if there is a large increase.'\\ndescription['BatchSize'] = 'Checks if GPUs are underutilized because the batch size is too small. \\\\\\nTo detect this problem, the rule analyzes the average GPU memory footprint, \\\\\\nthe CPU and the GPU utilization. '\\ndescription['LowGPUUtilization'] = 'Checks if the GPU utilization is low or fluctuating. \\\\\\nThis can happen due to bottlenecks, blocking calls for synchronizations, \\\\\\nor a small batch size.'\\ndescription['MaxInitializationTime'] = 'Checks if the time spent on initialization exceeds a threshold percent of the total training time. \\\\\\nThe rule waits until the first step of training loop starts. The initialization can take longer \\\\\\nif downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.'\\ndescription['LoadBalancing'] = 'Detects workload balancing issues across GPUs. \\\\\\nWorkload imbalance can occur in training jobs with data parallelism. \\\\\\nThe gradients are accumulated on a primary GPU, and this GPU might be overused \\\\\\nwith regard to other GPUs, resulting in reducing the efficiency of data parallelization.'\\ndescription['StepOutlier'] = 'Detects outliers in step duration. The step duration for forward and backward pass should be \\\\\\nroughly the same throughout the training. If there are significant outliers, \\\\\\nit may indicate a system stall or bottleneck issues.'\"}, {'cell_type': 'code', 'execution_count': 15, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.204613', 'end_time': '2021-04-23T17:59:28.233580', 'duration': 0.028967, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.231028Z', 'iopub.execute_input': '2021-04-23T17:59:28.231730Z', 'shell.execute_reply': '2021-04-23T17:59:28.233014Z', 'iopub.status.idle': '2021-04-23T17:59:28.233466Z'}}, 'outputs': [], 'source': \"recommendation = {}\\nrecommendation['CPUBottleneck'] = 'Consider increasing the number of data loaders \\\\\\nor applying data pre-fetching.'\\nrecommendation['IOBottleneck'] = 'Pre-fetch data or choose different file formats, such as binary formats that \\\\\\nimprove I/O performance.'\\nrecommendation['Dataloader'] = 'Change the number of data loader processes.'\\nrecommendation['GPUMemoryIncrease'] = 'Choose a larger instance type with more memory if footprint is close to maximum available memory.'\\nrecommendation['BatchSize'] = 'The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.'\\nrecommendation['LowGPUUtilization'] = 'Check if there are bottlenecks, minimize blocking calls, \\\\\\nchange distributed training strategy, or increase the batch size.'\\nrecommendation['MaxInitializationTime'] = 'Initialization takes too long. \\\\\\nIf using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.'\\nrecommendation['LoadBalancing'] = 'Choose a different distributed training strategy or \\\\\\na different distributed training framework.'\\nrecommendation['StepOutlier'] = 'Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.'\"}, {'cell_type': 'code', 'execution_count': 16, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.255941', 'end_time': '2021-04-23T17:59:28.296943', 'duration': 0.041002, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.284613Z', 'iopub.execute_input': '2021-04-23T17:59:28.285534Z', 'iopub.status.idle': '2021-04-23T17:59:28.296382Z', 'shell.execute_reply': '2021-04-23T17:59:28.296777Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': 'The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the LowGPUUtilization rule\\nwas the most frequently triggered. It processed 0 datapoints and was triggered 0 times.'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.HTML object>', 'text/html': '<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: left;\">\\n      <th></th>\\n      <th>Description</th>\\n      <th>Recommendation</th>\\n      <th>Number of times rule triggered</th>\\n      <th>Number of datapoints</th>\\n      <th>Rule parameters</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr style=\"text-align: left;\">\\n      <th>LowGPUUtilization</th>\\n      <td>Checks if the GPU utilization is low or fluctuating. This can happen due to bottlenecks, blocking calls for synchronizations, or a small batch size.</td>\\n      <td>Check if there are bottlenecks, minimize blocking calls, change distributed training strategy, or increase the batch size.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold_p95:70<br>threshold_p5:10<br>window:500<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>StepOutlier</th>\\n      <td>Detects outliers in step duration. The step duration for forward and backward pass should be roughly the same throughout the training. If there are significant outliers, it may indicate a system stall or bottleneck issues.</td>\\n      <td>Check if there are any bottlenecks (CPU, I/O) correlated to the step outliers.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:3<br>mode:None<br>n_outliers:10<br>stddev:3</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>GPUMemoryIncrease</th>\\n      <td>Measures the average GPU memory footprint and triggers if there is a large increase.</td>\\n      <td>Choose a larger instance type with more memory if footprint is close to maximum available memory.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>increase:5<br>patience:1000<br>window:10</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>IOBottleneck</th>\\n      <td>Checks if the data I/O wait time is high and the GPU utilization is low. It might indicate IO bottlenecks where GPU is waiting for data to arrive from storage. The rule evaluates the I/O and GPU utilization rates and triggers the issue if the time spent on the IO bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Pre-fetch data or choose different file formats, such as binary formats that improve I/O performance.</td>\\n      <td>0</td>\\n      <td>111</td>\\n      <td>threshold:50<br>io_threshold:50<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>BatchSize</th>\\n      <td>Checks if GPUs are underutilized because the batch size is too small. To detect this problem, the rule analyzes the average GPU memory footprint, the CPU and the GPU utilization.</td>\\n      <td>The batch size is too small, and GPUs are underutilized. Consider running on a smaller instance type or increasing the batch size.</td>\\n      <td>0</td>\\n      <td>107</td>\\n      <td>cpu_threshold_p95:70<br>gpu_threshold_p95:70<br>gpu_memory_threshold_p95:70<br>patience:1000<br>window:500</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>MaxInitializationTime</th>\\n      <td>Checks if the time spent on initialization exceeds a threshold percent of the total training time. The rule waits until the first step of training loop starts. The initialization can take longer if downloading the entire dataset from Amazon S3 in File mode. The default threshold is 20 minutes.</td>\\n      <td>Initialization takes too long. If using File mode, consider switching to Pipe mode in case you are using TensorFlow framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:20</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>CPUBottleneck</th>\\n      <td>Checks if the CPU utilization is high and the GPU utilization is low. It might indicate CPU bottlenecks, where the GPUs are waiting for data to arrive from the CPUs. The rule evaluates the CPU and GPU utilization rates, and triggers the issue if the time spent on the CPU bottlenecks exceeds a threshold percent of the total training time. The default threshold is 50 percent.</td>\\n      <td>Consider increasing the number of data loaders or applying data pre-fetching.</td>\\n      <td>0</td>\\n      <td>111</td>\\n      <td>threshold:50<br>cpu_threshold:90<br>gpu_threshold:10<br>patience:1000</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>Dataloader</th>\\n      <td>Checks how many data loaders are running in parallel and whether the total number is equal the number of available CPU cores. The rule triggers if number is much smaller or larger than the number of available cores. If too small, it might lead to low GPU utilization. If too large, it might impact other compute intensive operations on CPU.</td>\\n      <td>Change the number of data loader processes.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>min_threshold:70<br>max_threshold:200</td>\\n    </tr>\\n    <tr style=\"text-align: left;\">\\n      <th>LoadBalancing</th>\\n      <td>Detects workload balancing issues across GPUs. Workload imbalance can occur in training jobs with data parallelism. The gradients are accumulated on a primary GPU, and this GPU might be overused with regard to other GPUs, resulting in reducing the efficiency of data parallelization.</td>\\n      <td>Choose a different distributed training strategy or a different distributed training framework.</td>\\n      <td>0</td>\\n      <td>0</td>\\n      <td>threshold:0.2<br>patience:1000</td>\\n    </tr>\\n  </tbody>\\n</table>'}}], 'source': 'files = glob.glob(\\'/opt/ml/processing/output/rule/profiler-output/profiler-reports/*json\\')\\nsummary = {}\\nfor i in files:\\n    rule_name = i.split(\\'/\\')[-1].replace(\\'.json\\',\\'\\')\\n    if rule_name == \"OverallSystemUsage\" or rule_name == \"OverallFrameworkMetrics\":\\n        continue\\n    rule_report = json.load(open(i))\\n    summary[rule_name] = {}\\n    summary[rule_name][\\'Description\\'] = description[rule_name]\\n    summary[rule_name][\\'Recommendation\\'] = recommendation[rule_name]\\n    summary[rule_name][\\'Number of times rule triggered\\'] = rule_report[\\'RuleTriggered\\'] \\n    #summary[rule_name][\\'Number of violations\\'] = rule_report[\\'Violations\\\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m '] \\n    summary[rule_name][\\'Number of datapoints\\'] = rule_report[\\'Datapoints\\']\\n    summary[rule_name][\\'Rule parameters\\'] = rule_report[\\'RuleParameters\\']\\n\\ndf = pd.DataFrame.from_dict(summary, orient=\\'index\\')\\ndf = df.sort_values(by=[\\'Number of times rule triggered\\'], ascending=False)\\n\\n\\ndisplay(Markdown(f\"\"\"The following table shows a profiling summary of the Debugger built-in rules. \\nThe table is sorted by the rules that triggered the most frequently. During your training job, the {df.index[0]} rule\\nwas the most frequently triggered. It processed {df.values[0,3]} datapoints and was triggered {df.values[0,2]} times.\"\"\"))\\n\\nwith pd.option_context(\\'display.colheader_justify\\',\\'left\\'):    \\n    pretty_print(df)'}, {'cell_type': 'code', 'execution_count': 17, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.320879', 'end_time': '2021-04-23T17:59:28.352809', 'duration': 0.03193, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.349508Z', 'iopub.execute_input': '2021-04-23T17:59:28.350180Z', 'iopub.status.idle': '2021-04-23T17:59:28.352274Z', 'shell.execute_reply': '2021-04-23T17:59:28.352668Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '## Analyzing the training loop\\n\\n'}}], 'source': 'analyse_phase = \"training\"\\nif job_statistics and \"initialization_in_seconds\" in job_statistics:\\n    if job_statistics[\"initialization_in_seconds\"] > job_statistics[\"training_loop_duration_in_seconds\"]:\\n        analyse_phase = \"initialization\"\\n        time = job_statistics[\"initialization_in_seconds\"]\\n        perc = job_statistics[\"initialization_%\"]\\n        display(Markdown(f\"\"\"The initialization phase took {int(time)} seconds, which is {int(perc)}%*\\n        of the total training time. Since the training loop has taken the most time, \\n        we dive deep into the events occurring during this phase\"\"\"))\\n        display(Markdown(\"\"\"## Analyzing initialization\\\\n\\\\n\"\"\"))\\n    time = job_statistics[\"training_loop_duration_in_seconds\"]\\n    perc = job_statistics[\"training_loop_%\"]\\n    display(Markdown(f\"\"\"The training loop lasted for {int(time)} seconds which is {int(perc)}% of the training job time.\\n                    Since the training loop has taken the most time, we dive deep into the events occured during this phase.\"\"\"))\\nif analyse_phase == \\'training\\':\\n    display(Markdown(\"\"\"## Analyzing the training loop\\\\n\\\\n\"\"\"))'}, {'cell_type': 'code', 'execution_count': 18, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.377417', 'end_time': '2021-04-23T17:59:28.407146', 'duration': 0.029729, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.404808Z', 'iopub.execute_input': '2021-04-23T17:59:28.405414Z', 'shell.execute_reply': '2021-04-23T17:59:28.406606Z', 'iopub.status.idle': '2021-04-23T17:59:28.407031Z'}}, 'outputs': [], 'source': 'if analyse_phase == \"initialization\":\\n    display(Markdown(\"\"\"### MaxInitializationTime\\\\n\\\\nThis rule helps to detect if the training initialization is taking too much time. \\\\nThe rule waits until first step is available. The rule takes the parameter `threshold` that defines how many minutes to wait for the first step to become available. Default is 20 minutes.\\\\nYou can run the rule locally in the following way:\\n    \"\"\"))\\n    \\n    _ = load_report(\"MaxInitializationTime\")'}, {'cell_type': 'code', 'execution_count': 19, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.431430', 'end_time': '2021-04-23T17:59:28.486391', 'duration': 0.054961, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.477655Z', 'iopub.execute_input': '2021-04-23T17:59:28.478273Z', 'shell.execute_reply': '2021-04-23T17:59:28.485749Z', 'iopub.status.idle': '2021-04-23T17:59:28.486270Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Step duration analysis'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"7ef01035-2d63-4859-b7a5-14903b546b97\" data-root-id=\"1175\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1175'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"5ad7c7f5-94ea-42dc-88cb-7805809d6bf6\":{\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"1174\"}]},\"id\":\"1175\",\"type\":\"Column\"},{\"attributes\":{\"text\":\"The StepOutlier rule measures step durations and checks for outliers. The rule \\\\n        returns True if duration is larger than 3 times the standard deviation. The rule \\\\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\\\n        should be checked. In your processing job mode was specified as None. \\\\n        Typically the first step is taking significantly more time and to avoid the \\\\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\\\n        n_outliers was set to 10.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\\\\n        \",\"width\":900},\"id\":\"1174\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1175\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"5ad7c7f5-94ea-42dc-88cb-7805809d6bf6\",\"root_ids\":[\"1175\"],\"roots\":{\"1175\":\"7ef01035-2d63-4859-b7a5-14903b546b97\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Step duration analysis\"\"\"))\\n    report = load_report(\\'StepOutlier\\')\\n    if report:\\n        parameters = report[\\'RuleParameters\\']\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        stddev = params[3].split(\\':\\')[1]\\n        mode = params[1].split(\\':\\')[1]\\n        n_outlier = params[2].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n\\n        text = f\"\"\"The StepOutlier rule measures step durations and checks for outliers. The rule \\n        returns True if duration is larger than {stddev} times the standard deviation. The rule \\n        also takes the parameter mode, that specifies whether steps from training or validation phase \\n        should be checked. In your processing job mode was specified as {mode}. \\n        Typically the first step is taking significantly more time and to avoid the \\n        rule triggering immediately, one can use n_outliers to specify the number of outliers to ignore. \\n        n_outliers was set to {n_outlier}.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\"\\n\\n        paragraph = Paragraph(text=text, width=900)\\n        show(column(paragraph))\\n\\n        if report and len(report[\\'Details\\'][\\'step_details\\']) > 0:\\n            for node_id in report[\\'Details\\'][\\'step_details\\']:\\n                tmp = report[\\'RuleParameters\\'].split(\\'threshold:\\')\\n                threshold = tmp[1].split(\\'\\\\n\\')[0]\\n                n_outliers = report[\\'Details\\'][\\'step_details\\'][node_id][\\'number_of_outliers\\']\\n                mean = report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'][\\'mean\\']\\n                stddev = report[\\'Details\\'][\\'step_details\\'][node_id][\\'stddev\\']\\n                phase = report[\\'Details\\'][\\'step_details\\'][node_id][\\'phase\\']\\n                display(Markdown(f\"\"\"**Step durations on node {node_id}:**\"\"\"))\\n                display(Markdown(f\"\"\"The following table is a summary of the statistics of step durations measured on node {node_id}.\\n                The rule has analyzed the step duration from {phase} phase.\\n                The average step duration on node {node_id} was {round(mean, 2)}s. \\n                The rule detected {n_outliers} outliers, where step duration was larger than {threshold} times the standard deviation of {stddev}s\\n                                 \\\\n\"\"\"))\\n                step_stats_df = pd.DataFrame.from_dict(report[\\'Details\\'][\\'step_details\\'][node_id][\\'step_stats\\'], orient=\\'index\\').T\\n                step_stats_df.index = [\\'Step Durations in [s]\\']\\n                pretty_print(step_stats_df)\\n\\n            display(Markdown(f\"\"\"The following histogram shows the step durations measured on the different nodes. \\n                You can turn on or turn off the visualization of histograms by selecting or unselecting the labels in the legend.\"\"\"))\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              title=f\"\"\"Step durations\"\"\")  \\n\\n            colors = bokeh.palettes.viridis(len(report[\\'Details\\'][\\'step_details\\']))\\n\\n            for index, node_id in enumerate(report[\\'Details\\'][\\'step_details\\']):\\n                probs = report[\\'Details\\'][\\'step_details\\'][node_id][\\'probs\\']\\n                binedges = report[\\'Details\\'][\\'step_details\\'][node_id][\\'binedges\\']\\n\\n                plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[index],\\n                        fill_alpha=0.7,\\n                        legend=node_id)\\n\\n            plot.add_layout(Legend(), \\'right\\')    \\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Step durations in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n            plot.legend.location = \\'center_right\\'\\n            show(plot)\\n\\n        if report[\\'RuleTriggered\\'] > 0:\\n\\n            text=f\"\"\"To get a better understanding of what may have caused those outliers,\\n            we correlate the timestamps of step outliers with other framework metrics that happened at the same time.\\n            The left chart shows how much time was spent in the different framework\\n            metrics aggregated by event phase. The chart on the right shows the histogram of normal step durations (without\\n            outliers). The following chart shows how much time was spent in the different \\n            framework metrics when step outliers occurred. In this chart framework metrics are not aggregated byphase.\"\"\"\\n            plots = []\\n            if \\'phase\\' in report[\\'Details\\']:\\n                text = f\"\"\"{text} The chart (in the middle) shows whether step outliers mainly happened during TRAIN or EVAL phase.\\n                \"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                plots.append(plot)\\n\\n            if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                text = f\"\"\"{text} The pie chart on the right shows a detailed breakdown. \\n                It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"The Ratio between forward and backward pass\") \\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators executed during the step outliers. \\n                    It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between CPU/GPU operators\")\\n                plots.append(plot)\\n\\n\\n            if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n                plots.append(plot)\\n\\n            if len(plots) > 0:\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plots)))\\n\\n            plots = []\\n            text = \"\"\\n            if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                recorded when step outliers happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"General metrics recorded in framework \")\\n\\n                paragraph = Paragraph(text=text, width=900)\\n                show(column(paragraph, row(plot)))      '}, {'cell_type': 'code', 'execution_count': 20, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.512153', 'end_time': '2021-04-23T17:59:28.561664', 'duration': 0.049511, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.552637Z', 'iopub.execute_input': '2021-04-23T17:59:28.553218Z', 'iopub.status.idle': '2021-04-23T17:59:28.561105Z', 'shell.execute_reply': '\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m 2021-04-23T17:59:28.561519Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU utilization analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Usage per GPU** \\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"36d04e65-842b-4f20-b324-9c3c42302fe7\" data-root-id=\"1200\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1200'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"0f68d2bc-b23b-4096-82fb-738168a90b62\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\\\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\\\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\\\n        percentile of GPU utilization on 500 continuous datapoints and found 0 cases where \\\\n        p95 was above 70% and p5 was below 10%. If p95 is high and p5 is low,\\\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\\\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\\\n        so the rule skipped the first 1000 data points.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":800},\"id\":\"1200\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1200\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"0f68d2bc-b23b-4096-82fb-738168a90b62\",\"root_ids\":[\"1200\"],\"roots\":{\"1200\":\"36d04e65-842b-4f20-b324-9c3c42302fe7\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU utilization analysis\\\\n\\\\n\"\"\"))\\n    display(Markdown(\"\"\"**Usage per GPU** \\\\n\\\\n\"\"\"))\\n    report = load_report(\\'LowGPUUtilization\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold_p95 = params[0].split(\\':\\')[1]\\n        threshold_p5 = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        patience = params[3].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text=Paragraph(text=f\"\"\"The LowGPUUtilization rule checks for a low and fluctuating GPU usage. If the GPU usage is \\n        consistently low, it might be caused by bottlenecks or a small batch size. If usage is heavily \\n        fluctuating, it can be due to bottlenecks or blocking calls. The rule computed the 95th and 5th \\n        percentile of GPU utilization on {window} continuous datapoints and found {violations} cases where \\n        p95 was above {threshold_p95}% and p5 was below {threshold_p5}%. If p95 is high and p5 is low,\\n        it might indicate that the GPU usage is highly fluctuating. If both values are very low, \\n        it would mean that the machine is underutilized. During initialization, the GPU usage is likely zero, \\n        so the rule skipped the first {patience} data points.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\", width=800)\\n        show(text)\\n\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            to either switch to a smaller instance type or to increase the batch size. \\n            The last time that the LowGPUUtilization rule was triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. \\n            They show the utilization per GPU (without outliers).\\n            To get a better understanding of the workloads throughout the whole training,\\n            you can check the workload histogram in the next section.\"\"\", width=800)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**GPU utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    p_95 = report[\\'Details\\'][node_id][key][\\'gpu_95\\']\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'gpu_5\\']\\n                    text = f\"\"\"{text} The max utilization of {key} on node {node_id} was {gpu_max}%\"\"\"\\n                    if p_95 < int(threshold_p95): \\n                        text = f\"\"\"{text} and the 95th percentile was only {p_95}%. \\n                        {key} on node {node_id} is underutilized\"\"\"\\n                    if p_5 < int(threshold_p5): \\n                        text = f\"\"\"{text} and the 5th percentile was only {p_5}%\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that utilization on {key} is fluctuating quite a lot.\\\\n\"\"\"\\n     \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                    text=Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                \\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 21, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.588900', 'end_time': '2021-04-23T17:59:28.634764', 'duration': 0.045864, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.626663Z', 'iopub.execute_input': '2021-04-23T17:59:28.627228Z', 'iopub.status.idle': '2021-04-23T17:59:28.634243Z', 'shell.execute_reply': '2021-04-23T17:59:28.634623Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '**Workload balancing**\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"7dbeeeaf-452b-42bd-a083-5d4fe42c2acf\" data-root-id=\"1225\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1225'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"0b733138-ca0d-461d-8060-c73d59562b03\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The LoadBalancing rule helps to detect issues in workload balancing \\\\n        between multiple GPUs. \\\\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\\\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\\\n        threshold of 0.2.\\\\n        During initialization utilization is likely zero, so the rule skipped the first 1000 data points.\\\\n        \",\"width\":900},\"id\":\"1225\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1225\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"0b733138-ca0d-461d-8060-c73d59562b03\",\"root_ids\":[\"1225\"],\"roots\":{\"1225\":\"7dbeeeaf-452b-42bd-a083-5d4fe42c2acf\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': ' \\nif analyse_phase == \"training\": \\n    display(Markdown(\"\"\"**Workload balancing**\\\\n\\\\n\"\"\")) \\n    report = load_report(\\'LoadBalancing\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = params[0].split(\\':\\')[1]\\n        patience = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        paragraph = Paragraph(text=f\"\"\"The LoadBalancing rule helps to detect issues in workload balancing \\n        between multiple GPUs. \\n        It computes a histogram of GPU utilization values for each GPU and compares then the \\n        similarity between histograms. The rule checked if the distance of histograms is larger than the \\n        threshold of {threshold}.\\n        During initialization utilization is likely zero, so the rule skipped the first {patience} data points.\\n        \"\"\", width=900)\\n        show(paragraph)\\n        \\n        if len(report[\\'Details\\']) > 0:\\n            for node_id in report[\\'Details\\']: \\n                \\n                \\n                text = f\"\"\"The following histogram shows the workload per GPU on node {node_id}. \\n                You can enable/disable the visualization of a workload by clicking on the label in the legend.\\n                \"\"\"\\n                if len(report[\\'Details\\']) == 1 and len(report[\\'Details\\'][node_id][\\'workloads\\']) == 1:\\n                    text = f\"\"\"{text} Your training job only used one GPU so there is no workload balancing issue.\"\"\"\\n                \\n                plot = figure(plot_height=450, \\n                              plot_width=850, \\n                              x_range=(-1,100),\\n                              title=f\"\"\"Workloads on node {node_id}\"\"\")\\n                \\n                colors = bokeh.palettes.viridis(len(report[\\'Details\\'][node_id][\\'workloads\\']))\\n                \\n                for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'workloads\\']):\\n                    probs = report[\\'Details\\'][node_id][\\'workloads\\'][gpu_id2]\\n                    plot.quad( top=probs,\\n                                bottom=0,\\n                                left=np.arange(0,98,2),\\n                                right=np.arange(2,100,2),\\n                                line_color=\"white\",\\n                                fill_color=colors[index],\\n                                fill_alpha=0.8,\\n                                legend=gpu_id2 )\\n\\n                    plot.y_range.start = 0\\n                    plot.xaxis.axis_label = f\"\"\"Utilization\"\"\"\\n                    plot.yaxis.axis_label = \"Occurrences\"\\n                    plot.grid.grid_line_color = \"white\"\\n                    plot.legend.click_policy=\"hide\"\\n                \\n                paragraph = Paragraph(text=text)\\n                show(column(paragraph, plot))\\n                \\n                if \"distances\" in report[\\'Details\\'][node_id]:\\n                    text = f\"\"\"The rule identified workload balancing issues on node {node_id} \\n                    where workloads differed by more than threshold {threshold}. \\n                    \"\"\"\\n                    for index, gpu_id2 in enumerate(report[\\'Details\\'][node_id][\\'distances\\']):\\n                        for gpu_id1 in report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2]:\\n                            distance = round(report[\\'Details\\'][node_id][\\'distances\\'][gpu_id2][gpu_id1], 2)\\n                            text = f\"\"\"{text} The difference of workload between {gpu_id2} and {gpu_id1} is: {distance}.\"\"\"\\n\\n                    paragraph = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(column(paragraph))'}, {'cell_type': 'code', 'execution_count': 22, 'metadata': {'scrolled': True, 'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.663308', 'end_time': '2021-04-23T17:59:28.715975', 'duration': 0.052667, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.703534Z', 'iopub.execute_input': '2021-04-23T17:59:28.704106Z', 'iopub.status.idle': '2021-04-23T17:59:28.715437Z', 'shell.execute_reply': '2021-04-23T17:59:28.715820Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### Dataloading analysis\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"c74ea7ab-17ec-4851-b675-cd66a722000e\" data-root-id=\"1250\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1250'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"344f2954-1bca-4e2a-a831-0b0b79beb7cc\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The number of dataloader workers can greatly affect the overall performance \\\\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\\\n        parallel on the training instance and compares it against the total number of cores. \\\\n        The rule checked if the number of processes is smaller than 70% or larger than \\\\n        200% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\\\n        underutilization. Having too many dataloader workers may hurt the\\\\n        overall performance if you are running other compute intensive tasks on the CPU.\\\\n        The rule analysed 0 datapoints and triggered 0 ti\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m mes.\",\"width\":900},\"id\":\"1250\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1250\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"344f2954-1bca-4e2a-a831-0b0b79beb7cc\",\"root_ids\":[\"1250\"],\"roots\":{\"1250\":\"c74ea7ab-17ec-4851-b675-cd66a722000e\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"168be60a-c740-4d89-8f12-33b161a0719c\" data-root-id=\"1275\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1275'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"3713002c-4017-4b21-980a-75727e00f937\":{\"roots\":{\"references\":[{\"attributes\":{\"width\":900},\"id\":\"1275\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1275\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"3713002c-4017-4b21-980a-75727e00f937\",\"root_ids\":[\"1275\"],\"roots\":{\"1275\":\"168be60a-c740-4d89-8f12-33b161a0719c\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### Dataloading analysis\\\\n\\\\n\"\"\"))\\n    report = load_report(\\'Dataloader\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\"\\\\n\")\\n        min_threshold = params[0].split(\\':\\')[1]\\n        max_threshold = params[1].split(\\':\\')[1]\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=f\"\"\"The number of dataloader workers can greatly affect the overall performance \\n        of your training job. The rule analyzed the number of dataloading processes that have been running in \\n        parallel on the training instance and compares it against the total number of cores. \\n        The rule checked if the number of processes is smaller than {min_threshold}% or larger than \\n        {max_threshold}% the total number of cores. Having too few dataloader workers can slowdown data preprocessing and lead to GPU \\n        underutilization. Having too many dataloader workers may hurt the\\n        overall performance if you are running other compute intensive tasks on the CPU.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        text = \"\"\\n        if \\'cores\\' in report[\\'Details\\']:\\n            cores = int(report[\\'Details\\'][\\'cores\\'])\\n            dataloaders = report[\\'Details\\'][\\'dataloaders\\']\\n            if dataloaders < cores: \\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job only \\n                ran on average {dataloaders} dataloader workers in parallel. We recommend you to increase the number of\\n                dataloader workers.\"\"\"\\n            if dataloaders > cores:\\n                text=f\"\"\"{text} Your training instance provided {cores} CPU cores, however your training job ran \\n                on average {dataloaders} dataloader workers. We recommed you to decrease the number of dataloader\\n                workers.\"\"\"\\n        if \\'pin_memory\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'pin_memory\\'] == False:\\n            text=f\"\"\"{text} Using pinned memory also improves performance because it enables fast data transfer to CUDA-enabled GPUs.\\n            The rule detected that your training job was not using pinned memory. \\n            In case of using PyTorch Dataloader, you can enable this by setting pin_memory=True.\"\"\"\\n            \\n        if \\'prefetch\\' in report[\\'Details\\'] and report[\\'Details\\'][\\'prefetch\\'] == False:\\n            text=f\"\"\"{text} It appears that your training job did not perform any data pre-fetching. Pre-fetching can improve your\\n            data input pipeline as it produces the data ahead of time.\"\"\"\\n        paragraph = Paragraph(text=f\"{text}\", width=900)\\n        show(paragraph)\\n        \\n        colors=bokeh.palettes.viridis(10)\\n        if \"dataloading_time\" in report[\\'Details\\']:\\n            median = round(report[\\'Details\\'][\"dataloading_time\"][\\'p50\\'],4)\\n            p95 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p95\\'],4)\\n            p25 = round(report[\\'Details\\'][\"dataloading_time\"][\\'p25\\'],4)\\n            binedges = report[\\'Details\\'][\"dataloading_time\"][\\'binedges\\']\\n            probs = report[\\'Details\\'][\"dataloading_time\"][\\'probs\\']\\n            text=f\"\"\"The following histogram shows the distribution of dataloading times that have been measured throughout your training job. The median dataloading time was {median}s. \\n            The 95th percentile was {p95}s and the 25th percentile was {p25}s\"\"\"\\n\\n            plot = figure(plot_height=450, \\n                              plot_width=850,\\n                              toolbar_location=\\'right\\',\\n                              tools=\"hover,wheel_zoom,reset,pan\",\\n                              x_range=(binedges[0], binedges[-1])\\n                              )\\n            \\n            plot.quad( top=probs,\\n                        bottom=0,\\n                        left=binedges[:-1],\\n                        right=binedges[1:],\\n                        line_color=\"white\",\\n                        fill_color=colors[0],\\n                        fill_alpha=0.8,\\n                        legend=\"Dataloading events\" )\\n\\n            plot.y_range.start = 0\\n            plot.xaxis.axis_label = f\"\"\"Dataloading in [s]\"\"\"\\n            plot.yaxis.axis_label = \"Occurrences\"\\n            plot.grid.grid_line_color = \"white\"\\n            plot.legend.click_policy=\"hide\"\\n\\n            paragraph = Paragraph(text=f\"{text}\", width=900)\\n            show(column(paragraph, plot))'}, {'cell_type': 'code', 'execution_count': 23, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.746641', 'end_time': '2021-04-23T17:59:28.798282', 'duration': 0.051641, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.789900Z', 'iopub.execute_input': '2021-04-23T17:59:28.790468Z', 'iopub.status.idle': '2021-04-23T17:59:28.797761Z', 'shell.execute_reply': '2021-04-23T17:59:28.798139Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': ' ### Batch size'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"155295b3-d996-43d9-aa22-ce9d7c9b8b80\" data-root-id=\"1300\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1300'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"5a6f7f2c-9763-4993-a2d3-da36df2e5c45\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\\\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\\\n        70%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of 70% and the 95th percentile of memory footprint         below gpu_memory_threshold_p95 of 70%. In your training job this happened 0 times.         The rule skipped the first 1000 datapoints. The rule computed the percentiles over window size of 500 continuous datapoints.\\\\n\\\\n        The rule analysed 107 datapoints and triggered 0 times.\\\\n        \",\"width\":800},\"id\":\"1300\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1300\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"5a6f7f2c-9763-4993-a2d3-da36df2e5c45\",\"root_ids\":[\"1300\"],\"roots\":{\"1300\":\"155295b3-d996-43d9-aa22-ce9d7c9b8b80\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\" ### Batch size\"\"\"))\\n    report = load_report(\\'BatchSize\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        cpu_threshold_p95 = int(params[0].split(\\':\\')[1])\\n        gpu_threshold_p95 = int(params[1].split(\\':\\')[1])\\n        gpu_memory_threshold_p95 = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        window = int(params[4].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        text = Paragraph(text=f\"\"\"The BatchSize rule helps to detect if GPU is underutilized because of the batch size being \\n        too small. To detect this the rule analyzes the GPU memory footprint, CPU and GPU utilization. The rule checked if the 95th percentile of CPU utilization is below cpu_threshold_p95 of \\n        {cpu_threshold_p95}%, the 95th percentile of GPU utilization is below gpu_threshold_p95 of {gpu_threshold_p95}% and the 95th percentile of memory footprint \\\\\\n        below gpu_memory_threshold_p95 of {gpu_memory_threshold_p95}%. In your training job this happened {violations} times. \\\\\\n        The rule skipped the first {patience} datapoints. The rule computed the percentiles over window size of {window} continuous datapoints.\\\\n\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\\n        \"\"\", width=800)\\n        show(text)\\n        if len(report[\\'Details\\']) >0: \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            text = Paragraph(text=f\"\"\"Your training job is underutilizing the instance. You may want to consider\\n            either switch to a smaller instance type or to increase the batch size. \\n            The last time the BatchSize rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They the total \\n            CPU utilization, the GPU utilization, and the GPU memory usage per GPU (without outliers).\"\"\", \\n            width=800)\\n            show(text)\\n\\n            for node_id in report[\\'Details\\']:\\n                xmax = max(20, len(report[\\'Details\\'][node_id]))\\n                \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,xmax)\\n                          )\\n                \\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                        upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                        lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                        p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                        p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                        p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                        plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                        plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                        plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                        plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                        plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                        plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                        plot.xaxis.major_label_overrides[index+1] = key\\n                        plot.xgrid.grid_line_color = None\\n                        plot.ygrid.grid_line_color = \"white\"\\n                        plot.grid.grid_line_width = 0\\n\\n                        plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}, {'cell_type': 'code', 'execution_count': 24, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.830276', 'end_time': '2021-04-23T17:59:28.888825', 'duration': 0.058549, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.877646Z', 'iopub.execute_input': '2021-04-23T17:59:28.880947Z', 'shell.execute_reply': '2021-04-23T17:59:28.888269Z', 'iopub.status.idle': '2021-04-23T17:59:28.888711Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### CPU bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d9b9dc80-deec-443f-bd77-81421ae19e70\" data-root-id=\"1325\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1325'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"21934f03-4c72-4906-a2c0-c5c56a0f8293\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of 90% \\\\n        and GPU utilization was below gpu_threshold of 10%. \\\\n        During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints.\\\\n        With this configuration the rule found 0 CPU bottlenecks which is 0% of the total time. This is below the threshold of 50%\\\\n        The rule analysed 111 data points and triggered 0 times.\",\"width\":900},\"id\":\"1325\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1325\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"21934f03-4c72-4906-a2c0-c5c56a0f8293\",\"root_ids\":[\"1325\"],\"roots\":{\"1325\":\"d9b9dc80-deec-443f-bd77-81421ae19e70\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n\r\n",
      "        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### CPU bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'CPUBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        cpu_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n        \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The CPUBottleneck rule checked when the CPU utilization was above cpu_threshold of {cpu_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}%. \\n        During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints.\\n        With this configuration the rule found {violations} CPU bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%\\n        The rule analysed {datapoints} data points and triggered {triggered} times.\"\"\"\\n        \\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to CPU bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a CPU bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by CPU bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by CPU bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether CPU bottlenecks mainly \\n                    happened during train/validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between time spent on TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event {event}\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened during CPU bottlenecks. \\n                        It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\'], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"The ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n                \\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics \\n                    that have been recorded when the CPU bottleneck happened. The most expensive function was \\n                    {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))'}, {'cell_type': 'code', 'execution_count': 25, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:28.921976', 'end_time': '2021-04-23T17:59:28.982022', 'duration': 0.060046, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:28.970668Z', 'iopub.execute_input': '2021-04-23T17:59:28.974138Z', 'iopub.status.idle': '2021-04-23T17:59:28.981492Z', 'shell.execute_reply': '2021-04-23T17:59:28.981880Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### I/O bottlenecks\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"06cede60-56d1-427a-bbc4-6940cd312cd7\" data-root-id=\"1350\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1350'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"c2dfa0d5-537c-4756-a799-5996151c0121\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The IOBottleneck rule checked when I/O wait time was above io_threshold of 50% \\\\n        and GPU utilization was below gpu_threshold of 10. During initialization utilization is likely to be zero, so the rule skipped the first 1000 datapoints. \\\\n        With this configuration the rule found 0 I/O bottlenecks which is 0% of the total time. This is below the threshold of 50%.\\\\n        The rule analysed 111 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1350\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1350\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"c2dfa0d5-537c-4756-a799-5996151c0121\",\"root_ids\":[\"1350\"],\"roots\":{\"1350\":\"06cede60-56d1-427a-bbc4-6940cd312cd7\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\": \\n    display(Markdown(\"\"\"### I/O bottlenecks\\\\n\\\\n\"\"\"))\\n\\n    report = load_report(\\'IOBottleneck\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        threshold = int(params[0].split(\\':\\')[1])\\n        io_threshold = int(params[1].split(\\':\\')[1])\\n        gpu_threshold = int(params[2].split(\\':\\')[1])\\n        patience = int(params[3].split(\\':\\')[1])\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        if report[\\'Violations\\'] > 0:\\n            perc = int(report[\\'Violations\\']/report[\\'Datapoints\\']*100)\\n        else:\\n            perc = 0\\n        if perc < threshold:\\n            string = \\'below\\'\\n        else:\\n            string = \\'above\\'\\n        text = f\"\"\"The IOBottleneck rule checked when I/O wait time was above io_threshold of {io_threshold}% \\n        and GPU utilization was below gpu_threshold of {gpu_threshold}. During initialization utilization is likely to be zero, so the rule skipped the first {patience} datapoints. \\n        With this configuration the rule found {violations} I/O bottlenecks which is {perc}% of the total time. This is {string} the threshold of {threshold}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\"\\n        paragraph = Paragraph(text=text, width=900)\\n        show(paragraph)\\n        \\n        if report:\\n\\n            plots = []\\n            text = \"\"\\n            if report[\\'RuleTriggered\\'] > 0:\\n\\n                low_gpu = report[\\'Details\\'][\\'low_gpu_utilization\\']\\n                cpu_bottleneck = {}\\n                cpu_bottleneck[\"GPU usage above threshold\"] = report[\"Datapoints\"] - report[\"Details\"][\"low_gpu_utilization\"]\\n                cpu_bottleneck[\"GPU usage below threshold\"] = report[\"Details\"][\"low_gpu_utilization\"] - len(report[\"Details\"])\\n                cpu_bottleneck[\"Low GPU usage due to I/O bottlenecks\"] = len(report[\"Details\"][\"bottlenecks\"])\\n\\n                n_bottlenecks = round(len(report[\\'Details\\'][\\'bottlenecks\\'])/datapoints * 100, 2)\\n                text = f\"\"\"The following chart (left) shows how many datapoints were below the gpu_threshold of {gpu_threshold}%\\n                and how many of those datapoints were likely caused by a I/O bottleneck. The rule found {low_gpu} out of {datapoints} datapoints which had a GPU utilization \\n                below {gpu_threshold}%. Out of those datapoints {n_bottlenecks}% were likely caused by I/O bottlenecks. \\n                \"\"\"\\n\\n                plot = create_piechart(cpu_bottleneck, \\n                                    height=350,\\n                                    width=600,\\n                                    x1=0.2,\\n                                    x2=0.6,\\n                                    radius=0.3, \\n                                    title=\"Low GPU usage caused by I/O bottlenecks\")\\n\\n                plots.append(plot)\\n\\n                if \\'phase\\' in report[\\'Details\\']:\\n                    text = f\"\"\"{text} The chart (in the middle) shows whether I/O bottlenecks mainly happened during  trianing or validation phase.\\n                    \"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'phase\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between the time spent on the TRAIN/EVAL phase\")\\n                    plots.append(plot)\\n\\n                if \\'forward_backward\\' in report[\\'Details\\'] and  len(report[\\'Details\\'][\\'forward_backward\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'forward_backward\\'], key=report[\\'Details\\'][\\'forward_backward\\'].get)\\n                    perc = report[\\'Details\\'][\\'forward_backward\\'][event]\\n\\n                    text = f\"\"\"{text} The pie charts on the right shows a more detailed breakdown. \\n                    It shows that {int(perc)}% of the training time was spent on event \"{event}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'forward_backward\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"The ratio between forward and backward pass\") \\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'ratio\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'ratio\\']) > 0:\\n\\n                    key = list(report[\\'Details\\'][\\'ratio\\'].keys())[0]\\n                    ratio = report[\\'Details\\'][\\'ratio\\'][key]\\n\\n                    text = f\"\"\"The following pie chart shows a breakdown of the CPU/GPU operators that happened \\n                    during I/O bottlenecks. It shows that {int(ratio)}% of the training time was spent on executing operators in \"{key}\".\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'ratio\\\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m '], \\n                                            height=350,\\n                                            width=600,\\n                                            x1=0.2,\\n                                            x2=0.6,\\n                                            radius=0.3, \\n                                            title=\"Ratio between CPU/GPU operators\")\\n                    plots.append(plot)\\n\\n\\n                if \\'general\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'general\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'general\\'], key=report[\\'Details\\'][\\'general\\'].get)\\n                    perc = report[\\'Details\\'][\\'general\\'][event]\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'general\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n                    plots.append(plot)\\n\\n                if len(plots) > 0:\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plots)))\\n\\n                plots = []\\n                text = \"\"\\n                if \\'horovod\\' in report[\\'Details\\'] and len(report[\\'Details\\'][\\'horovod\\']) > 0:\\n\\n                    event = max(report[\\'Details\\'][\\'horovod\\'], key=report[\\'Details\\'][\\'horovod\\'].get)\\n                    perc = report[\\'Details\\'][\\'horovod\\'][event]\\n                    text = f\"\"\"The following pie chart shows a detailed breakdown of the Horovod metrics that have been\\n                    recorded when I/O bottleneck happened. The most expensive function was {event} with {int(perc)}%\"\"\"\\n\\n                    plot = create_piechart(report[\\'Details\\'][\\'horovod\\'], \\n                                        height=350,\\n                                        width=600,\\n                                        x1=0.2,\\n                                        x2=0.6,\\n                                        radius=0.3, \\n                                        title=\"General metrics recorded in framework \")\\n\\n                    paragraph = Paragraph(text=text, width=900)\\n                    show(column(paragraph, row(plot)))    \\n'}, {'cell_type': 'code', 'execution_count': 26, 'metadata': {'tags': ['hide-input'], 'papermill': {'exception': False, 'start_time': '2021-04-23T17:59:29.016754', 'end_time': '2021-04-23T17:59:29.075512', 'duration': 0.058758, 'status': 'completed'}, 'execution': {'iopub.status.busy': '2021-04-23T17:59:29.066937Z', 'iopub.execute_input': '2021-04-23T17:59:29.067568Z', 'shell.execute_reply': '2021-04-23T17:59:29.074973Z', 'iopub.status.idle': '2021-04-23T17:59:29.075395Z'}}, 'outputs': [{'output_type': 'display_data', 'metadata': {}, 'data': {'text/plain': '<IPython.core.display.Markdown object>', 'text/markdown': '### GPU memory\\n\\n'}}, {'output_type': 'display_data', 'metadata': {}, 'data': {'text/html': '\\n\\n\\n\\n\\n\\n  <div class=\"bk-root\" id=\"d755d26a-2008-4d8c-820d-db4bb34ae936\" data-root-id=\"1375\"></div>\\n'}}, {'output_type': 'display_data', 'metadata': {'application/vnd.bokehjs_exec.v0+json': {'id': '1375'}}, 'data': {'application/javascript': '(function(root) {\\n  function embed_document(root) {\\n    \\n  var docs_json = {\"6dab851e-03f1-44ea-aed8-a369cb1bd65f\":{\"roots\":{\"references\":[{\"attributes\":{\"text\":\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\\\n        The rule checked if the moving average of memory increased by more than 5.0%. \\\\n        So if the moving average increased for instance from 10% to 16.0%, \\\\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first 1000 datapoints.\\\\n        The moving average was computed on a window size of 10 continuous datapoints. The rule detected 0 violations\\\\n        where the moving average between previous and current time window increased by more than 5.0%.\\\\n        The rule analysed 0 datapoints and triggered 0 times.\",\"width\":900},\"id\":\"1375\",\"type\":\"Paragraph\"}],\"root_ids\":[\"1375\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\\n  var render_items = [{\"docid\":\"6dab851e-03f1-44ea-aed8-a369cb1bd65f\",\"root_ids\":[\"1375\"],\"roots\":{\"1375\":\"d755d26a-2008-4d8c-820d-db4bb34ae936\"}}];\\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\\n\\n  }\\n  if (root.Bokeh !== undefined) {\\n    embed_document(root);\\n  } else {\\n    var attempts = 0;\\n    var timer = setInterval(function(root) {\\n      if (root.Bokeh !== undefined) {\\n        clearInterval(timer);\\n        embed_document(root);\\n      } else {\\n        attempts++;\\n        if (attempts > 100) {\\n          clearInterval(timer);\\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\\n        }\\n      }\\n    }, 10, root)\\n  }\\n})(window);', 'application/vnd.bokehjs_exec.v0+json': ''}}], 'source': 'if analyse_phase == \"training\":\\n    display(Markdown(\"\"\"### GPU memory\\\\n\\\\n\"\"\"))\\n    \\n    report = load_report(\\'GPUMemoryIncrease\\')\\n    if report:\\n        params = report[\\'RuleParameters\\'].split(\\'\\\\n\\')\\n        increase = float(params[0].split(\\':\\')[1])\\n        patience = params[1].split(\\':\\')[1]\\n        window = params[2].split(\\':\\')[1]\\n        violations = report[\\'Violations\\']\\n        triggered = report[\\'RuleTriggered\\']\\n        datapoints = report[\\'Datapoints\\']\\n    \\n        text=Paragraph(text=f\"\"\"The GPUMemoryIncrease rule helps to detect large increase in memory usage on GPUs. \\n        The rule checked if the moving average of memory increased by more than {increase}%. \\n        So if the moving average increased for instance from 10% to {11+increase}%, \\n        the rule would have triggered. During initialization utilization  is likely 0, so the rule skipped the first {patience} datapoints.\\n        The moving average was computed on a window size of {window} continuous datapoints. The rule detected {violations} violations\\n        where the moving average between previous and current time window increased by more than {increase}%.\\n        The rule analysed {datapoints} datapoints and triggered {triggered} times.\"\"\",\\n                       width=900)\\n        show(text)\\n\\n        if len(report[\\'Details\\']) > 0:\\n            \\n            timestamp = us_since_epoch_to_human_readable_time(report[\\'Details\\'][\\'last_timestamp\\'])\\n            date = datetime.datetime.strptime(timestamp, \\'%Y-%m-%dT%H:%M:%S:%f\\')\\n            day = date.date().strftime(\"%m/%d/%Y\")\\n            hour = date.time().strftime(\"%H:%M:%S\")\\n            text = Paragraph(text=f\"\"\"Your training job triggered memory spikes. \\n            The last time the GPUMemoryIncrease rule triggered in your training job was on {day} at {hour}.\\n            The following boxplots are a snapshot from the timestamps. They show for each node and GPU the corresponding\\n            memory utilization (without outliers).\"\"\", width=900)\\n            show(text)\\n            \\n            del report[\\'Details\\'][\\'last_timestamp\\']\\n            \\n            for node_id in report[\\'Details\\']:\\n    \\n                plot = figure(plot_height=350, \\n                          plot_width=1000,\\n                          toolbar_location=\\'right\\',\\n                          tools=\"hover,wheel_zoom,reset,pan\", \\n                          title=f\"Node {node_id}\",\\n                          x_range=(0,17),\\n                          )\\n\\n                for index, key in enumerate(report[\\'Details\\'][node_id]):\\n                    display(Markdown(f\"\"\"**Memory utilization of {key} on node {node_id}:**\"\"\"))\\n                    text = \"\"\\n                    gpu_max = report[\\'Details\\'][node_id][key][\\'gpu_max\\']\\n                    text = f\"\"\"{text} The max memory utilization of {key} on node {node_id} was {gpu_max}%.\"\"\"\\n                    \\n                    p_95 = int(report[\\'Details\\'][node_id][key][\\'p95\\'])\\n                    p_5 = report[\\'Details\\'][node_id][key][\\'p05\\']\\n                    if p_95 < int(50): \\n                        text = f\"\"\"{text} The 95th percentile was only {p_95}%.\"\"\"\\n                    if p_5 < int(5): \\n                        text = f\"\"\"{text} The 5th percentile was only {p_5}%.\"\"\"\\n                    if p_95 - p_5 > 50:\\n                        text = f\"\"\"{text} The difference between 5th percentile {p_5}% and 95th percentile {p_95}% is quite \\n                        significant, which means that memory utilization on {key} is fluctuating quite a lot.\"\"\"\\n                        \\n                    text = Paragraph(text=f\"\"\"{text}\"\"\", width=900)\\n                    show(text)\\n                    \\n                    upper = report[\\'Details\\'][node_id][key][\\'upper\\']\\n                    lower = report[\\'Details\\'][node_id][key][\\'lower\\']\\n                    p75 = report[\\'Details\\'][node_id][key][\\'p75\\']\\n                    p25 = report[\\'Details\\'][node_id][key][\\'p25\\']\\n                    p50 = report[\\'Details\\'][node_id][key][\\'p50\\']\\n\\n                    plot.segment(index+1, upper, index+1, p75, line_color=\"black\")\\n                    plot.segment(index+1, lower, index+1, p25, line_color=\"black\")\\n\\n                    plot.vbar(index+1, 0.7, p50, p75, fill_color=\"#FDE725\", line_color=\"black\")\\n                    plot.vbar(index+1, 0.7, p25, p50, fill_color=\"#440154\", line_color=\"black\")\\n\\n                    plot.rect(index+1, lower, 0.2, 0.01, line_color=\"black\")\\n                    plot.rect(index+1, upper, 0.2, 0.01, line_color=\"black\")\\n\\n                    plot.xaxis.major_label_overrides[index+1] = key\\n                    plot.xgrid.grid_line_color = None\\n                    plot.ygrid.grid_line_color = \"white\"\\n                    plot.grid.grid_line_width = 0\\n\\n                    plot.xaxis.major_label_text_font_size=\"10px\"\\n                plot.xaxis.ticker = np.arange(index+2)\\n                plot.yaxis.axis_label = \"Utilization in %\"\\n                show(plot)'}], 'metadata': {'celltoolbar': 'Tags', 'kernelspec': {'display_name': 'Python 3', 'language': 'python', 'name': 'python3'}, 'language_info': {'name': 'python', 'version': '3.7.9', 'mimetype': 'text/x-python', 'codemirror_mode': {'name': 'ipython', 'version': 3}, 'pygments_lexer': 'ipython3', 'nbconvert_exporter': 'python', 'file_extension': '.py'}, 'papermill': {'parameters': {}, 'environment_variables': {}, 'version': '2.1.2', 'input_path': '/opt/ml/code/profiler_report.ipynb', 'output_path': '/opt/ml/processing/output/rule/profiler-output/.sagemaker-ignore/out.tmp', 'start_time': '2021-04-23T17:59:25.289881', 'end_time': '2021-04-23T17:59:29.517792', 'duration': 4.227911, 'exception': None}}, 'nbformat': 4, 'nbformat_minor': 4}\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:29.591 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:106] Putting output notebook in /opt/ml/processing/output/rule/profiler-output/profiler-report.ipynb\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:29.591 ip-10-2-219-252.ec2.internal:1 INFO profiler_report.py:111] Putting html in /opt/ml/processing/output/rule/profiler-output/profiler-report.html\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:30.009 ip-10-2-219-252.ec2.internal:1 INFO rule_invoker.py:34] No more profiler data for rule ProfilerReport at timestamp 1619200740000000\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m [2021-04-23 17:59:30.009 ip-10-2-219-252.ec2.internal:1 INFO rule_invoker.py:41] Ended execution of rule ProfilerReport at end_step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m #015Executing:   3%|         | 1/30 [00:01<00:30,  1.06s/cell]#015Executing:   7%|         | 2/30 [00:01<00:26,  1.07cell/s]#015Executing:  10%|         | 3/30 [00:02<00:16,  1.62cell/s]#015Executing:  20%|        | 6/30 [00:02<00:05,  4.22cell/s]#015Executing:  30%|       | 9/30 [00:02<00:03,  6.74cell/s]#015Executing:  40%|      | 12/30 [00:02<00:01,  9.40cell/s]#015Executing:  47%|     | 14/30 [00:02<00:01, 10.48cell/s]#015Executing:  53%|    | 16/30 [00:02<00:01, 11.95cell/s]#015Executing:  63%|   | 19/30 [00:02<00:00, 14.17cell/s]#015Executing:  70%|   | 21/30 [00:03<00:00, 14.72cell/s]#015Executing:  77%|  | 23/30 [00:03<00:00, 14.75cell/s]#015Executing:  83%| | 25/30 [00:03<00:00, 14.29cell/s]#015Executing:  90%| | 27/30 [00:03<00:00, 13.54cell/s]#015Executing:  97%|| 29/30 [00:03<00:00, 12.58cell/s]#015Executing: 100%|| 30/30 [00:04<00:00,  7.10cell/s]\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--ProfilerReport-1619200479-3e9cafb8/algo-1-1619200718\u001b[0m Rule evaluation complete.\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:53.091 ip-10-0-131-86.ec2.internal:1 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:55.721 ip-10-0-131-86.ec2.internal:1 INFO local_trial.py:35] Loading trial base_trial at path /opt/ml/processing/input/tensors\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m No environment variable found with name \"base_trial\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m No environment variable found with name \"tensor_regex\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m No environment variable found with name \"use_losses_collection\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m No environment variable found with name \"increase_threshold_percent\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m No environment variable found with name \"mode\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m No environment variable found with name \"absolute_val\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m No environment variable found with name \"action_json\". Will use default param value if present\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.612 ip-10-0-131-86.ec2.internal:1 INFO action.py:20] No action specified. Action str is\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.612 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:92] LossNotDecreasing rule created with num_steps: 2, diff_percent: 5.0, increase_threshold_percent: 5.0, mode: GLOBAL, tensor_regex: , collection_names: metrics,losses absolute_val:True\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.612 ip-10-0-131-86.ec2.internal:1 INFO rule_invoker.py:16] Started execution of rule LossNotDecreasing at step 0\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.694 ip-10-0-131-86.ec2.internal:1 WARNING trial.py:396] No tensors matching the regex pattern:[] given were saved\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.720 ip-10-0-131-86.ec2.internal:1 WARNING trial.py:396] No tensors matching the regex pattern:[] given were saved\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.796 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:179] Adding tname:validation-error and steps:[0, 2] to req_tensors\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.807 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:179] Adding tname:train-error and steps:[0, 2] to req_tensors\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.893 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:199] Checking loss values(tensor_name:validation-error) at step:0 and step:2\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.894 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:254] loss_mode:ModeKeys.GLOBAL step:2 loss_value:0.181 previous_step:0 previous_loss_value:0.192 min_decreased_loss_allowed:0.1824 , max_increased_loss_allowed:0.2016\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.894 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:199] Checking loss values(tensor_name:train-error) at step:0 and step:2\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.894 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:254] loss_mode:ModeKeys.GLOBAL step:2 loss_value:0.105333 previous_step:0 previous_loss_value:0.125667 min_decreased_loss_allowed:0.11938365000000001 , max_increased_loss_allowed:0.13195035\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.909 ip-10-0-131-86.ec2.internal:1 WARNING trial.py:396] No tensors matching the regex pattern:[] given were saved\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.915 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:179] Adding tname:validation-error and steps:[2, 4] to req_tensors\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:57.991 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:179] Adding tname:train-error and steps:[2, 4] to req_tensors\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.099 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:199] Checking loss values(tensor_name:validation-error) at step:2 and step:4\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.100 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:254] loss_mode:ModeKeys.GLOBAL step:4 loss_value:0.184 previous_step:2 previous_loss_value:0.181 min_decreased_loss_allowed:0.17195 , max_increased_loss_allowed:0.19005\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.100 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:199] Checking loss values(tensor_name:train-error) at step:2 and step:4\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.101 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:254] loss_mode:ModeKeys.GLOBAL step:4 loss_value:0.089333 previous_step:2 previous_loss_value:0.105333 min_decreased_loss_allowed:0.10006635 , max_increased_loss_allowed:0.11059965\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.201 ip-10-0-131-86.ec2.internal:1 WARNING trial.py:396] No tensors matching the regex pattern:[] given were saved\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.210 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:179] Adding tname:validation-error and steps:[4, 6] to req_tensors\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.298 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:179] Adding tname:train-error and steps:[4, 6] to req_tensors\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.314 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:199] Checking loss values(tensor_name:validation-error) at step:4 and step:6\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.315 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:254] loss_mode:ModeKeys.GLOBAL step:6 loss_value:0.185 previous_step:4 previous_loss_value:0.184 min_decreased_loss_allowed:0.1748 , max_increased_loss_allowed:0.19319999999999998\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.315 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:199] Checking loss values(tensor_name:train-error) at step:4 and step:6\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.316 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:254] loss_mode:ModeKeys.GLOBAL step:6 loss_value:0.08 previous_step:4 previous_loss_value:0.089333 min_decreased_loss_allowed:0.08486635 , max_increased_loss_allowed:0.09379965\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.405 ip-10-0-131-86.ec2.internal:1 WARNING trial.py:396] No tensors matching the regex pattern:[] given were saved\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.494 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:179] Adding tname:validation-error and steps:[6, 8] to req_tensors\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.500 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:179] Adding tname:train-error and steps:[6, 8] to req_tensors\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.517 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:199] Checking loss values(tensor_name:validation-error) at step:6 and step:8\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.517 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:231] Loss is not decreasing fast enough, diff is:0.0010000000000000009 , abs_old_val:0.185 , abs_new_val:0.184 min_required_change_percent:5.0 min_decreased_loss_allowed:0.17575\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.517 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:187] Loss validation-error is not decreasing over the last 2 steps at step 8. It was 0.185 and now is 0.184\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.517 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:254] loss_mode:ModeKeys.GLOBAL step:8 loss_value:0.184 previous_step:6 previous_loss_value:0.185 min_decreased_loss_allowed:0.17575 , max_increased_loss_allowed:0.19425\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.518 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:199] Checking loss values(tensor_name:train-error) at step:6 and step:8\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.518 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:254] loss_mode:ModeKeys.GLOBAL step:8 loss_value:0.067 previous_step:6 previous_loss_value:0.08 min_decreased_loss_allowed:0.076 , max_increased_loss_allowed:0.084\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.518 ip-10-0-131-86.ec2.internal:1 INFO loss_decrease.py:269] 1 loss is not decreasing over the last 2 steps at step 8\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m [2021-04-23 18:03:58.518 ip-10-0-131-86.ec2.internal:1 INFO action.py:81] Invoking actions\r\n",
      "\u001b[32m/aws/sagemaker/ProcessingJobs\u001b[0m \u001b[36msagemaker-xgboost-2021-04--LossNotDecreasing-9b0b053b/algo-1-1619200928\u001b[0m Exception during rule evaluation: RuleEvaluationConditionMet: Evaluation of the rule LossNotDecreasing at step 8 resulted in the condition being met\r\n"
     ]
    }
   ],
   "source": [
    "!awslogs get {group} --log-stream-name-prefix {prefix} -s3h --aws-region {region}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
